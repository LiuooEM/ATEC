{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-27T07:28:01.303074Z",
     "start_time": "2021-05-27T07:28:00.860885Z"
    }
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "import torch\n",
    "import time\n",
    "import json\n",
    "import numpy as np\n",
    "import math\n",
    "import random\n",
    "import codecs\n",
    "import os\n",
    "from pytorchtools import EarlyStopping\n",
    "from sklearn.model_selection import train_test_split\n",
    "from utils_train import save_data, batch_generator, valid_loss, generate_idx_word, remove_temporary_file\n",
    "from generate_boundary_train_data import generate_boundary_train_data, generate_boundary_start_index, load_data, merge_boundary_train_data, merge_boundary_train_data_final\n",
    "from generate_number_train_data import generate_number_train_data, save_number_train_data, merge_number_train_data, merge_number_train_data_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-27T03:09:56.319574Z",
     "start_time": "2021-05-27T03:09:56.316535Z"
    }
   },
   "outputs": [],
   "source": [
    "seed = 1337\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "random_state = seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-27T03:09:56.328253Z",
     "start_time": "2021-05-27T03:09:56.320637Z"
    }
   },
   "outputs": [],
   "source": [
    "class Model(torch.nn.Module):\n",
    "    def __init__(self, gen_emb, domain_emb, num_classes=3, dropout=0.55, crf=True):\n",
    "        super(Model, self).__init__()\n",
    "        self.gen_embedding = torch.nn.Embedding(gen_emb.shape[0], gen_emb.shape[1])\n",
    "        self.gen_embedding.weight=torch.nn.Parameter(torch.from_numpy(gen_emb), requires_grad=False)\n",
    "        self.domain_embedding = torch.nn.Embedding(domain_emb.shape[0], domain_emb.shape[1])\n",
    "        self.domain_embedding.weight=torch.nn.Parameter(torch.from_numpy(domain_emb), requires_grad=False)\n",
    "        \n",
    "        self.conv1=torch.nn.Conv1d(gen_emb.shape[1]+domain_emb.shape[1], 128, 5, padding=2)\n",
    "        self.conv2=torch.nn.Conv1d(gen_emb.shape[1]+domain_emb.shape[1], 128, 3, padding=1)\n",
    "        self.dropout=torch.nn.Dropout(dropout)\n",
    "        \n",
    "        self.conv3=torch.nn.Conv1d(256, 256, 5, padding=2)\n",
    "        self.conv4=torch.nn.Conv1d(256, 256, 5, padding=2)\n",
    "        self.conv5=torch.nn.Conv1d(256, 256, 5, padding=2)\n",
    "        self.linear_ae=torch.nn.Linear(256, num_classes)\n",
    "        self.crf_flag=crf\n",
    "        if self.crf_flag:\n",
    "            from allennlp.modules import ConditionalRandomField\n",
    "            self.crf=ConditionalRandomField(num_classes)\n",
    "        \n",
    "    def forward(self, x, x_len, x_mask, x_tag=None, testing=False):\n",
    "        x_emb=torch.cat((self.gen_embedding(x), self.domain_embedding(x)), dim=2)\n",
    "        x_emb=self.dropout(x_emb).transpose(1, 2)\n",
    "        x_conv=torch.nn.functional.relu(torch.cat((self.conv1(x_emb), self.conv2(x_emb)), dim=1))\n",
    "        \n",
    "        x_conv=self.dropout(x_conv)\n",
    "        x_conv=torch.nn.functional.relu(self.conv3(x_conv))\n",
    "        x_conv=self.dropout(x_conv)\n",
    "        x_conv=torch.nn.functional.relu(self.conv4(x_conv))\n",
    "        x_conv=self.dropout(x_conv)\n",
    "        x_conv=torch.nn.functional.relu(self.conv5(x_conv))\n",
    "        x_conv=x_conv.transpose(1, 2)\n",
    "        x_logit=self.linear_ae(x_conv)\n",
    "        if testing:\n",
    "            if self.crf_flag:\n",
    "                score=self.crf.viterbi_tags(x_logit, x_mask)\n",
    "            else:\n",
    "                x_logit=x_logit.transpose(2, 0)\n",
    "                score=torch.nn.functional.log_softmax(x_logit).transpose(2, 0)\n",
    "        else:\n",
    "            if self.crf_flag:\n",
    "                score=-self.crf(x_logit, x_tag, x_mask)\n",
    "            else:\n",
    "                x_logit=torch.nn.utils.rnn.pack_padded_sequence(x_logit, x_len, batch_first=True)\n",
    "                score=torch.nn.functional.nll_loss(torch.nn.functional.log_softmax(x_logit.data), x_tag.data)\n",
    "        return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-27T03:09:56.340100Z",
     "start_time": "2021-05-27T03:09:56.329266Z"
    }
   },
   "outputs": [],
   "source": [
    "def train(train_X, train_y, valid_X, valid_y, model, model_fn, optimizer, parameters, run_epoch, epochs, batch_size, crf, generate_data, early_stopping, earlystopping, domain):\n",
    "    best_loss=float(\"inf\")\n",
    "    valid_history=[]\n",
    "    train_history=[]\n",
    "    idx_word = generate_idx_word(model_fn)\n",
    "    for epoch in range(epochs):\n",
    "        pred_y=np.zeros((train_X.shape[0], train_X.shape[1]), np.int16)\n",
    "        offset = range(0, train_X.shape[0], batch_size)\n",
    "        i_th = 0\n",
    "        results = []\n",
    "        for batch in batch_generator(train_X, train_y, batch_size, crf=crf):\n",
    "            batch_train_X, batch_train_y, batch_train_X_len, batch_train_X_mask=batch\n",
    "            loss = model(batch_train_X, batch_train_X_len, batch_train_X_mask, batch_train_y)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm(parameters, 1.)\n",
    "            optimizer.step()\n",
    "            \n",
    "            if generate_data:\n",
    "                model.eval()\n",
    "                batch_print_X_len=np.sum(train_X[offset[i_th]:offset[i_th]+batch_size]!=0, axis=1)\n",
    "                batch_idx=batch_print_X_len.argsort()[::-1]\n",
    "                batch_print_X_len=batch_print_X_len[batch_idx]\n",
    "                batch_print_X_mask=(train_X[offset[i_th]:offset[i_th]+batch_size]!=0)[batch_idx].astype(np.uint8)\n",
    "                batch_print_X=train_X[offset[i_th]:offset[i_th]+batch_size][batch_idx]\n",
    "                batch_print_X_mask=torch.autograd.Variable(torch.from_numpy(batch_print_X_mask).long().cuda())\n",
    "                batch_print_X=torch.autograd.Variable(torch.from_numpy(batch_print_X).long().cuda())\n",
    "                batch_pred_y=model(batch_print_X, batch_print_X_len, batch_print_X_mask, testing=True)\n",
    "                r_idx=batch_idx.argsort()\n",
    "                if crf:\n",
    "                    batch_pred_y=[batch_pred_y[idx] for idx in r_idx]\n",
    "                    for ix in range(len(batch_pred_y)):\n",
    "                        for jx in range(len(batch_pred_y[ix][0])):\n",
    "                            pred_y[offset[i_th]+ix,jx]=batch_pred_y[ix][0][jx]\n",
    "                else:\n",
    "                    batch_pred_y=batch_pred_y.data.cpu().numpy().argmax(axis=2)[r_idx]\n",
    "                    pred_y[offset[i_th]:offset[i_th]+batch_size,:batch_pred_y.shape[1]]=batch_pred_y\n",
    "                model.train()\n",
    "                i_th += 1\n",
    "            \n",
    "        if generate_data:\n",
    "            for j_th in range(len(train_X)):\n",
    "                result = []\n",
    "                words_num = train_X[j_th]\n",
    "                words_str = []\n",
    "                for w in words_num:\n",
    "                    if(w != 0):\n",
    "                        words_str.append(idx_word[w])\n",
    "                gold = train_y[j_th]\n",
    "                pred = pred_y[j_th]\n",
    "                for words_str, gold, pred in zip(words_str, gold, pred):\n",
    "                    result.append(\" \".join([words_str, str(gold), str(pred)]))\n",
    "                results.append(result)\n",
    "            save_data(domain, results, epoch, run_epoch)\n",
    "            context, query, answer, answer_start_index = load_data(domain, epoch, run_epoch)\n",
    "            update_index = generate_boundary_start_index(context, answer_start_index, answer)\n",
    "            generate_boundary_train_data(domain, context, query, answer, update_index, epoch, run_epoch)\n",
    "            save_number_train_data(domain, run_epoch, epoch)\n",
    "        \n",
    "        loss=valid_loss(model, train_X, train_y, crf=crf)\n",
    "        train_history.append(loss)\n",
    "        loss=valid_loss(model, valid_X, valid_y, crf=crf)\n",
    "        valid_history.append(loss)\n",
    "        if loss<best_loss:\n",
    "            best_loss=loss\n",
    "            torch.save(model, model_fn)\n",
    "        shuffle_idx=np.random.permutation(len(train_X))\n",
    "        train_X=train_X[shuffle_idx]\n",
    "        train_y=train_y[shuffle_idx]\n",
    "        if(epoch % 10 == 0):\n",
    "            print(str(epoch) + '/' + str(epochs))\n",
    "        \n",
    "        if earlystopping:\n",
    "            early_stopping(loss,model)\n",
    "            epoch_end = 0\n",
    "            if early_stopping.early_stop:\n",
    "                epoch_end = epoch\n",
    "                print('当前epoch为：' + str(epoch) + ' 已执行提前停止')\n",
    "                break\n",
    "    model=torch.load(model_fn)\n",
    "    if not earlystopping:\n",
    "        epoch_end = epochs - 1\n",
    "    return train_history, valid_history, epoch_end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-27T03:09:56.346116Z",
     "start_time": "2021-05-27T03:09:56.340968Z"
    }
   },
   "outputs": [],
   "source": [
    "def run(domain, data_dir, model_dir, valid_split, runs, epochs, lr, dropout, batch_size, crf, generate_data, earlystopping, patience):\n",
    "    gen_emb=np.load(data_dir+\"gen.vec.npy\")\n",
    "    domain_emb=np.load(data_dir+domain+\"_emb.vec.npy\")\n",
    "    ae_data=np.load(data_dir+domain+\".npz\")\n",
    "    \"\"\"\n",
    "    train_data = ae_data['train_X']\n",
    "    train_label = ae_data['train_y']\n",
    "    train_X, valid_X, train_y, valid_y = train_test_split(train_data,\n",
    "                                                          train_label,\n",
    "                                                          test_size = valid_split,\n",
    "                                                          random_state = random_state)\n",
    "    \"\"\"\n",
    "    valid_X=ae_data['train_X'][-valid_split:]\n",
    "    valid_y=ae_data['train_y'][-valid_split:]\n",
    "    train_X=ae_data['train_X'][:-valid_split]\n",
    "    train_y=ae_data['train_y'][:-valid_split]\n",
    "    \n",
    "    print(\"数据集总大小：\", len(ae_data['train_X']))\n",
    "    print(\"训练集大小：\", len(train_X))\n",
    "    print(\"验证集大小：\", len(valid_X))\n",
    "\n",
    "    epochs_end = []\n",
    "    \n",
    "    for r in range(runs):\n",
    "        print('正在训练第 ' + str(r + 1) + '轮')\n",
    "        model=Model(gen_emb, domain_emb, 3, dropout, crf)\n",
    "        model.cuda()\n",
    "        print(model)\n",
    "        parameters = [p for p in model.parameters() if p.requires_grad]\n",
    "        optimizer=torch.optim.Adam(parameters, lr=lr)\n",
    "        patience = patience\n",
    "        early_stopping = EarlyStopping(patience, verbose = False)\n",
    "        train_history, valid_history, epoch_end = train(train_X, train_y, valid_X, valid_y, model, model_dir+domain+str(r), \n",
    "                                                        optimizer, parameters, r, epochs, batch_size, crf, generate_data, early_stopping, earlystopping, domain)\n",
    "        if generate_data:\n",
    "            if epoch_end != 0:\n",
    "                epochs_end.append(epoch_end)\n",
    "                merge_boundary_train_data(domain, r, epoch_end)\n",
    "                merge_number_train_data(domain, r, epoch_end)\n",
    "            else:\n",
    "                epochs_end.append(epochs)\n",
    "                merge_boundary_train_data(domain, r, epochs)\n",
    "                merge_number_train_data(domain, r, epochs)\n",
    "    if generate_data:\n",
    "        merge_boundary_train_data_final(domain, runs, epochs_end)\n",
    "        merge_number_train_data_final(domain, runs, epochs_end)\n",
    "        generate_number_train_data(domain, runs, epochs_end)\n",
    "    #the generated temporary file is very big (about 6G for each domain), you can decide wheather to delete them\n",
    "    remove_temporary_file(domain, runs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-27T03:38:55.977698Z",
     "start_time": "2021-05-27T03:09:56.347087Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--model_dir', type=str, default=\"model/DECNN/\")\n",
    "    parser.add_argument('--batch_size', type=int, default=128)\n",
    "    parser.add_argument('--epochs', type=int, default=200)\n",
    "    parser.add_argument('--runs', type=int, default=5)\n",
    "    #you can set this parameter to [laptop], [restaurant], [restaurant14], [restaurant15]\n",
    "    parser.add_argument('--domain', type=str, default=\"laptop\")\n",
    "    #if you set the above parameter--domian to [laptop] or [restaurant], you need to set this parameter to [data/prep_data/]\n",
    "    #else you need to set this parameter to [data/prep_data_15/]\n",
    "    parser.add_argument('--data_dir', type=str, default=\"data/prep_data/\")\n",
    "    parser.add_argument('--valid', type=int, default=150)\n",
    "    parser.add_argument('--lr', type=float, default=0.0001)\n",
    "    parser.add_argument('--dropout', type=float, default=0.55)\n",
    "    #you can replace the softmax layer with CRF layer\n",
    "    parser.add_argument('--crf', type=bool, default=False)\n",
    "    #this parameter will decide whether to generate the training data for post-process modules\n",
    "    #if you just want to train DE-CNN, you can set this parameter to False\n",
    "    parser.add_argument('--generate_data', type=bool, default=True)\n",
    "    #we have added earlystopping mechanism\n",
    "    parser.add_argument('--earlystopping', type=bool, default=False)\n",
    "    parser.add_argument('--patience', type=int, default=30)\n",
    "    args = parser.parse_known_args()[0]\n",
    "\n",
    "    run(args.domain, args.data_dir, args.model_dir, args.valid, args.runs, args.epochs, args.lr, args.dropout, args.batch_size, args.crf, args.generate_data, args.earlystopping, args.patience)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "opensource",
   "language": "python",
   "name": "opensource"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "274.667px",
    "left": "454.333px",
    "right": "20px",
    "top": "136px",
    "width": "550px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
