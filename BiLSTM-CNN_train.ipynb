{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-26T09:29:23.789773Z",
     "start_time": "2021-05-26T09:29:23.305215Z"
    }
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "import torch\n",
    "import time\n",
    "import json\n",
    "import numpy as np\n",
    "import math\n",
    "import random\n",
    "import codecs\n",
    "import os\n",
    "from pytorchtools import EarlyStopping\n",
    "from sklearn.model_selection import train_test_split\n",
    "from utils_train import save_data, batch_generator, valid_loss, generate_idx_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-26T09:29:23.793693Z",
     "start_time": "2021-05-26T09:29:23.790840Z"
    }
   },
   "outputs": [],
   "source": [
    "seed = 1337\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "random_state = seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-26T09:29:23.803402Z",
     "start_time": "2021-05-26T09:29:23.794881Z"
    }
   },
   "outputs": [],
   "source": [
    "class Model(torch.nn.Module):\n",
    "    #定义神经网络层\n",
    "    def __init__(self, gen_emb, domain_emb, num_classes=3, dropout=0.55, crf=True):\n",
    "        super(Model, self).__init__()\n",
    "        self.gen_embedding = torch.nn.Embedding(gen_emb.shape[0], gen_emb.shape[1])\n",
    "        self.gen_embedding.weight=torch.nn.Parameter(torch.from_numpy(gen_emb), requires_grad=False)\n",
    "        self.domain_embedding = torch.nn.Embedding(domain_emb.shape[0], domain_emb.shape[1])\n",
    "        self.domain_embedding.weight=torch.nn.Parameter(torch.from_numpy(domain_emb), requires_grad=False)\n",
    "        \n",
    "        self.rnn = torch.nn.LSTM(gen_emb.shape[1]+domain_emb.shape[1], 128, 1, batch_first = True, bidirectional=True)\n",
    "        self.dropout=torch.nn.Dropout(dropout)\n",
    "        \n",
    "        self.conv1=torch.nn.Conv1d(256, 256, 5, padding=2)\n",
    "        self.conv2=torch.nn.Conv1d(256, 256, 5, padding=2)\n",
    "        self.conv3=torch.nn.Conv1d(256, 256, 5, padding=2)\n",
    "\n",
    "        self.linear_ae=torch.nn.Linear(256, num_classes)\n",
    "        self.crf_flag=crf\n",
    "        if self.crf_flag:\n",
    "            from allennlp.modules import ConditionalRandomField\n",
    "            self.crf=ConditionalRandomField(num_classes)\n",
    "        \n",
    "    def forward(self, x, x_len, x_mask, x_tag=None, testing=False):\n",
    "        x_emb=torch.cat((self.gen_embedding(x), self.domain_embedding(x)), dim=2)\n",
    "        x_lstm, _ = self.rnn(x_emb)\n",
    "        \n",
    "        x_conv=self.dropout(x_lstm).transpose(1, 2)\n",
    "        x_conv=torch.nn.functional.relu(self.conv1(x_conv))\n",
    "        x_conv=self.dropout(x_conv)\n",
    "        x_conv=torch.nn.functional.relu(self.conv2(x_conv))\n",
    "        x_conv=self.dropout(x_conv)\n",
    "        x_conv=torch.nn.functional.relu(self.conv3(x_conv))\n",
    "        x_conv=x_conv.transpose(1, 2)\n",
    "        x_logit=self.linear_ae(x_conv)\n",
    "        if testing:\n",
    "            if self.crf_flag:\n",
    "                score=self.crf.viterbi_tags(x_logit, x_mask)\n",
    "            else:\n",
    "                x_logit=x_logit.transpose(2, 0)\n",
    "                score=torch.nn.functional.log_softmax(x_logit).transpose(2, 0)\n",
    "        else:\n",
    "            if self.crf_flag:\n",
    "                score=-self.crf(x_logit, x_tag, x_mask)\n",
    "            else:\n",
    "                x_logit=torch.nn.utils.rnn.pack_padded_sequence(x_logit, x_len, batch_first=True)\n",
    "                score=torch.nn.functional.nll_loss(torch.nn.functional.log_softmax(x_logit.data), x_tag.data)\n",
    "        return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-26T09:29:23.809711Z",
     "start_time": "2021-05-26T09:29:23.804530Z"
    }
   },
   "outputs": [],
   "source": [
    "def train(train_X, train_y, valid_X, valid_y, model, model_fn, optimizer, parameters, run_epoch, epochs, batch_size, crf, early_stopping):\n",
    "    best_loss=float(\"inf\")\n",
    "    valid_history=[]\n",
    "    train_history=[]\n",
    "    for epoch in range(epochs):\n",
    "        pred_y=np.zeros((train_X.shape[0], train_X.shape[1]), np.int16)\n",
    "        offset = range(0, train_X.shape[0], batch_size)\n",
    "        i_th = 0\n",
    "        results = []\n",
    "        for batch in batch_generator(train_X, train_y, batch_size, crf=crf):\n",
    "            batch_train_X, batch_train_y, batch_train_X_len, batch_train_X_mask=batch\n",
    "            loss = model(batch_train_X, batch_train_X_len, batch_train_X_mask, batch_train_y)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm(parameters, 1.)\n",
    "            optimizer.step()\n",
    "        loss=valid_loss(model, train_X, train_y, crf=crf)\n",
    "        train_history.append(loss)\n",
    "        loss=valid_loss(model, valid_X, valid_y, crf=crf)\n",
    "        valid_history.append(loss)\n",
    "        if loss<best_loss:\n",
    "            best_loss=loss\n",
    "            torch.save(model, model_fn)\n",
    "        shuffle_idx=np.random.permutation(len(train_X))\n",
    "        train_X=train_X[shuffle_idx]\n",
    "        train_y=train_y[shuffle_idx]\n",
    "        if(epoch % 10 == 0):\n",
    "            print(str(epoch) + '/' + str(epochs))\n",
    "        early_stopping(loss,model)\n",
    "        epoch_end = 0\n",
    "        if early_stopping.early_stop:\n",
    "            epoch_end = epoch\n",
    "            print('当前epoch为：' + str(epoch) + ' 已执行提前停止')\n",
    "            break\n",
    "    model=torch.load(model_fn)\n",
    "    return train_history, valid_history, epoch_end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-26T09:29:23.815236Z",
     "start_time": "2021-05-26T09:29:23.810491Z"
    }
   },
   "outputs": [],
   "source": [
    "def run(domain, data_dir, model_dir, valid_split, runs, epochs, lr, dropout, batch_size, crf, earlystopping, patience):\n",
    "    gen_emb=np.load(data_dir+\"gen.vec.npy\")\n",
    "    domain_emb=np.load(data_dir+domain+\"_emb.vec.npy\")\n",
    "    ae_data=np.load(data_dir+domain+\".npz\")\n",
    "    \"\"\"\n",
    "    train_data = ae_data['train_X']\n",
    "    train_label = ae_data['train_y']\n",
    "    train_X, valid_X, train_y, valid_y = train_test_split(train_data,\n",
    "                                                          train_label,\n",
    "                                                          test_size = valid_split,\n",
    "                                                          random_state = random_state)\n",
    "    \"\"\"\n",
    "    valid_X=ae_data['train_X'][-valid_split:]\n",
    "    valid_y=ae_data['train_y'][-valid_split:]\n",
    "    train_X=ae_data['train_X'][:-valid_split]\n",
    "    train_y=ae_data['train_y'][:-valid_split]\n",
    "    print(\"数据集总大小：\", len(ae_data['train_X']))\n",
    "    print(\"训练集大小：\", len(train_X))\n",
    "    print(\"验证集大小：\", len(valid_X))\n",
    "\n",
    "    epochs_end = []\n",
    "    \n",
    "    for r in range(runs):\n",
    "        print('正在训练第 ' + str(r + 1) + '轮')\n",
    "        model=Model(gen_emb, domain_emb, 3, dropout, crf)\n",
    "        model.cuda()\n",
    "        print(model)\n",
    "        parameters = [p for p in model.parameters() if p.requires_grad]\n",
    "        optimizer=torch.optim.Adam(parameters, lr=lr)\n",
    "        patience = patience\n",
    "        early_stopping = EarlyStopping(patience, verbose = False)\n",
    "        train_history, valid_history, epoch_end = train(train_X, train_y, valid_X, valid_y, model, model_dir+domain+str(r), \n",
    "                                           optimizer, parameters, r, epochs, batch_size, crf, early_stopping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-26T09:39:24.462744Z",
     "start_time": "2021-05-26T09:29:23.816003Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "数据集总大小： 3045\n",
      "训练集大小： 2895\n",
      "验证集大小： 150\n",
      "正在训练第 1轮\n",
      "Model(\n",
      "  (gen_embedding): Embedding(8518, 300)\n",
      "  (domain_embedding): Embedding(8518, 100)\n",
      "  (rnn): LSTM(400, 128, batch_first=True, bidirectional=True)\n",
      "  (dropout): Dropout(p=0.55, inplace=False)\n",
      "  (conv1): Conv1d(256, 256, kernel_size=(5,), stride=(1,), padding=(2,))\n",
      "  (conv2): Conv1d(256, 256, kernel_size=(5,), stride=(1,), padding=(2,))\n",
      "  (conv3): Conv1d(256, 256, kernel_size=(5,), stride=(1,), padding=(2,))\n",
      "  (linear_ae): Linear(in_features=256, out_features=3, bias=True)\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xinke901/anaconda3/envs/de-cnn/lib/python3.7/site-packages/ipykernel_launcher.py:46: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "/home/xinke901/anaconda3/envs/de-cnn/lib/python3.7/site-packages/ipykernel_launcher.py:15: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/200\n",
      "10/200\n",
      "20/200\n",
      "30/200\n",
      "40/200\n",
      "50/200\n",
      "60/200\n",
      "70/200\n",
      "80/200\n",
      "90/200\n",
      "100/200\n",
      "110/200\n",
      "120/200\n",
      "130/200\n",
      "140/200\n",
      "150/200\n",
      "160/200\n",
      "170/200\n",
      "180/200\n",
      "190/200\n",
      "正在训练第 2轮\n",
      "Model(\n",
      "  (gen_embedding): Embedding(8518, 300)\n",
      "  (domain_embedding): Embedding(8518, 100)\n",
      "  (rnn): LSTM(400, 128, batch_first=True, bidirectional=True)\n",
      "  (dropout): Dropout(p=0.55, inplace=False)\n",
      "  (conv1): Conv1d(256, 256, kernel_size=(5,), stride=(1,), padding=(2,))\n",
      "  (conv2): Conv1d(256, 256, kernel_size=(5,), stride=(1,), padding=(2,))\n",
      "  (conv3): Conv1d(256, 256, kernel_size=(5,), stride=(1,), padding=(2,))\n",
      "  (linear_ae): Linear(in_features=256, out_features=3, bias=True)\n",
      ")\n",
      "0/200\n",
      "10/200\n",
      "20/200\n",
      "30/200\n",
      "40/200\n",
      "50/200\n",
      "60/200\n",
      "70/200\n",
      "80/200\n",
      "90/200\n",
      "100/200\n",
      "110/200\n",
      "120/200\n",
      "130/200\n",
      "140/200\n",
      "150/200\n",
      "160/200\n",
      "170/200\n",
      "180/200\n",
      "190/200\n",
      "正在训练第 3轮\n",
      "Model(\n",
      "  (gen_embedding): Embedding(8518, 300)\n",
      "  (domain_embedding): Embedding(8518, 100)\n",
      "  (rnn): LSTM(400, 128, batch_first=True, bidirectional=True)\n",
      "  (dropout): Dropout(p=0.55, inplace=False)\n",
      "  (conv1): Conv1d(256, 256, kernel_size=(5,), stride=(1,), padding=(2,))\n",
      "  (conv2): Conv1d(256, 256, kernel_size=(5,), stride=(1,), padding=(2,))\n",
      "  (conv3): Conv1d(256, 256, kernel_size=(5,), stride=(1,), padding=(2,))\n",
      "  (linear_ae): Linear(in_features=256, out_features=3, bias=True)\n",
      ")\n",
      "0/200\n",
      "10/200\n",
      "20/200\n",
      "30/200\n",
      "40/200\n",
      "50/200\n",
      "60/200\n",
      "70/200\n",
      "80/200\n",
      "90/200\n",
      "100/200\n",
      "110/200\n",
      "120/200\n",
      "130/200\n",
      "140/200\n",
      "150/200\n",
      "160/200\n",
      "170/200\n",
      "180/200\n",
      "190/200\n",
      "正在训练第 4轮\n",
      "Model(\n",
      "  (gen_embedding): Embedding(8518, 300)\n",
      "  (domain_embedding): Embedding(8518, 100)\n",
      "  (rnn): LSTM(400, 128, batch_first=True, bidirectional=True)\n",
      "  (dropout): Dropout(p=0.55, inplace=False)\n",
      "  (conv1): Conv1d(256, 256, kernel_size=(5,), stride=(1,), padding=(2,))\n",
      "  (conv2): Conv1d(256, 256, kernel_size=(5,), stride=(1,), padding=(2,))\n",
      "  (conv3): Conv1d(256, 256, kernel_size=(5,), stride=(1,), padding=(2,))\n",
      "  (linear_ae): Linear(in_features=256, out_features=3, bias=True)\n",
      ")\n",
      "0/200\n",
      "10/200\n",
      "20/200\n",
      "30/200\n",
      "40/200\n",
      "50/200\n",
      "60/200\n",
      "70/200\n",
      "80/200\n",
      "90/200\n",
      "100/200\n",
      "110/200\n",
      "120/200\n",
      "130/200\n",
      "140/200\n",
      "150/200\n",
      "160/200\n",
      "170/200\n",
      "180/200\n",
      "190/200\n",
      "正在训练第 5轮\n",
      "Model(\n",
      "  (gen_embedding): Embedding(8518, 300)\n",
      "  (domain_embedding): Embedding(8518, 100)\n",
      "  (rnn): LSTM(400, 128, batch_first=True, bidirectional=True)\n",
      "  (dropout): Dropout(p=0.55, inplace=False)\n",
      "  (conv1): Conv1d(256, 256, kernel_size=(5,), stride=(1,), padding=(2,))\n",
      "  (conv2): Conv1d(256, 256, kernel_size=(5,), stride=(1,), padding=(2,))\n",
      "  (conv3): Conv1d(256, 256, kernel_size=(5,), stride=(1,), padding=(2,))\n",
      "  (linear_ae): Linear(in_features=256, out_features=3, bias=True)\n",
      ")\n",
      "0/200\n",
      "10/200\n",
      "20/200\n",
      "30/200\n",
      "40/200\n",
      "50/200\n",
      "60/200\n",
      "70/200\n",
      "80/200\n",
      "90/200\n",
      "100/200\n",
      "110/200\n",
      "120/200\n",
      "130/200\n",
      "140/200\n",
      "150/200\n",
      "160/200\n",
      "170/200\n",
      "180/200\n",
      "190/200\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--model_dir', type=str, default=\"model/BiLSTM/\")\n",
    "    parser.add_argument('--batch_size', type=int, default=128)\n",
    "    parser.add_argument('--epochs', type=int, default=200)\n",
    "    parser.add_argument('--runs', type=int, default=5)\n",
    "    parser.add_argument('--domain', type=str, default=\"laptop\")\n",
    "    parser.add_argument('--data_dir', type=str, default=\"data/prep_data/\")\n",
    "    parser.add_argument('--valid', type=int, default=150)\n",
    "    parser.add_argument('--lr', type=float, default=0.0001)\n",
    "    parser.add_argument('--dropout', type=float, default=0.55)\n",
    "    parser.add_argument('--crf', type=bool, default=False)\n",
    "    parser.add_argument('--earlystopping', type=bool, default=False)\n",
    "    parser.add_argument('--patience', type=int, default=300)\n",
    "    args = parser.parse_known_args()[0]\n",
    "\n",
    "    run(args.domain, args.data_dir, args.model_dir, args.valid, args.runs, args.epochs, args.lr, args.dropout, args.batch_size, args.crf, args.earlystopping, args.patience)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "de-cnn",
   "language": "python",
   "name": "de-cnn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "274.667px",
    "left": "454.333px",
    "right": "20px",
    "top": "136px",
    "width": "550px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
