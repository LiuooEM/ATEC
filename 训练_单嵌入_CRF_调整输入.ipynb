{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-31T11:22:47.239197Z",
     "start_time": "2020-12-31T11:22:47.237391Z"
    }
   },
   "outputs": [],
   "source": [
    "#前期的工作在于构建3个嵌入矩阵"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-31T11:22:47.241528Z",
     "start_time": "2020-12-31T11:22:47.240318Z"
    }
   },
   "outputs": [],
   "source": [
    "#使用pytorch构建深度学习模型有4步\n",
    "#1.定义网络的各个层\n",
    "#2.定义各个对层的操作\n",
    "#3.定义损失函数\n",
    "#4.更新网络参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-31T11:22:48.161259Z",
     "start_time": "2020-12-31T11:22:47.242505Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xinke901/anaconda3/envs/de-cnn/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/xinke901/anaconda3/envs/de-cnn/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/xinke901/anaconda3/envs/de-cnn/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/xinke901/anaconda3/envs/de-cnn/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/xinke901/anaconda3/envs/de-cnn/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/xinke901/anaconda3/envs/de-cnn/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/xinke901/anaconda3/envs/de-cnn/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/xinke901/anaconda3/envs/de-cnn/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/xinke901/anaconda3/envs/de-cnn/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/xinke901/anaconda3/envs/de-cnn/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/xinke901/anaconda3/envs/de-cnn/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/xinke901/anaconda3/envs/de-cnn/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import torch\n",
    "import time\n",
    "import json\n",
    "import numpy as np\n",
    "import math\n",
    "import random\n",
    "import codecs\n",
    "import os\n",
    "from utils_train import load_data, generate_repositioning, generate_repositioning_data, predict_aspects_nums\n",
    "from utils_train import generate_start_index, train_as, merge_data, merge_data_final, save_batch, clean_batch\n",
    "from utils_train import return_predicted_nums\n",
    "from generate_train_data import generate_aspects_nums_train_data, merge_aspects_nums_data, merge_aspects_nums_final_data\n",
    "from process_batch_data import generate_aspects_nums_batch_data\n",
    "from json_to_csv import json_to_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-31T11:22:48.164464Z",
     "start_time": "2020-12-31T11:22:48.162165Z"
    }
   },
   "outputs": [],
   "source": [
    "np.random.seed(1337)\n",
    "random.seed(1337)\n",
    "torch.manual_seed(1337)\n",
    "torch.cuda.manual_seed(1337)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-31T11:22:48.169489Z",
     "start_time": "2020-12-31T11:22:48.165303Z"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "输入：训练集或者验证集的数据和标签，batch_size，\n",
    "输出：\n",
    "代码思路：\n",
    "\"\"\"\n",
    "def batch_generator(X, y, batch_size=128, return_idx=False, crf=True):\n",
    "    #以batch_size为步长\n",
    "    for offset in range(0, X.shape[0], batch_size):\n",
    "        batch_X_len=np.sum(X[offset:offset+batch_size]!=0, axis=1)\n",
    "        batch_idx=batch_X_len.argsort()[::-1]\n",
    "        batch_X_len=batch_X_len[batch_idx]\n",
    "        batch_X_mask=(X[offset:offset+batch_size]!=0)[batch_idx].astype(np.uint8)\n",
    "        batch_X=X[offset:offset+batch_size][batch_idx]\n",
    "        batch_y=y[offset:offset+batch_size][batch_idx]\n",
    "        batch_X = torch.autograd.Variable(torch.from_numpy(batch_X).long().cuda())\n",
    "        batch_X_mask=torch.autograd.Variable(torch.from_numpy(batch_X_mask).long().cuda())\n",
    "        batch_y = torch.autograd.Variable(torch.from_numpy(batch_y).long().cuda())\n",
    "        if len(batch_y.size())==2 and not crf:\n",
    "            batch_y=torch.nn.utils.rnn.pack_padded_sequence(batch_y, batch_X_len, batch_first=True)\n",
    "        if return_idx: #in testing, need to sort back.\n",
    "            yield (batch_X, batch_y, batch_X_len, batch_X_mask, batch_idx)\n",
    "        else:\n",
    "            yield (batch_X, batch_y, batch_X_len, batch_X_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-31T11:22:48.178245Z",
     "start_time": "2020-12-31T11:22:48.170628Z"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "输入：通用领域嵌入，特定领域嵌入，3个参数，分类个数、dropout率、是否使用CRF\n",
    "输出：3个标签的概率\n",
    "代码思路：\n",
    "\"\"\"\n",
    "class Model(torch.nn.Module):\n",
    "    #定义神经网络层\n",
    "    def __init__(self, gen_emb, domain_emb, num_classes=3, dropout=0.55, crf=True):\n",
    "        super(Model, self).__init__()\n",
    "        #构建通用领域嵌入矩阵，先生成相应大小的空间，再将参数传进去\n",
    "        self.gen_embedding = torch.nn.Embedding(gen_emb.shape[0], gen_emb.shape[1])\n",
    "        self.gen_embedding.weight=torch.nn.Parameter(torch.from_numpy(gen_emb), requires_grad=False)\n",
    "        #构建特定领域嵌入矩阵，先生成相应大小的空间，再将参数传进去\n",
    "        \n",
    "        #设定模型的卷积层参数\n",
    "        #第一层第一种卷积核,输入维度是400，输出维度是128，跨步是5，填充是2\n",
    "        self.conv1=torch.nn.Conv1d(gen_emb.shape[1], 128, 5, padding=2)\n",
    "        self.conv2=torch.nn.Conv1d(gen_emb.shape[1], 128, 3, padding=1)\n",
    "        self.dropout=torch.nn.Dropout(dropout)\n",
    "        \n",
    "        #剩下三层的参数相同，输入维度是256，输出维度是128，跨步是5，填充是2\n",
    "        self.conv3=torch.nn.Conv1d(256, 256, 5, padding=2)\n",
    "        self.conv4=torch.nn.Conv1d(256, 256, 5, padding=2)\n",
    "        self.conv5=torch.nn.Conv1d(256, 256, 5, padding=2)\n",
    "        #设置解码层1：就是一个全连接层进行分类，分为3类\n",
    "        self.linear_ae=torch.nn.Linear(256, num_classes)\n",
    "        self.crf_flag=crf\n",
    "        #设置解码层2：CRF\n",
    "        if self.crf_flag:\n",
    "            from allennlp.modules import ConditionalRandomField\n",
    "            self.crf=ConditionalRandomField(num_classes)\n",
    "        \n",
    "    #定义操作\n",
    "    def forward(self, x, x_len, x_mask, x_tag=None, testing=False):\n",
    "        #cat是拼接的意思，0是竖着拼，1是横着拼，2是（3维的情况）\n",
    "        #这段代码好像是传入batchsize的单词的编号，然后去除这些单词嵌入\n",
    "        x_emb=self.gen_embedding(x)\n",
    "        x_emb=self.dropout(x_emb).transpose(1, 2)\n",
    "        x_conv=torch.nn.functional.relu(torch.cat((self.conv1(x_emb), self.conv2(x_emb)), dim=1))\n",
    "        \n",
    "        #3个相同的卷积层\n",
    "        x_conv=self.dropout(x_conv)\n",
    "        x_conv=torch.nn.functional.relu(self.conv3(x_conv))\n",
    "        x_conv=self.dropout(x_conv)\n",
    "        x_conv=torch.nn.functional.relu(self.conv4(x_conv))\n",
    "        x_conv=self.dropout(x_conv)\n",
    "        x_conv=torch.nn.functional.relu(self.conv5(x_conv))\n",
    "        x_conv=x_conv.transpose(1, 2)\n",
    "        x_logit=self.linear_ae(x_conv)\n",
    "        if testing:\n",
    "            if self.crf_flag:\n",
    "                score=self.crf.viterbi_tags(x_logit, x_mask)\n",
    "            else:\n",
    "                x_logit=x_logit.transpose(2, 0)\n",
    "                score=torch.nn.functional.log_softmax(x_logit).transpose(2, 0)\n",
    "        else:\n",
    "            if self.crf_flag:\n",
    "                score=-self.crf(x_logit, x_tag, x_mask)\n",
    "            else:\n",
    "                x_logit=torch.nn.utils.rnn.pack_padded_sequence(x_logit, x_len, batch_first=True)\n",
    "                score=torch.nn.functional.nll_loss(torch.nn.functional.log_softmax(x_logit.data), x_tag.data)\n",
    "        return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-31T11:22:48.181658Z",
     "start_time": "2020-12-31T11:22:48.179306Z"
    }
   },
   "outputs": [],
   "source": [
    "def valid_loss(model, valid_X, valid_y, crf=True):\n",
    "    model.eval()\n",
    "    losses=[]\n",
    "    for batch in batch_generator(valid_X, valid_y, crf=crf):\n",
    "        batch_valid_X, batch_valid_y, batch_valid_X_len, batch_valid_X_mask=batch\n",
    "        loss=model(batch_valid_X, batch_valid_X_len, batch_valid_X_mask, batch_valid_y)\n",
    "        losses.append(loss.item())\n",
    "    model.train()\n",
    "    return sum(losses)/len(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-31T11:22:48.186656Z",
     "start_time": "2020-12-31T11:22:48.182461Z"
    }
   },
   "outputs": [],
   "source": [
    "word_idx_fn = 'data/prep_data/word_idx.json'\n",
    "with open(word_idx_fn) as f:\n",
    "    word_idx=json.load(f)\n",
    "idx_word={}\n",
    "for key,val in word_idx.items():\n",
    "    idx_word[val]=key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-31T11:22:48.200564Z",
     "start_time": "2020-12-31T11:22:48.187494Z"
    }
   },
   "outputs": [],
   "source": [
    "def train(train_X, train_y, valid_X, valid_y, model, model_fn, optimizer, parameters, run_epoch, epochs, batch_size, crf, generate_data, add_num_loss):\n",
    "    best_loss=float(\"inf\")\n",
    "    valid_history=[]\n",
    "    train_history=[]\n",
    "    for epoch in range(epochs):\n",
    "        pred_y=np.zeros((train_X.shape[0], train_X.shape[1]), np.int16)\n",
    "        offset = range(0, train_X.shape[0], batch_size)\n",
    "        i_th = 0\n",
    "        results = []\n",
    "        for batch in batch_generator(train_X, train_y, batch_size, crf=crf):\n",
    "            batch_train_X, batch_train_y, batch_train_X_len, batch_train_X_mask=batch\n",
    "            \n",
    "            #最开始的时候loss变化很大，等稳定了再加入，而且太慢了。。。。。。\n",
    "            if add_num_loss and epoch >= 250:\n",
    "                results_num = []\n",
    "                #先按照之前的格式输出每个batch，然后识别出预测的方面，再组成csv格式\n",
    "                model.eval()\n",
    "                batch_pred_y=model(batch_train_X, batch_train_X_len, batch_train_X_mask, testing=True)\n",
    "                model.train()\n",
    "                \n",
    "                for j_th in range(len(batch_train_X)):\n",
    "                    result = []\n",
    "                    #需要把tensor转成np\n",
    "                    words_num = [i for i in batch_train_X[j_th].cpu().numpy()]\n",
    "                    words_str = []\n",
    "                    #把数字转换成单词\n",
    "                    for w in words_num:\n",
    "                        if(w != 0):\n",
    "                            words_str.append(idx_word[w])\n",
    "                    #需要把tensor转成np\n",
    "                    gold = [i for i in batch_train_y[j_th].cpu().numpy()]\n",
    "                    #预测的原始原始输出需要处理\n",
    "                    pred = batch_pred_y[j_th][0]\n",
    "                    for words_str, gold, pred in zip(words_str, gold, pred):\n",
    "                        result.append(\" \".join([words_str, str(gold), str(pred)]))\n",
    "                    results_num.append(result)\n",
    "                save_batch(results_num)\n",
    "                #从原始输出中找出提取的方面\n",
    "                pred_aspects_nums = generate_aspects_nums_batch_data()\n",
    "                total_pred_aspects_nums = 0\n",
    "                #print('len of pred_aspects_nums', len(pred_aspects_nums))\n",
    "                #print('pred_aspects_nums', pred_aspects_nums)\n",
    "                \n",
    "                for i in pred_aspects_nums:\n",
    "                    total_pred_aspects_nums += i\n",
    "                if total_pred_aspects_nums != 0:\n",
    "                    json_to_csv()\n",
    "                    #调用bert进行预测，返回预测的每个句子的结果\n",
    "                    predict_aspects_nums()\n",
    "                    predicted_nums = return_predicted_nums()\n",
    "                    num_loss = 0\n",
    "                    not_zero_pred_aspects_nums = []\n",
    "                    for i in range(len(pred_aspects_nums)):\n",
    "                        if pred_aspects_nums[i] != 0:\n",
    "                            not_zero_pred_aspects_nums.append(pred_aspects_nums[i])\n",
    "                    \"\"\"\n",
    "                    print('len of not_zero_pred_aspects_nums', len(not_zero_pred_aspects_nums))\n",
    "                    print('not_zero_pred_aspects_nums', not_zero_pred_aspects_nums)\n",
    "                    print('len of predicted_nums', len(predicted_nums))\n",
    "                    print('predicted_nums', predicted_nums)\n",
    "                    if len(not_zero_pred_aspects_nums) != len(predicted_nums):\n",
    "                        print('len of not_zero_pred_aspects_nums', len(not_zero_pred_aspects_nums))\n",
    "                        print('not_zero_pred_aspects_nums', not_zero_pred_aspects_nums)\n",
    "                        print('len of predicted_nums', len(predicted_nums))\n",
    "                        print('predicted_nums', predicted_nums)\n",
    "                        assert len(not_zero_pred_aspects_nums) == len(predicted_nums)\n",
    "                    \"\"\"\n",
    "                    #存在一个BUG--predicted_nums和not_zero_pred_aspects_nums长度可能不等\n",
    "                    for i in range(len(predicted_nums)):\n",
    "                        num_loss += (predicted_nums[i] - not_zero_pred_aspects_nums[i]) ** 2\n",
    "                    num_loss = (num_loss / len(predicted_nums)) * 10\n",
    "                    print('bert计算的loss是：', num_loss)\n",
    "                else:\n",
    "                    #因为所有的句子长度为83\n",
    "                    num_loss = 83\n",
    "                #生成bert需要的输入格式\n",
    "                #clean_batch('output_data/batch_result.utf8')\n",
    "                #clean_batch('output_data/batch_result.json')\n",
    "                #clean_batch('output_data/batch_result.csv')\n",
    "                \n",
    "            #开始训练\n",
    "            #这里的model是Model里面的forward函数\n",
    "            if add_num_loss and epoch >= 250:\n",
    "                loss=model(batch_train_X, batch_train_X_len, batch_train_X_mask, batch_train_y) + num_loss\n",
    "                print('loss是：', loss)\n",
    "            else:\n",
    "                loss=model(batch_train_X, batch_train_X_len, batch_train_X_mask, batch_train_y)\n",
    "            #print('loss是：', loss)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm(parameters, 1.)\n",
    "            optimizer.step()\n",
    "            \n",
    "            if generate_data:\n",
    "                \"\"\"\n",
    "                输出预测标签\n",
    "                作用：输出每行句子的每个单词的预测标签\n",
    "                \"\"\"\n",
    "                model.eval()\n",
    "                batch_print_X_len=np.sum(train_X[offset[i_th]:offset[i_th]+batch_size]!=0, axis=1)\n",
    "                batch_idx=batch_print_X_len.argsort()[::-1]\n",
    "                batch_print_X_len=batch_print_X_len[batch_idx]\n",
    "                batch_print_X_mask=(train_X[offset[i_th]:offset[i_th]+batch_size]!=0)[batch_idx].astype(np.uint8)\n",
    "                batch_print_X=train_X[offset[i_th]:offset[i_th]+batch_size][batch_idx]\n",
    "                batch_print_X_mask=torch.autograd.Variable(torch.from_numpy(batch_print_X_mask).long().cuda())\n",
    "                batch_print_X=torch.autograd.Variable(torch.from_numpy(batch_print_X).long().cuda())\n",
    "                batch_pred_y=model(batch_print_X, batch_print_X_len, batch_print_X_mask, testing=True)\n",
    "                r_idx=batch_idx.argsort()\n",
    "                if crf:\n",
    "                    batch_pred_y=[batch_pred_y[idx] for idx in r_idx]\n",
    "                    for ix in range(len(batch_pred_y)):\n",
    "                        for jx in range(len(batch_pred_y[ix][0])):\n",
    "                            pred_y[offset[i_th]+ix,jx]=batch_pred_y[ix][0][jx]\n",
    "                else:\n",
    "                    batch_pred_y=batch_pred_y.data.cpu().numpy().argmax(axis=2)[r_idx]\n",
    "                    pred_y[offset[i_th]:offset[i_th]+batch_size,:batch_pred_y.shape[1]]=batch_pred_y\n",
    "                model.train()\n",
    "                i_th += 1\n",
    "            \n",
    "        if generate_data:\n",
    "            #写入句子、真实标签、预测标签\n",
    "            #要求：一行句子一行句子地读，然后一个单词一个单词地写，每个句子读完后以回车标志结束\n",
    "            for j_th in range(len(train_X)):\n",
    "                result = []\n",
    "                words_num = train_X[j_th]\n",
    "                words_str = []\n",
    "                #把数字转换成单词\n",
    "                for w in words_num:\n",
    "                    if(w != 0):\n",
    "                        words_str.append(idx_word[w])\n",
    "                gold = train_y[j_th]\n",
    "                pred = pred_y[j_th]\n",
    "                for words_str, gold, pred in zip(words_str, gold, pred):\n",
    "                    result.append(\" \".join([words_str, str(gold), str(pred)]))\n",
    "                results.append(result)\n",
    "            train_as(results, epoch, run_epoch)\n",
    "\n",
    "            context, query, answer, answer_start_index = load_data(epoch, run_epoch)\n",
    "            update_index = generate_start_index(context, answer_start_index, answer)\n",
    "            generate_repositioning_data(context, query, answer, update_index, epoch, run_epoch)\n",
    "            generate_aspects_nums_train_data(run_epoch, epoch)\n",
    "            \"\"\"\n",
    "            输出预测标签结束\n",
    "            \"\"\"\n",
    "        \n",
    "        #一轮训练完了，计算loss\n",
    "        loss=valid_loss(model, train_X, train_y, crf=crf)\n",
    "        train_history.append(loss)\n",
    "        loss=valid_loss(model, valid_X, valid_y, crf=crf)\n",
    "        valid_history.append(loss)\n",
    "        if loss<best_loss:\n",
    "            best_loss=loss\n",
    "            torch.save(model, model_fn)\n",
    "        shuffle_idx=np.random.permutation(len(train_X))\n",
    "        train_X=train_X[shuffle_idx]\n",
    "        train_y=train_y[shuffle_idx]\n",
    "        if(epoch % 10 == 0):\n",
    "            print(str(epoch) + '/' + str(epochs))\n",
    "    model=torch.load(model_fn)\n",
    "    return train_history, valid_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-31T11:22:48.205747Z",
     "start_time": "2020-12-31T11:22:48.201359Z"
    }
   },
   "outputs": [],
   "source": [
    "def run(domain, data_dir, model_dir, valid_split, runs, epochs, lr, dropout, batch_size, crf, generate_data, add_num_loss):\n",
    "    #2个单词嵌入\n",
    "    gen_emb=np.load(data_dir+\"gen.vec.npy\")\n",
    "    domain_emb=np.load(data_dir+domain+\"_emb.vec.npy\")\n",
    "    #训练集\n",
    "    ae_data=np.load(data_dir+domain+\".npz\")\n",
    "    \n",
    "    #分割训练集，此处是固定分割\n",
    "    valid_X=ae_data['train_X'][-valid_split:]\n",
    "    valid_y=ae_data['train_y'][-valid_split:]\n",
    "    train_X=ae_data['train_X'][:-valid_split]\n",
    "    train_y=ae_data['train_y'][:-valid_split]\n",
    "\n",
    "    print(\"数据集总大小：\", len(ae_data['train_X']))\n",
    "    print(\"训练集大小：\", len(train_X))\n",
    "    print(\"验证集大小：\", len(valid_X))\n",
    "\n",
    "    if add_num_loss:\n",
    "        model_dir = 'model_num/'\n",
    "    \n",
    "    for r in range(runs):\n",
    "        print('正在训练第 ' + str(r + 1) + '轮')\n",
    "        #初始化model\n",
    "        model=Model(gen_emb, domain_emb, 3, dropout, crf)\n",
    "        model.cuda()\n",
    "        print(model)\n",
    "        parameters = [p for p in model.parameters() if p.requires_grad]\n",
    "        optimizer=torch.optim.Adam(parameters, lr=lr)\n",
    "        #调用train模块\n",
    "        train_history, valid_history=train(train_X, train_y, valid_X, valid_y, model, model_dir+domain+str(r), \n",
    "                                           optimizer, parameters, r, epochs, batch_size, crf, generate_data, add_num_loss)\n",
    "        if generate_data:\n",
    "            merge_data(r, epochs)\n",
    "            merge_aspects_nums_data(r, epochs)\n",
    "    if generate_data:\n",
    "        merge_data_final(runs, epochs)\n",
    "        merge_aspects_nums_final_data(runs, epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-31T17:35:14.770202Z",
     "start_time": "2020-12-31T11:22:48.206538Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "数据集总大小： 3045\n",
      "训练集大小： 2895\n",
      "验证集大小： 150\n",
      "正在训练第 1轮\n",
      "Model(\n",
      "  (gen_embedding): Embedding(8518, 300)\n",
      "  (conv1): Conv1d(300, 128, kernel_size=(5,), stride=(1,), padding=(2,))\n",
      "  (conv2): Conv1d(300, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "  (dropout): Dropout(p=0.55, inplace=False)\n",
      "  (conv3): Conv1d(256, 256, kernel_size=(5,), stride=(1,), padding=(2,))\n",
      "  (conv4): Conv1d(256, 256, kernel_size=(5,), stride=(1,), padding=(2,))\n",
      "  (conv5): Conv1d(256, 256, kernel_size=(5,), stride=(1,), padding=(2,))\n",
      "  (linear_ae): Linear(in_features=256, out_features=3, bias=True)\n",
      "  (crf): ConditionalRandomField()\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xinke901/anaconda3/envs/de-cnn/lib/python3.7/site-packages/ipykernel_launcher.py:90: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/300\n",
      "10/300\n",
      "20/300\n",
      "30/300\n",
      "40/300\n",
      "50/300\n",
      "60/300\n",
      "70/300\n",
      "80/300\n",
      "90/300\n",
      "100/300\n",
      "110/300\n",
      "120/300\n",
      "130/300\n",
      "140/300\n",
      "150/300\n",
      "160/300\n",
      "170/300\n",
      "180/300\n",
      "190/300\n",
      "200/300\n",
      "210/300\n",
      "220/300\n",
      "230/300\n",
      "240/300\n",
      "bert计算的loss是： 36.271186440677965\n",
      "loss是： tensor(83.1781, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 31.52542372881356\n",
      "loss是： tensor(98.1197, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 37.68115942028985\n",
      "loss是： tensor(111.1966, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 34.14285714285714\n",
      "loss是： tensor(99.3063, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 30.3125\n",
      "loss是： tensor(98.4859, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 33.333333333333336\n",
      "loss是： tensor(83.0194, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 37.84615384615385\n",
      "loss是： tensor(109.2029, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 37.77777777777778\n",
      "loss是： tensor(106.9251, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 33.333333333333336\n",
      "loss是： tensor(84.4525, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 29.1044776119403\n",
      "loss是： tensor(104.0609, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 68.7719298245614\n",
      "loss是： tensor(130.5295, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 24.067796610169488\n",
      "loss是： tensor(81.0980, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 27.21311475409836\n",
      "loss是： tensor(102.4058, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 32.34375\n",
      "loss是： tensor(83.1533, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 28.333333333333336\n",
      "loss是： tensor(88.0806, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 34.21875\n",
      "loss是： tensor(93.6474, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 19.11764705882353\n",
      "loss是： tensor(70.0603, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 39.310344827586206\n",
      "loss是： tensor(88.3553, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 23.77049180327869\n",
      "loss是： tensor(72.5697, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 18.360655737704917\n",
      "loss是： tensor(87.8668, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 30.344827586206897\n",
      "loss是： tensor(81.7345, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 33.07692307692307\n",
      "loss是： tensor(104.1715, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 35.95744680851064\n",
      "loss是： tensor(85.3207, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "250/300\n",
      "bert计算的loss是： 21.52542372881356\n",
      "loss是： tensor(67.0698, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 35.6140350877193\n",
      "loss是： tensor(95.5895, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 38.57142857142857\n",
      "loss是： tensor(105.3297, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 29.076923076923077\n",
      "loss是： tensor(89.6612, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 51.875\n",
      "loss是： tensor(104.8052, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 23.703703703703702\n",
      "loss是： tensor(76.3980, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 33.421052631578945\n",
      "loss是： tensor(111.3737, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 30.370370370370374\n",
      "loss是： tensor(122.7145, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 33.8235294117647\n",
      "loss是： tensor(109.3484, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 48.4375\n",
      "loss是： tensor(118.7085, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 36.774193548387096\n",
      "loss是： tensor(76.3514, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 32.12121212121212\n",
      "loss是： tensor(76.6802, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 33.10344827586207\n",
      "loss是： tensor(99.6303, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 23.58490566037736\n",
      "loss是： tensor(81.1749, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 39.86486486486486\n",
      "loss是： tensor(127.8077, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 26.176470588235293\n",
      "loss是： tensor(92.3014, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 29.48275862068965\n",
      "loss是： tensor(96.4593, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 39.38461538461539\n",
      "loss是： tensor(95.3503, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 35.230769230769226\n",
      "loss是： tensor(124.9604, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 31.228070175438596\n",
      "loss是： tensor(82.1751, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 35.0\n",
      "loss是： tensor(110.9347, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 30.428571428571427\n",
      "loss是： tensor(121.1268, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 53.5\n",
      "loss是： tensor(119.1214, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 33.768115942028984\n",
      "loss是： tensor(119.4939, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 47.12121212121212\n",
      "loss是： tensor(119.0725, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 34.21052631578947\n",
      "loss是： tensor(130.5227, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 31.911764705882355\n",
      "loss是： tensor(107.3341, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 22.65625\n",
      "loss是： tensor(96.9315, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 35.90909090909091\n",
      "loss是： tensor(96.7197, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 20.476190476190474\n",
      "loss是： tensor(84.3318, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 27.903225806451616\n",
      "loss是： tensor(87.4290, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 35.6140350877193\n",
      "loss是： tensor(83.0389, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 60.892857142857146\n",
      "loss是： tensor(105.0083, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 26.481481481481485\n",
      "loss是： tensor(119.1679, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 41.32352941176471\n",
      "loss是： tensor(125.2601, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 21.5\n",
      "loss是： tensor(83.5802, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 30.133333333333333\n",
      "loss是： tensor(105.8311, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 30.0\n",
      "loss是： tensor(114.6344, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 32.73972602739726\n",
      "loss是： tensor(111.1201, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 51.92982456140351\n",
      "loss是： tensor(113.5046, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 42.09677419354839\n",
      "loss是： tensor(84.6496, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 29.166666666666664\n",
      "loss是： tensor(110.2842, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 16.94915254237288\n",
      "loss是： tensor(70.4444, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 32.41935483870967\n",
      "loss是： tensor(130.3882, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 29.48275862068965\n",
      "loss是： tensor(81.5365, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 29.23076923076923\n",
      "loss是： tensor(62.1994, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 27.166666666666668\n",
      "loss是： tensor(100.4063, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 42.714285714285715\n",
      "loss是： tensor(86.8484, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 40.410958904109584\n",
      "loss是： tensor(108.5658, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 37.2463768115942\n",
      "loss是： tensor(93.9638, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 26.274509803921568\n",
      "loss是： tensor(80.2309, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 32.45614035087719\n",
      "loss是： tensor(88.6038, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 34.84848484848485\n",
      "loss是： tensor(100.4101, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 40.0\n",
      "loss是： tensor(124.7589, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 36.61290322580645\n",
      "loss是： tensor(113.5399, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 29.285714285714285\n",
      "loss是： tensor(71.2852, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 7.323943661971831\n",
      "loss是： tensor(73.2288, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 35.16129032258065\n",
      "loss是： tensor(110.1751, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 26.923076923076927\n",
      "loss是： tensor(82.3277, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 20.526315789473685\n",
      "loss是： tensor(93.6402, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 36.37931034482759\n",
      "loss是： tensor(81.6416, device='cuda:0', grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert计算的loss是： 44.39393939393939\n",
      "loss是： tensor(113.6517, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 5.737704918032787\n",
      "loss是： tensor(57.5522, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 44.34210526315789\n",
      "loss是： tensor(120.6564, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 28.301886792452827\n",
      "loss是： tensor(86.9473, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 31.61764705882353\n",
      "loss是： tensor(82.0013, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 27.352941176470587\n",
      "loss是： tensor(105.8284, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 11.35593220338983\n",
      "loss是： tensor(68.5929, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 34.6875\n",
      "loss是： tensor(55.8830, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 33.07692307692307\n",
      "loss是： tensor(102.0371, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 38.771929824561404\n",
      "loss是： tensor(114.3759, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 25.0\n",
      "loss是： tensor(89.5088, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 63.09859154929577\n",
      "loss是： tensor(122.1895, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 28.153846153846157\n",
      "loss是： tensor(91.3956, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 33.50877192982456\n",
      "loss是： tensor(86.6202, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 40.882352941176464\n",
      "loss是： tensor(107.9860, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 28.153846153846157\n",
      "loss是： tensor(70.2033, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 20.491803278688522\n",
      "loss是： tensor(64.1438, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 27.115384615384617\n",
      "loss是： tensor(82.3360, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 39.827586206896555\n",
      "loss是： tensor(93.3186, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 29.322033898305087\n",
      "loss是： tensor(79.8718, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 38.5\n",
      "loss是： tensor(89.4734, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 25.36231884057971\n",
      "loss是： tensor(108.5058, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 39.701492537313435\n",
      "loss是： tensor(104.7453, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 34.339622641509436\n",
      "loss是： tensor(98.0215, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 34.85294117647059\n",
      "loss是： tensor(90.8053, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 30.57142857142857\n",
      "loss是： tensor(101.2261, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 34.266666666666666\n",
      "loss是： tensor(83.2403, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 35.19230769230769\n",
      "loss是： tensor(123.5014, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 44.44444444444444\n",
      "loss是： tensor(100.2513, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 25.238095238095237\n",
      "loss是： tensor(71.4361, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 29.375\n",
      "loss是： tensor(70.0819, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 30.0\n",
      "loss是： tensor(85.2523, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 36.12903225806451\n",
      "loss是： tensor(87.3411, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 46.166666666666664\n",
      "loss是： tensor(99.7296, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 34.583333333333336\n",
      "loss是： tensor(117.0513, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 35.46875\n",
      "loss是： tensor(103.5461, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 27.323943661971832\n",
      "loss是： tensor(137.5336, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 43.07692307692307\n",
      "loss是： tensor(103.4193, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 38.333333333333336\n",
      "loss是： tensor(89.5528, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 34.107142857142854\n",
      "loss是： tensor(96.8844, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 34.42622950819672\n",
      "loss是： tensor(112.6372, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 34.0625\n",
      "loss是： tensor(86.6506, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 43.728813559322035\n",
      "loss是： tensor(122.2489, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 9.649122807017545\n",
      "loss是： tensor(68.6343, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 28.90625\n",
      "loss是： tensor(117.9206, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 28.333333333333336\n",
      "loss是： tensor(83.0279, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 30.0\n",
      "loss是： tensor(72.1620, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 26.29032258064516\n",
      "loss是： tensor(87.3243, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 20.81967213114754\n",
      "loss是： tensor(76.5547, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 48.833333333333336\n",
      "loss是： tensor(114.3010, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 33.96825396825397\n",
      "loss是： tensor(77.4995, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 29.81818181818182\n",
      "loss是： tensor(100.4321, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 26.10169491525424\n",
      "loss是： tensor(68.5906, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 24.0\n",
      "loss是： tensor(64.0865, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 25.074626865671643\n",
      "loss是： tensor(88.4612, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 32.5\n",
      "loss是： tensor(81.0287, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 30.32258064516129\n",
      "loss是： tensor(87.6779, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 31.23076923076923\n",
      "loss是： tensor(87.2806, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 28.955223880597014\n",
      "loss是： tensor(94.2006, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 25.737704918032787\n",
      "loss是： tensor(70.4294, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 40.943396226415096\n",
      "loss是： tensor(116.3509, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 34.35483870967742\n",
      "loss是： tensor(98.4174, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 35.43859649122807\n",
      "loss是： tensor(100.2023, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 28.928571428571427\n",
      "loss是： tensor(70.3258, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 28.529411764705884\n",
      "loss是： tensor(90.3797, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 40.0\n",
      "loss是： tensor(103.4081, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 39.0625\n",
      "loss是： tensor(104.3435, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 27.34375\n",
      "loss是： tensor(93.7183, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 27.413793103448274\n",
      "loss是： tensor(101.7058, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 29.23076923076923\n",
      "loss是： tensor(89.5098, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 38.24324324324324\n",
      "loss是： tensor(91.9011, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 45.079365079365076\n",
      "loss是： tensor(124.1877, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 64.70588235294117\n",
      "loss是： tensor(125.8419, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 26.379310344827584\n",
      "loss是： tensor(79.1259, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 31.666666666666664\n",
      "loss是： tensor(87.3691, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 24.363636363636367\n",
      "loss是： tensor(70.8515, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 36.666666666666664\n",
      "loss是： tensor(85.3913, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 23.650793650793652\n",
      "loss是： tensor(99.6225, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 27.462686567164177\n",
      "loss是： tensor(76.5649, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 33.93939393939394\n",
      "loss是： tensor(85.7482, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 41.724137931034484\n",
      "loss是： tensor(83.8113, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 25.294117647058822\n",
      "loss是： tensor(90.7762, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 31.724137931034484\n",
      "loss是： tensor(97.4351, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 4.909090909090909\n",
      "loss是： tensor(56.8568, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 63.793103448275865\n",
      "loss是： tensor(114.7046, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 34.33333333333333\n",
      "loss是： tensor(89.2344, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 32.08955223880597\n",
      "loss是： tensor(118.5453, device='cuda:0', grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert计算的loss是： 38.10344827586207\n",
      "loss是： tensor(96.1845, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 30.0\n",
      "loss是： tensor(72.6549, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 29.420289855072465\n",
      "loss是： tensor(111.7428, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 32.36363636363636\n",
      "loss是： tensor(96.5018, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 25.074626865671643\n",
      "loss是： tensor(104.8165, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 37.5\n",
      "loss是： tensor(95.8905, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 32.666666666666664\n",
      "loss是： tensor(76.1473, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 34.06779661016949\n",
      "loss是： tensor(87.3670, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 43.52112676056338\n",
      "loss是： tensor(93.2779, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 15.714285714285714\n",
      "loss是： tensor(73.7798, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 14.571428571428571\n",
      "loss是： tensor(57.3866, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 14.696969696969697\n",
      "loss是： tensor(74.1264, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 41.73913043478261\n",
      "loss是： tensor(81.5953, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 38.064516129032256\n",
      "loss是： tensor(105.0489, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 64.42622950819671\n",
      "loss是： tensor(130.3178, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 25.901639344262296\n",
      "loss是： tensor(66.7013, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 35.87301587301587\n",
      "loss是： tensor(88.7549, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 36.30769230769231\n",
      "loss是： tensor(124.1066, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 42.131147540983605\n",
      "loss是： tensor(106.6790, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 31.451612903225804\n",
      "loss是： tensor(99.5110, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 29.516129032258064\n",
      "loss是： tensor(98.4412, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 28.305084745762713\n",
      "loss是： tensor(63.6767, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 33.91304347826087\n",
      "loss是： tensor(104.8657, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 37.16666666666667\n",
      "loss是： tensor(86.0532, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 35.714285714285715\n",
      "loss是： tensor(97.5615, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 34.13333333333333\n",
      "loss是： tensor(103.8775, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 17.82608695652174\n",
      "loss是： tensor(74.2047, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 38.61538461538461\n",
      "loss是： tensor(100.6084, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 8.448275862068966\n",
      "loss是： tensor(54.5065, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 2.0689655172413794\n",
      "loss是： tensor(51.6431, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 31.428571428571427\n",
      "loss是： tensor(78.7196, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 27.586206896551726\n",
      "loss是： tensor(76.1917, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 20.483870967741936\n",
      "loss是： tensor(68.3169, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 40.3125\n",
      "loss是： tensor(115.3411, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 36.857142857142854\n",
      "loss是： tensor(91.3372, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 34.31818181818181\n",
      "loss是： tensor(80.0112, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 40.0\n",
      "loss是： tensor(100.7034, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 19.375\n",
      "loss是： tensor(75.8304, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 24.347826086956523\n",
      "loss是： tensor(119.6364, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 59.833333333333336\n",
      "loss是： tensor(114.6143, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 34.32432432432432\n",
      "loss是： tensor(90.6762, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 33.275862068965516\n",
      "loss是： tensor(108.5129, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 30.32258064516129\n",
      "loss是： tensor(86.0056, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 26.842105263157894\n",
      "loss是： tensor(81.7983, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 31.791044776119403\n",
      "loss是： tensor(100.6062, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 36.885245901639344\n",
      "loss是： tensor(91.9350, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 42.65625\n",
      "loss是： tensor(103.1329, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 39.35483870967742\n",
      "loss是： tensor(103.0807, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 23.958333333333336\n",
      "loss是： tensor(64.6108, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 23.96551724137931\n",
      "loss是： tensor(65.7671, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 36.610169491525426\n",
      "loss是： tensor(125.2581, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 31.71428571428571\n",
      "loss是： tensor(98.3034, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 37.64705882352941\n",
      "loss是： tensor(88.5050, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 26.619718309859156\n",
      "loss是： tensor(91.4177, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 35.66666666666667\n",
      "loss是： tensor(97.5481, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 35.23809523809524\n",
      "loss是： tensor(90.4126, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 24.666666666666668\n",
      "loss是： tensor(96.5179, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 27.301587301587304\n",
      "loss是： tensor(91.4022, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 38.25\n",
      "loss是： tensor(89.0107, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 27.5\n",
      "loss是： tensor(77.2836, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 36.774193548387096\n",
      "loss是： tensor(107.5299, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 40.317460317460316\n",
      "loss是： tensor(99.4272, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 29.55223880597015\n",
      "loss是： tensor(95.8939, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 35.23809523809524\n",
      "loss是： tensor(109.9402, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 24.754098360655735\n",
      "loss是： tensor(69.1169, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 33.77049180327869\n",
      "loss是： tensor(90.0355, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 21.56862745098039\n",
      "loss是： tensor(56.1382, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 36.714285714285715\n",
      "loss是： tensor(106.5490, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 41.69230769230769\n",
      "loss是： tensor(116.3758, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 29.26470588235294\n",
      "loss是： tensor(91.6357, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 35.54054054054054\n",
      "loss是： tensor(104.7815, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 33.75\n",
      "loss是： tensor(110.7125, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 34.90909090909091\n",
      "loss是： tensor(97.7286, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 26.40625\n",
      "loss是： tensor(75.8266, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 46.22641509433962\n",
      "loss是： tensor(84.5284, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 30.344827586206897\n",
      "loss是： tensor(83.6492, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 58.245614035087726\n",
      "loss是： tensor(127.1917, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 29.558823529411768\n",
      "loss是： tensor(122.3060, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 44.117647058823536\n",
      "loss是： tensor(114.4042, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 31.298701298701296\n",
      "loss是： tensor(95.8826, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 29.1044776119403\n",
      "loss是： tensor(87.0880, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 33.333333333333336\n",
      "loss是： tensor(64.9880, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "260/300\n",
      "bert计算的loss是： 37.87878787878788\n",
      "loss是： tensor(101.5807, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 20.5\n",
      "loss是： tensor(73.4098, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 35.833333333333336\n",
      "loss是： tensor(83.2290, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 8.196721311475411\n",
      "loss是： tensor(79.7032, device='cuda:0', grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert计算的loss是： 41.21212121212121\n",
      "loss是： tensor(110.2995, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 29.714285714285715\n",
      "loss是： tensor(102.4692, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 8.548387096774194\n",
      "loss是： tensor(70.8489, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 35.60606060606061\n",
      "loss是： tensor(97.3765, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 23.75\n",
      "loss是： tensor(81.7994, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 14.210526315789473\n",
      "loss是： tensor(91.9696, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 34.33333333333333\n",
      "loss是： tensor(87.0280, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 33.65079365079365\n",
      "loss是： tensor(83.1721, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 34.32835820895522\n",
      "loss是： tensor(95.0290, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 32.18181818181818\n",
      "loss是： tensor(84.3285, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 4.0983606557377055\n",
      "loss是： tensor(62.5308, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 34.50704225352113\n",
      "loss是： tensor(94.5443, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 39.29824561403508\n",
      "loss是： tensor(90.9115, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 38.8235294117647\n",
      "loss是： tensor(119.6021, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 28.484848484848484\n",
      "loss是： tensor(85.8186, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 28.28125\n",
      "loss是： tensor(100.6060, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 54.13793103448276\n",
      "loss是： tensor(116.0947, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 30.0\n",
      "loss是： tensor(90.2222, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 40.26315789473684\n",
      "loss是： tensor(72.5516, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 8.333333333333334\n",
      "loss是： tensor(53.1157, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 30.833333333333336\n",
      "loss是： tensor(72.1636, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 29.310344827586206\n",
      "loss是： tensor(107.4832, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 26.363636363636363\n",
      "loss是： tensor(79.9842, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 11.857142857142858\n",
      "loss是： tensor(82.7219, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 38.93939393939394\n",
      "loss是： tensor(111.3535, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 43.43283582089552\n",
      "loss是： tensor(107.0632, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 28.059701492537314\n",
      "loss是： tensor(86.5506, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 31.818181818181817\n",
      "loss是： tensor(73.9514, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 29.666666666666668\n",
      "loss是： tensor(76.0128, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 30.151515151515152\n",
      "loss是： tensor(78.1784, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 30.46875\n",
      "loss是： tensor(79.8600, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 25.23076923076923\n",
      "loss是： tensor(71.5940, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 24.375\n",
      "loss是： tensor(78.1755, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 33.529411764705884\n",
      "loss是： tensor(105.8967, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 50.92307692307692\n",
      "loss是： tensor(119.5487, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 62.24137931034483\n",
      "loss是： tensor(146.6817, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 41.61290322580645\n",
      "loss是： tensor(96.9825, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 32.142857142857146\n",
      "loss是： tensor(71.1943, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 26.93548387096774\n",
      "loss是： tensor(86.0284, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 31.639344262295083\n",
      "loss是： tensor(99.0802, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 21.92982456140351\n",
      "loss是： tensor(67.7823, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 35.714285714285715\n",
      "loss是： tensor(61.8678, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 29.857142857142858\n",
      "loss是： tensor(101.4446, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 21.38888888888889\n",
      "loss是： tensor(65.0880, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 32.93103448275862\n",
      "loss是： tensor(107.6512, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 30.0\n",
      "loss是： tensor(92.5940, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 33.73134328358209\n",
      "loss是： tensor(107.2917, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 69.28571428571429\n",
      "loss是： tensor(143.4482, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 34.03225806451613\n",
      "loss是： tensor(76.8636, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 23.392857142857146\n",
      "loss是： tensor(55.7236, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 30.57142857142857\n",
      "loss是： tensor(103.4809, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 28.970588235294116\n",
      "loss是： tensor(88.3463, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 27.258064516129032\n",
      "loss是： tensor(80.4339, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 51.12903225806452\n",
      "loss是： tensor(106.2580, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 20.185185185185187\n",
      "loss是： tensor(84.1507, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 30.483870967741936\n",
      "loss是： tensor(70.8927, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 28.153846153846157\n",
      "loss是： tensor(78.1577, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 36.36363636363636\n",
      "loss是： tensor(76.4436, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 43.166666666666664\n",
      "loss是： tensor(89.0733, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 31.206896551724135\n",
      "loss是： tensor(75.7370, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 39.285714285714285\n",
      "loss是： tensor(82.9640, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 30.163934426229506\n",
      "loss是： tensor(87.3839, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 19.193548387096776\n",
      "loss是： tensor(83.9514, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 34.827586206896555\n",
      "loss是： tensor(81.4139, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 33.24324324324324\n",
      "loss是： tensor(65.9862, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 55.76271186440678\n",
      "loss是： tensor(114.3484, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 24.833333333333336\n",
      "loss是： tensor(72.5589, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 38.484848484848484\n",
      "loss是： tensor(101.1228, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 33.97260273972603\n",
      "loss是： tensor(113.1231, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 25.272727272727273\n",
      "loss是： tensor(78.0845, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 17.36842105263158\n",
      "loss是： tensor(78.5685, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 34.03508771929825\n",
      "loss是： tensor(66.7081, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 31.833333333333332\n",
      "loss是： tensor(81.7702, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 33.70967741935484\n",
      "loss是： tensor(112.8343, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 40.625\n",
      "loss是： tensor(102.0536, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 23.61111111111111\n",
      "loss是： tensor(80.0869, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 46.349206349206355\n",
      "loss是： tensor(107.1861, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 30.0\n",
      "loss是： tensor(92.9685, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 26.721311475409838\n",
      "loss是： tensor(75.9523, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 26.307692307692307\n",
      "loss是： tensor(96.8828, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 14.848484848484848\n",
      "loss是： tensor(67.2047, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 37.84615384615385\n",
      "loss是： tensor(83.6699, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 18.615384615384617\n",
      "loss是： tensor(68.5087, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 31.379310344827584\n",
      "loss是： tensor(63.8314, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 35.21739130434783\n",
      "loss是： tensor(104.0313, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 32.83582089552239\n",
      "loss是： tensor(92.2165, device='cuda:0', grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert计算的loss是： 36.55172413793104\n",
      "loss是： tensor(87.8144, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 39.729729729729726\n",
      "loss是： tensor(75.7928, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 48.61111111111111\n",
      "loss是： tensor(115.8215, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 56.875\n",
      "loss是： tensor(111.7659, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 28.548387096774196\n",
      "loss是： tensor(78.5679, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 26.615384615384613\n",
      "loss是： tensor(84.7354, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 42.98245614035088\n",
      "loss是： tensor(121.5893, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 29.71014492753623\n",
      "loss是： tensor(121.0539, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 32.5\n",
      "loss是： tensor(103.6003, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 38.113207547169814\n",
      "loss是： tensor(89.8344, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 32.83333333333333\n",
      "loss是： tensor(70.8671, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 38.644067796610166\n",
      "loss是： tensor(118.3486, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 28.309859154929576\n",
      "loss是： tensor(76.6549, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 38.113207547169814\n",
      "loss是： tensor(108.3663, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 28.615384615384617\n",
      "loss是： tensor(100.4289, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 28.088235294117645\n",
      "loss是： tensor(89.5276, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 36.885245901639344\n",
      "loss是： tensor(79.8946, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 40.54545454545455\n",
      "loss是： tensor(83.0561, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 34.21875\n",
      "loss是： tensor(95.5535, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 25.0\n",
      "loss是： tensor(100.5708, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 32.459016393442624\n",
      "loss是： tensor(83.9399, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 29.833333333333336\n",
      "loss是： tensor(87.5128, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 30.8955223880597\n",
      "loss是： tensor(101.5100, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 33.064516129032256\n",
      "loss是： tensor(82.4365, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 29.11111111111111\n",
      "loss是： tensor(56.1556, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 27.16417910447761\n",
      "loss是： tensor(82.1217, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 27.53846153846154\n",
      "loss是： tensor(95.1059, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 51.53846153846154\n",
      "loss是： tensor(127.2273, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 29.23076923076923\n",
      "loss是： tensor(107.0264, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 41.53846153846154\n",
      "loss是： tensor(93.9694, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 44.375\n",
      "loss是： tensor(114.0028, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 31.076923076923077\n",
      "loss是： tensor(82.3236, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 31.153846153846153\n",
      "loss是： tensor(77.0723, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 32.42424242424242\n",
      "loss是： tensor(77.0036, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 13.442622950819672\n",
      "loss是： tensor(78.4406, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 36.557377049180324\n",
      "loss是： tensor(105.7963, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 33.333333333333336\n",
      "loss是： tensor(94.0895, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 33.333333333333336\n",
      "loss是： tensor(98.2427, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 36.29032258064516\n",
      "loss是： tensor(114.8158, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 12.985074626865671\n",
      "loss是： tensor(56.6401, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 24.242424242424242\n",
      "loss是： tensor(86.1508, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 18.571428571428573\n",
      "loss是： tensor(86.4801, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 26.545454545454547\n",
      "loss是： tensor(66.8705, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 33.80952380952381\n",
      "loss是： tensor(84.4721, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 24.516129032258064\n",
      "loss是： tensor(86.0953, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 28.125\n",
      "loss是： tensor(111.6909, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 33.278688524590166\n",
      "loss是： tensor(84.0987, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 33.513513513513516\n",
      "loss是： tensor(78.8050, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 21.5\n",
      "loss是： tensor(77.6377, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 42.5\n",
      "loss是： tensor(92.3964, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 35.44117647058823\n",
      "loss是： tensor(126.5233, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 33.07692307692307\n",
      "loss是： tensor(96.3448, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 23.81818181818182\n",
      "loss是： tensor(55.5328, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 33.84615384615385\n",
      "loss是： tensor(77.1724, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 32.43589743589744\n",
      "loss是： tensor(112.4336, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 30.857142857142858\n",
      "loss是： tensor(85.8864, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 22.121212121212118\n",
      "loss是： tensor(74.8461, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 30.508474576271183\n",
      "loss是： tensor(77.2085, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 27.65625\n",
      "loss是： tensor(63.2386, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 25.217391304347828\n",
      "loss是： tensor(79.7642, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 23.96825396825397\n",
      "loss是： tensor(58.9287, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 34.696969696969695\n",
      "loss是： tensor(96.9312, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 36.666666666666664\n",
      "loss是： tensor(77.8543, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 35.53846153846153\n",
      "loss是： tensor(112.6220, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 7.1875\n",
      "loss是： tensor(85.5270, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 31.551724137931036\n",
      "loss是： tensor(82.1135, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 29.64912280701754\n",
      "loss是： tensor(76.7772, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 36.25\n",
      "loss是： tensor(109.9714, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 21.964285714285715\n",
      "loss是： tensor(70.0963, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 62.698412698412696\n",
      "loss是： tensor(164.5663, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 23.75\n",
      "loss是： tensor(67.7243, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 37.301587301587304\n",
      "loss是： tensor(105.0473, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 58.95522388059702\n",
      "loss是： tensor(125.9072, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 26.26865671641791\n",
      "loss是： tensor(86.3949, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 23.06451612903226\n",
      "loss是： tensor(75.1606, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 24.727272727272727\n",
      "loss是： tensor(74.0487, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 25.32258064516129\n",
      "loss是： tensor(89.5479, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 36.3235294117647\n",
      "loss是： tensor(86.9083, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 43.17460317460317\n",
      "loss是： tensor(111.8754, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 36.800000000000004\n",
      "loss是： tensor(98.6592, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 34.25925925925926\n",
      "loss是： tensor(95.3363, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 30.3125\n",
      "loss是： tensor(99.3308, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 33.23076923076923\n",
      "loss是： tensor(79.2954, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 31.93548387096774\n",
      "loss是： tensor(78.4760, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 10.0\n",
      "loss是： tensor(56.1659, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 34.237288135593225\n",
      "loss是： tensor(90.0701, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 34.06779661016949\n",
      "loss是： tensor(97.5846, device='cuda:0', grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert计算的loss是： 45.0\n",
      "loss是： tensor(102.9349, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 15.862068965517242\n",
      "loss是： tensor(68.1430, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 30.0\n",
      "loss是： tensor(88.6323, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 34.0\n",
      "loss是： tensor(81.5224, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 33.278688524590166\n",
      "loss是： tensor(101.9393, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 30.147058823529413\n",
      "loss是： tensor(92.5109, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 23.93939393939394\n",
      "loss是： tensor(51.9044, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 30.163934426229506\n",
      "loss是： tensor(91.9481, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 30.833333333333336\n",
      "loss是： tensor(72.0485, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 28.47222222222222\n",
      "loss是： tensor(100.3100, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 15.074626865671641\n",
      "loss是： tensor(82.9270, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 35.303030303030305\n",
      "loss是： tensor(92.9872, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 27.796610169491526\n",
      "loss是： tensor(84.1718, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 31.372549019607842\n",
      "loss是： tensor(90.6617, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 29.545454545454547\n",
      "loss是： tensor(93.6444, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 31.967213114754102\n",
      "loss是： tensor(103.7059, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 26.984126984126988\n",
      "loss是： tensor(62.6422, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 10.0\n",
      "loss是： tensor(72.4665, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 36.0\n",
      "loss是： tensor(104.0481, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 37.916666666666664\n",
      "loss是： tensor(88.2242, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 31.5625\n",
      "loss是： tensor(80.3761, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 45.73529411764706\n",
      "loss是： tensor(118.8093, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 63.934426229508205\n",
      "loss是： tensor(144.8376, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 36.30769230769231\n",
      "loss是： tensor(92.6116, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 35.38461538461539\n",
      "loss是： tensor(105.9038, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 25.967741935483872\n",
      "loss是： tensor(90.4748, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 43.97058823529412\n",
      "loss是： tensor(115.3457, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 25.344827586206897\n",
      "loss是： tensor(64.7198, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 25.857142857142858\n",
      "loss是： tensor(92.9473, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 5.588235294117647\n",
      "loss是： tensor(33.7742, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 33.0\n",
      "loss是： tensor(86.8256, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 28.03030303030303\n",
      "loss是： tensor(71.2242, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 21.846153846153843\n",
      "loss是： tensor(98.5170, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 62.08333333333333\n",
      "loss是： tensor(136.4431, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 10.161290322580644\n",
      "loss是： tensor(91.8173, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 36.45161290322581\n",
      "loss是： tensor(100.0317, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 34.42622950819672\n",
      "loss是： tensor(98.4264, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 27.678571428571427\n",
      "loss是： tensor(96.8017, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 37.58064516129033\n",
      "loss是： tensor(82.5132, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 18.35820895522388\n",
      "loss是： tensor(77.9995, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 26.96969696969697\n",
      "loss是： tensor(73.7025, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 9.538461538461538\n",
      "loss是： tensor(74.8566, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 22.5\n",
      "loss是： tensor(74.6447, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 11.020408163265305\n",
      "loss是： tensor(49.4365, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 36.901408450704224\n",
      "loss是： tensor(109.7365, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 44.83870967741935\n",
      "loss是： tensor(89.1219, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 23.75\n",
      "loss是： tensor(85.2331, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 44.66666666666667\n",
      "loss是： tensor(123.4820, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 30.29850746268657\n",
      "loss是： tensor(85.0933, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 3.6206896551724137\n",
      "loss是： tensor(76.0335, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 25.0\n",
      "loss是： tensor(74.0457, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 39.473684210526315\n",
      "loss是： tensor(82.6956, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 33.142857142857146\n",
      "loss是： tensor(64.1739, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "270/300\n",
      "bert计算的loss是： 34.14285714285714\n",
      "loss是： tensor(85.1486, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 34.44444444444444\n",
      "loss是： tensor(100.5671, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 36.52777777777778\n",
      "loss是： tensor(98.2587, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 24.153846153846153\n",
      "loss是： tensor(93.8570, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 29.322033898305087\n",
      "loss是： tensor(74.0450, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 29.4\n",
      "loss是： tensor(72.1573, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 37.87878787878788\n",
      "loss是： tensor(89.0918, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 23.220338983050848\n",
      "loss是： tensor(87.8306, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 39.375\n",
      "loss是： tensor(113.1045, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 31.864406779661017\n",
      "loss是： tensor(90.4461, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 30.476190476190474\n",
      "loss是： tensor(92.5514, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 30.327868852459016\n",
      "loss是： tensor(121.7836, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 37.06896551724138\n",
      "loss是： tensor(76.9496, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 22.941176470588232\n",
      "loss是： tensor(83.3168, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 25.51724137931035\n",
      "loss是： tensor(72.4008, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 40.79365079365079\n",
      "loss是： tensor(87.9928, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 29.333333333333332\n",
      "loss是： tensor(62.5299, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 28.5\n",
      "loss是： tensor(76.5084, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 19.666666666666664\n",
      "loss是： tensor(77.8247, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 73.11475409836065\n",
      "loss是： tensor(117.6271, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 25.23076923076923\n",
      "loss是： tensor(96.9874, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 25.555555555555554\n",
      "loss是： tensor(82.2223, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 29.75\n",
      "loss是： tensor(65.6781, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 33.833333333333336\n",
      "loss是： tensor(70.8065, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 33.064516129032256\n",
      "loss是： tensor(101.3906, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 26.96969696969697\n",
      "loss是： tensor(75.0144, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 30.307692307692307\n",
      "loss是： tensor(92.8953, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 34.76190476190476\n",
      "loss是： tensor(98.2775, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 41.76470588235294\n",
      "loss是： tensor(95.1459, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 25.151515151515152\n",
      "loss是： tensor(99.6425, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 38.16666666666667\n",
      "loss是： tensor(98.7645, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 20.847457627118647\n",
      "loss是： tensor(90.3605, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 28.59375\n",
      "loss是： tensor(70.0446, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 27.21311475409836\n",
      "loss是： tensor(85.5635, device='cuda:0', grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert计算的loss是： 25.59322033898305\n",
      "loss是： tensor(84.2851, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 58.45070422535211\n",
      "loss是： tensor(125.3017, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 39.130434782608695\n",
      "loss是： tensor(102.7300, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 38.732394366197184\n",
      "loss是： tensor(117.5639, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 33.15789473684211\n",
      "loss是： tensor(99.2254, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 31.96078431372549\n",
      "loss是： tensor(86.1404, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 28.93939393939394\n",
      "loss是： tensor(95.6847, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 33.484848484848484\n",
      "loss是： tensor(101.0497, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 30.0\n",
      "loss是： tensor(73.1797, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 26.911764705882355\n",
      "loss是： tensor(80.6193, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 29.516129032258064\n",
      "loss是： tensor(78.1712, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 40.5\n",
      "loss是： tensor(73.9202, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 19.107142857142858\n",
      "loss是： tensor(60.6831, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 34.56140350877193\n",
      "loss是： tensor(85.5814, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 35.73770491803279\n",
      "loss是： tensor(109.1862, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 27.301587301587304\n",
      "loss是： tensor(95.7482, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 40.0\n",
      "loss是： tensor(107.7329, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 26.666666666666664\n",
      "loss是： tensor(70.9316, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 29.545454545454547\n",
      "loss是： tensor(80.4149, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 57.68115942028985\n",
      "loss是： tensor(140.3890, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 50.16129032258065\n",
      "loss是： tensor(102.6468, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 40.1639344262295\n",
      "loss是： tensor(104.4538, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 25.405405405405403\n",
      "loss是： tensor(90.9173, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 33.09090909090909\n",
      "loss是： tensor(70.5810, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 32.22222222222222\n",
      "loss是： tensor(77.6370, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 27.91044776119403\n",
      "loss是： tensor(73.7778, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 9.0625\n",
      "loss是： tensor(57.1638, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 34.920634920634924\n",
      "loss是： tensor(105.9754, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 38.852459016393446\n",
      "loss是： tensor(105.2477, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 26.5\n",
      "loss是： tensor(74.6486, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 29.67741935483871\n",
      "loss是： tensor(98.5737, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 36.60377358490566\n",
      "loss是： tensor(78.8088, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 38.87323943661972\n",
      "loss是： tensor(102.8130, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 24.166666666666664\n",
      "loss是： tensor(99.2555, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 34.146341463414636\n",
      "loss是： tensor(62.6843, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 31.071428571428573\n",
      "loss是： tensor(89.3492, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 28.799999999999997\n",
      "loss是： tensor(93.2021, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 22.73972602739726\n",
      "loss是： tensor(79.9962, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 37.16417910447761\n",
      "loss是： tensor(90.0608, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 40.0\n",
      "loss是： tensor(83.6413, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 11.5625\n",
      "loss是： tensor(64.1275, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 31.186440677966104\n",
      "loss是： tensor(84.5364, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 5.3125\n",
      "loss是： tensor(89.6415, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 31.25\n",
      "loss是： tensor(99.2668, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 9.661016949152541\n",
      "loss是： tensor(85.6844, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 33.43283582089552\n",
      "loss是： tensor(93.6077, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 11.147540983606557\n",
      "loss是： tensor(82.6087, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 24.583333333333336\n",
      "loss是： tensor(76.2079, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 11.864406779661017\n",
      "loss是： tensor(67.5632, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 16.09375\n",
      "loss是： tensor(63.3511, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 35.07692307692308\n",
      "loss是： tensor(111.9151, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 26.8\n",
      "loss是： tensor(78.9590, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 39.07692307692307\n",
      "loss是： tensor(113.5029, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 54.91525423728813\n",
      "loss是： tensor(89.7920, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 21.11111111111111\n",
      "loss是： tensor(61.0097, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 30.689655172413794\n",
      "loss是： tensor(104.6093, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 35.75757575757576\n",
      "loss是： tensor(95.4566, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 32.72727272727273\n",
      "loss是： tensor(86.8243, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 29.5\n",
      "loss是： tensor(87.5200, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 39.09090909090909\n",
      "loss是： tensor(95.5931, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 39.83050847457627\n",
      "loss是： tensor(93.7603, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 28.870967741935484\n",
      "loss是： tensor(72.0051, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 33.8235294117647\n",
      "loss是： tensor(133.2840, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 40.64516129032258\n",
      "loss是： tensor(92.1851, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 38.57142857142857\n",
      "loss是： tensor(87.4071, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 29.836065573770494\n",
      "loss是： tensor(79.2484, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 2.878787878787879\n",
      "loss是： tensor(44.0294, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 38.51851851851852\n",
      "loss是： tensor(88.7879, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 27.01492537313433\n",
      "loss是： tensor(90.8527, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 30.8955223880597\n",
      "loss是： tensor(102.1788, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 36.10169491525424\n",
      "loss是： tensor(74.3073, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 25.303030303030305\n",
      "loss是： tensor(90.7723, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 32.41935483870967\n",
      "loss是： tensor(106.4476, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 31.69230769230769\n",
      "loss是： tensor(74.0518, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 30.28985507246377\n",
      "loss是： tensor(86.2836, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 31.967213114754102\n",
      "loss是： tensor(88.3925, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 15.161290322580644\n",
      "loss是： tensor(85.7788, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 33.27272727272727\n",
      "loss是： tensor(95.8453, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 33.23076923076923\n",
      "loss是： tensor(86.9995, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 62.72727272727273\n",
      "loss是： tensor(125.7139, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 37.5609756097561\n",
      "loss是： tensor(69.3672, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 27.5\n",
      "loss是： tensor(71.9352, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 20.793650793650794\n",
      "loss是： tensor(71.1205, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 30.491803278688522\n",
      "loss是： tensor(87.3873, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 39.6875\n",
      "loss是： tensor(110.2374, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 9.107142857142858\n",
      "loss是： tensor(48.2158, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 57.74193548387097\n",
      "loss是： tensor(118.1597, device='cuda:0', grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert计算的loss是： 31.836734693877553\n",
      "loss是： tensor(71.0749, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 43.125\n",
      "loss是： tensor(108.8625, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 44.705882352941174\n",
      "loss是： tensor(115.1590, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 36.666666666666664\n",
      "loss是： tensor(92.0819, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 31.0\n",
      "loss是： tensor(99.6063, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 29.365079365079364\n",
      "loss是： tensor(84.3575, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 24.126984126984127\n",
      "loss是： tensor(87.5820, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 33.75\n",
      "loss是： tensor(71.7813, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 29.473684210526315\n",
      "loss是： tensor(57.2287, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 34.0\n",
      "loss是： tensor(113.8183, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 30.714285714285715\n",
      "loss是： tensor(97.7421, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 29.42857142857143\n",
      "loss是： tensor(74.0675, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 33.73134328358209\n",
      "loss是： tensor(113.6013, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 31.89189189189189\n",
      "loss是： tensor(79.3281, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 35.689655172413794\n",
      "loss是： tensor(91.3793, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 18.26923076923077\n",
      "loss是： tensor(71.6094, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 20.5\n",
      "loss是： tensor(55.1274, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 39.34426229508197\n",
      "loss是： tensor(88.1180, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 35.833333333333336\n",
      "loss是： tensor(74.5874, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 35.0\n",
      "loss是： tensor(90.4646, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 33.90625\n",
      "loss是： tensor(82.8425, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 32.142857142857146\n",
      "loss是： tensor(79.6862, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 27.03125\n",
      "loss是： tensor(84.3643, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 32.758620689655174\n",
      "loss是： tensor(86.6911, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 33.333333333333336\n",
      "loss是： tensor(89.1207, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 56.42857142857143\n",
      "loss是： tensor(116.9429, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 37.0\n",
      "loss是： tensor(97.3098, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 31.23076923076923\n",
      "loss是： tensor(97.7392, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 28.38709677419355\n",
      "loss是： tensor(98.3933, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 7.971014492753623\n",
      "loss是： tensor(90.1252, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 27.647058823529413\n",
      "loss是： tensor(81.9341, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 22.857142857142854\n",
      "loss是： tensor(62.5388, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 45.40983606557377\n",
      "loss是： tensor(96.4524, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 32.41935483870967\n",
      "loss是： tensor(87.6461, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 27.540983606557376\n",
      "loss是： tensor(87.6157, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 26.363636363636363\n",
      "loss是： tensor(72.5850, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 43.75\n",
      "loss是： tensor(93.4649, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 35.73770491803279\n",
      "loss是： tensor(94.6090, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 34.5\n",
      "loss是： tensor(68.2900, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 25.555555555555554\n",
      "loss是： tensor(46.3640, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 25.084745762711865\n",
      "loss是： tensor(79.5286, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 34.55882352941177\n",
      "loss是： tensor(81.2464, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 26.071428571428573\n",
      "loss是： tensor(90.5395, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 38.90625\n",
      "loss是： tensor(83.6739, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 35.0\n",
      "loss是： tensor(117.3360, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 32.25352112676057\n",
      "loss是： tensor(96.3777, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 27.611940298507463\n",
      "loss是： tensor(69.6256, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 35.625\n",
      "loss是： tensor(98.8603, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 22.0\n",
      "loss是： tensor(80.2914, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 32.83333333333333\n",
      "loss是： tensor(69.0982, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 19.34426229508197\n",
      "loss是： tensor(78.9282, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 34.166666666666664\n",
      "loss是： tensor(96.6908, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 38.54838709677419\n",
      "loss是： tensor(96.7761, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 42.53968253968254\n",
      "loss是： tensor(88.7635, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 33.125\n",
      "loss是： tensor(99.7104, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 18.571428571428573\n",
      "loss是： tensor(66.9193, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 3.125\n",
      "loss是： tensor(67.8917, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 30.923076923076923\n",
      "loss是： tensor(91.5054, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 22.98507462686567\n",
      "loss是： tensor(74.6124, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 45.0\n",
      "loss是： tensor(97.7486, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 49.82758620689655\n",
      "loss是： tensor(103.8030, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 31.0\n",
      "loss是： tensor(74.1638, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 31.5\n",
      "loss是： tensor(70.1656, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 35.07692307692308\n",
      "loss是： tensor(103.8975, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 37.25806451612903\n",
      "loss是： tensor(78.1139, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 36.61290322580645\n",
      "loss是： tensor(125.4831, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 31.818181818181817\n",
      "loss是： tensor(71.8143, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 31.96078431372549\n",
      "loss是： tensor(94.2727, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 36.37931034482759\n",
      "loss是： tensor(116.6959, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 29.696969696969695\n",
      "loss是： tensor(89.5052, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 45.0\n",
      "loss是： tensor(93.8180, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 47.41935483870968\n",
      "loss是： tensor(117.9618, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 19.193548387096776\n",
      "loss是： tensor(54.8147, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 22.676056338028168\n",
      "loss是： tensor(99.6714, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 35.73529411764706\n",
      "loss是： tensor(95.1266, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 13.5\n",
      "loss是： tensor(66.6049, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 34.03508771929825\n",
      "loss是： tensor(80.7956, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 27.82608695652174\n",
      "loss是： tensor(87.2383, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 32.41379310344827\n",
      "loss是： tensor(89.4096, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 32.83783783783784\n",
      "loss是： tensor(77.2210, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 55.67567567567568\n",
      "loss是： tensor(120.9948, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 29.193548387096776\n",
      "loss是： tensor(72.2098, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 39.6551724137931\n",
      "loss是： tensor(86.6309, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 32.0\n",
      "loss是： tensor(90.2382, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 31.147540983606557\n",
      "loss是： tensor(93.3166, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 28.461538461538463\n",
      "loss是： tensor(71.1857, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 39.25925925925926\n",
      "loss是： tensor(81.0761, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 43.46666666666667\n",
      "loss是： tensor(103.6989, device='cuda:0', grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert计算的loss是： 24.86111111111111\n",
      "loss是： tensor(84.3609, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 42.32142857142857\n",
      "loss是： tensor(97.2821, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 33.80952380952381\n",
      "loss是： tensor(98.9919, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 5.294117647058823\n",
      "loss是： tensor(60.3805, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 52.0\n",
      "loss是： tensor(122.0020, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 31.774193548387096\n",
      "loss是： tensor(80.6107, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 21.551724137931036\n",
      "loss是： tensor(63.2512, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 11.311475409836065\n",
      "loss是： tensor(71.2390, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 6.129032258064516\n",
      "loss是： tensor(69.7098, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 21.11111111111111\n",
      "loss是： tensor(55.0849, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 32.57575757575758\n",
      "loss是： tensor(70.9907, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 31.538461538461537\n",
      "loss是： tensor(93.0418, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 28.550724637681157\n",
      "loss是： tensor(110.5689, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 39.13793103448276\n",
      "loss是： tensor(109.6029, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 32.295081967213115\n",
      "loss是： tensor(128.9602, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 32.1875\n",
      "loss是： tensor(90.0137, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 31.5\n",
      "loss是： tensor(89.2164, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 38.94736842105263\n",
      "loss是： tensor(84.1793, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 29.838709677419356\n",
      "loss是： tensor(87.9127, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 29.86111111111111\n",
      "loss是： tensor(89.1866, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 24.22222222222222\n",
      "loss是： tensor(53.6204, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "280/300\n",
      "bert计算的loss是： 27.21311475409836\n",
      "loss是： tensor(106.4686, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 37.96296296296296\n",
      "loss是： tensor(103.0808, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 30.967741935483872\n",
      "loss是： tensor(81.8851, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 36.09375\n",
      "loss是： tensor(93.9627, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 33.541666666666664\n",
      "loss是： tensor(85.4307, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 47.377049180327866\n",
      "loss是： tensor(116.4653, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 30.98360655737705\n",
      "loss是： tensor(81.8874, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 24.852941176470587\n",
      "loss是： tensor(77.5969, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 29.193548387096776\n",
      "loss是： tensor(89.2769, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 40.0\n",
      "loss是： tensor(82.7093, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 31.944444444444446\n",
      "loss是： tensor(96.6645, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 51.76470588235294\n",
      "loss是： tensor(105.6940, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 21.408450704225352\n",
      "loss是： tensor(85.9602, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 32.25352112676057\n",
      "loss是： tensor(87.8364, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 8.548387096774194\n",
      "loss是： tensor(67.8727, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 34.21052631578947\n",
      "loss是： tensor(111.0379, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 35.88235294117647\n",
      "loss是： tensor(94.3757, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 33.0188679245283\n",
      "loss是： tensor(95.8689, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 26.94915254237288\n",
      "loss是： tensor(82.8141, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 18.421052631578945\n",
      "loss是： tensor(96.3913, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 38.25396825396825\n",
      "loss是： tensor(109.9274, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 32.903225806451616\n",
      "loss是： tensor(77.4092, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 41.891891891891895\n",
      "loss是： tensor(80.6054, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 39.436619718309856\n",
      "loss是： tensor(113.5480, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 21.40625\n",
      "loss是： tensor(76.5725, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 28.73015873015873\n",
      "loss是： tensor(80.7587, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 32.698412698412696\n",
      "loss是： tensor(102.9502, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 29.71014492753623\n",
      "loss是： tensor(76.8519, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 23.389830508474574\n",
      "loss是： tensor(97.2738, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 31.92982456140351\n",
      "loss是： tensor(91.1400, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 41.73076923076923\n",
      "loss是： tensor(94.0520, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 24.56140350877193\n",
      "loss是： tensor(67.2203, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 43.793103448275865\n",
      "loss是： tensor(79.6297, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 28.75\n",
      "loss是： tensor(111.9371, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 33.97260273972603\n",
      "loss是： tensor(96.2385, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 65.88235294117646\n",
      "loss是： tensor(124.5037, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 30.422535211267604\n",
      "loss是： tensor(86.0779, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 18.548387096774192\n",
      "loss是： tensor(58.1174, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 37.714285714285715\n",
      "loss是： tensor(94.0375, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 33.93939393939394\n",
      "loss是： tensor(120.5795, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 38.983050847457626\n",
      "loss是： tensor(96.6804, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 36.166666666666664\n",
      "loss是： tensor(93.6370, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 30.357142857142854\n",
      "loss是： tensor(87.7029, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 28.088235294117645\n",
      "loss是： tensor(95.9823, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 27.352941176470587\n",
      "loss是： tensor(91.8127, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 22.352941176470587\n",
      "loss是： tensor(51.3683, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 24.444444444444446\n",
      "loss是： tensor(80.8400, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 25.857142857142858\n",
      "loss是： tensor(79.7766, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 28.857142857142858\n",
      "loss是： tensor(94.5471, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 38.028169014084504\n",
      "loss是： tensor(92.7588, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 27.424242424242422\n",
      "loss是： tensor(93.9790, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 34.46153846153847\n",
      "loss是： tensor(83.4182, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 34.5\n",
      "loss是： tensor(79.7727, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 30.64516129032258\n",
      "loss是： tensor(112.8674, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 28.421052631578945\n",
      "loss是： tensor(74.5761, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 33.529411764705884\n",
      "loss是： tensor(65.8556, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 34.411764705882355\n",
      "loss是： tensor(103.7396, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 34.75409836065574\n",
      "loss是： tensor(96.4548, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 27.21311475409836\n",
      "loss是： tensor(59.3712, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 32.38805970149254\n",
      "loss是： tensor(84.5234, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 29.206349206349206\n",
      "loss是： tensor(84.8845, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 42.31884057971015\n",
      "loss是： tensor(105.3997, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 28.392857142857146\n",
      "loss是： tensor(82.5018, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 41.66666666666667\n",
      "loss是： tensor(113.2288, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 33.75\n",
      "loss是： tensor(97.6372, device='cuda:0', grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert计算的loss是： 58.36363636363636\n",
      "loss是： tensor(121.2495, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 29.807692307692307\n",
      "loss是： tensor(105.9478, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 61.40625\n",
      "loss是： tensor(107.5275, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 29.393939393939394\n",
      "loss是： tensor(57.8127, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 31.23076923076923\n",
      "loss是： tensor(100.5361, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 39.66101694915254\n",
      "loss是： tensor(92.5251, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 39.53846153846154\n",
      "loss是： tensor(95.2855, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 38.0\n",
      "loss是： tensor(119.4446, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 28.5\n",
      "loss是： tensor(71.7714, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 27.681159420289852\n",
      "loss是： tensor(84.4564, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 28.518518518518515\n",
      "loss是： tensor(77.0580, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 21.186440677966104\n",
      "loss是： tensor(75.4139, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 39.85714285714286\n",
      "loss是： tensor(111.9275, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 22.678571428571427\n",
      "loss是： tensor(71.4871, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 31.83098591549296\n",
      "loss是： tensor(100.1210, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 55.97014925373134\n",
      "loss是： tensor(112.6454, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 33.10344827586207\n",
      "loss是： tensor(87.2662, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 29.63636363636364\n",
      "loss是： tensor(67.4182, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 30.78125\n",
      "loss是： tensor(90.3440, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 51.166666666666664\n",
      "loss是： tensor(97.3439, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 37.65625\n",
      "loss是： tensor(92.1687, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 33.73134328358209\n",
      "loss是： tensor(102.9906, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 24.285714285714285\n",
      "loss是： tensor(82.9404, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 33.269230769230774\n",
      "loss是： tensor(78.4404, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 33.87096774193549\n",
      "loss是： tensor(91.0774, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 32.794117647058826\n",
      "loss是： tensor(103.8447, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 41.19047619047619\n",
      "loss是： tensor(83.8893, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 19.032258064516128\n",
      "loss是： tensor(56.4516, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 34.75409836065574\n",
      "loss是： tensor(102.1076, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 26.769230769230766\n",
      "loss是： tensor(98.5725, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 32.142857142857146\n",
      "loss是： tensor(75.8595, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 41.5625\n",
      "loss是： tensor(106.1800, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 40.882352941176464\n",
      "loss是： tensor(106.2433, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 26.911764705882355\n",
      "loss是： tensor(93.2596, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 16.94915254237288\n",
      "loss是： tensor(76.3147, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 35.96491228070175\n",
      "loss是： tensor(94.2548, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 41.904761904761905\n",
      "loss是： tensor(106.6529, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 30.317460317460316\n",
      "loss是： tensor(75.9065, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 14.489795918367347\n",
      "loss是： tensor(55.9044, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 27.01492537313433\n",
      "loss是： tensor(71.6070, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 33.857142857142854\n",
      "loss是： tensor(97.8467, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 37.76119402985075\n",
      "loss是： tensor(99.7048, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 26.071428571428573\n",
      "loss是： tensor(86.1526, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 32.67857142857143\n",
      "loss是： tensor(80.6275, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 30.13888888888889\n",
      "loss是： tensor(82.6284, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 38.4375\n",
      "loss是： tensor(86.9267, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 32.3728813559322\n",
      "loss是： tensor(95.2751, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 63.62068965517241\n",
      "loss是： tensor(108.6894, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 25.166666666666664\n",
      "loss是： tensor(79.8675, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 14.25\n",
      "loss是： tensor(49.3243, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 27.017543859649123\n",
      "loss是： tensor(82.7642, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 31.836734693877553\n",
      "loss是： tensor(80.8066, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 60.87719298245614\n",
      "loss是： tensor(101.5151, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 33.134328358208954\n",
      "loss是： tensor(110.1750, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 39.09090909090909\n",
      "loss是： tensor(96.7818, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 37.213114754098356\n",
      "loss是： tensor(103.0462, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 43.03030303030303\n",
      "loss是： tensor(119.6591, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 26.0\n",
      "loss是： tensor(79.4251, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 41.935483870967744\n",
      "loss是： tensor(96.8112, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 23.859649122807017\n",
      "loss是： tensor(67.6956, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 27.8125\n",
      "loss是： tensor(66.6743, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 32.27272727272727\n",
      "loss是： tensor(84.6106, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 36.26865671641791\n",
      "loss是： tensor(109.7823, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 35.810810810810814\n",
      "loss是： tensor(101.8448, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 31.746031746031743\n",
      "loss是： tensor(91.0906, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 9.545454545454547\n",
      "loss是： tensor(91.9384, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 29.58904109589041\n",
      "loss是： tensor(89.5412, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 3.235294117647059\n",
      "loss是： tensor(60.3011, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 35.90909090909091\n",
      "loss是： tensor(94.6262, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 31.964285714285715\n",
      "loss是： tensor(99.3989, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 41.52542372881356\n",
      "loss是： tensor(108.1010, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 28.857142857142858\n",
      "loss是： tensor(82.4557, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 34.83870967741935\n",
      "loss是： tensor(70.9056, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 4.477611940298508\n",
      "loss是： tensor(60.4573, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 31.285714285714285\n",
      "loss是： tensor(113.0945, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 36.96969696969697\n",
      "loss是： tensor(92.5896, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 22.037037037037038\n",
      "loss是： tensor(61.7223, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 35.90909090909091\n",
      "loss是： tensor(107.3509, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 36.25\n",
      "loss是： tensor(80.9863, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 45.128205128205124\n",
      "loss是： tensor(103.9057, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 34.53125\n",
      "loss是： tensor(89.9755, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 30.0\n",
      "loss是： tensor(59.7899, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 8.852459016393443\n",
      "loss是： tensor(64.0791, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 36.142857142857146\n",
      "loss是： tensor(109.9261, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 35.63636363636363\n",
      "loss是： tensor(82.3590, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 29.333333333333332\n",
      "loss是： tensor(65.8178, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 25.40983606557377\n",
      "loss是： tensor(71.0464, device='cuda:0', grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert计算的loss是： 35.3125\n",
      "loss是： tensor(90.2882, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 35.68627450980392\n",
      "loss是： tensor(77.0073, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 35.357142857142854\n",
      "loss是： tensor(84.0882, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 60.81967213114754\n",
      "loss是： tensor(129.9419, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 30.46153846153846\n",
      "loss是： tensor(91.0982, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 28.93939393939394\n",
      "loss是： tensor(89.5624, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 31.186440677966104\n",
      "loss是： tensor(93.2637, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 33.5\n",
      "loss是： tensor(91.1071, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 21.38888888888889\n",
      "loss是： tensor(44.4425, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 37.58620689655173\n",
      "loss是： tensor(90.7837, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 32.205882352941174\n",
      "loss是： tensor(88.5398, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 30.29850746268657\n",
      "loss是： tensor(86.3107, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 18.484848484848484\n",
      "loss是： tensor(59.4105, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 31.71428571428571\n",
      "loss是： tensor(85.0753, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 26.041666666666664\n",
      "loss是： tensor(68.6413, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 41.31147540983606\n",
      "loss是： tensor(101.4246, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 27.936507936507937\n",
      "loss是： tensor(75.6399, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 35.42857142857143\n",
      "loss是： tensor(117.6252, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 28.548387096774196\n",
      "loss是： tensor(93.2453, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 32.5\n",
      "loss是： tensor(63.9859, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 27.037037037037038\n",
      "loss是： tensor(62.9282, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 36.36363636363636\n",
      "loss是： tensor(103.1796, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 30.153846153846153\n",
      "loss是： tensor(76.1610, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 27.82608695652174\n",
      "loss是： tensor(84.9320, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 33.7037037037037\n",
      "loss是： tensor(94.0204, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 35.806451612903224\n",
      "loss是： tensor(89.7921, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 38.771929824561404\n",
      "loss是： tensor(94.5714, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 40.70422535211268\n",
      "loss是： tensor(106.1865, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 33.333333333333336\n",
      "loss是： tensor(96.6761, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 30.877192982456137\n",
      "loss是： tensor(85.4745, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 55.599999999999994\n",
      "loss是： tensor(114.5454, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 8.40909090909091\n",
      "loss是： tensor(57.9179, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 30.307692307692307\n",
      "loss是： tensor(105.1715, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 35.9375\n",
      "loss是： tensor(81.2896, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 42.83333333333333\n",
      "loss是： tensor(90.9962, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 34.21875\n",
      "loss是： tensor(75.7153, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 26.0\n",
      "loss是： tensor(66.1095, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 28.96551724137931\n",
      "loss是： tensor(85.3256, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 39.55882352941177\n",
      "loss是： tensor(117.1427, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 28.61111111111111\n",
      "loss是： tensor(87.2064, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 29.2\n",
      "loss是： tensor(105.7929, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 39.66101694915254\n",
      "loss是： tensor(90.2738, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 31.5625\n",
      "loss是： tensor(78.4947, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 42.98245614035088\n",
      "loss是： tensor(108.4610, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 16.296296296296294\n",
      "loss是： tensor(61.6574, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 6.984126984126984\n",
      "loss是： tensor(64.6468, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 14.615384615384615\n",
      "loss是： tensor(49.4413, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 55.588235294117645\n",
      "loss是： tensor(109.3402, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 23.529411764705884\n",
      "loss是： tensor(62.2031, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 24.6875\n",
      "loss是： tensor(77.9424, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 31.694915254237287\n",
      "loss是： tensor(91.1778, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 30.178571428571427\n",
      "loss是： tensor(80.5970, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 34.375\n",
      "loss是： tensor(97.5132, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 30.892857142857146\n",
      "loss是： tensor(61.1255, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 45.641025641025635\n",
      "loss是： tensor(87.6562, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 33.0\n",
      "loss是： tensor(79.8345, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 36.75675675675676\n",
      "loss是： tensor(89.4371, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 40.5\n",
      "loss是： tensor(80.9697, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 27.258064516129032\n",
      "loss是： tensor(80.8128, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 31.80327868852459\n",
      "loss是： tensor(89.1090, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 38.813559322033896\n",
      "loss是： tensor(102.7577, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 24.915254237288135\n",
      "loss是： tensor(86.2319, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 12.452830188679245\n",
      "loss是： tensor(74.1711, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 31.076923076923077\n",
      "loss是： tensor(103.4816, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 24.516129032258064\n",
      "loss是： tensor(59.6014, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 23.75\n",
      "loss是： tensor(75.9224, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 31.96969696969697\n",
      "loss是： tensor(93.0090, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 32.375\n",
      "loss是： tensor(90.2178, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 32.61538461538461\n",
      "loss是： tensor(113.6830, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 43.33333333333333\n",
      "loss是： tensor(93.0174, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 17.678571428571427\n",
      "loss是： tensor(57.2167, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 21.09375\n",
      "loss是： tensor(73.0951, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 35.3448275862069\n",
      "loss是： tensor(99.6744, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 46.769230769230774\n",
      "loss是： tensor(95.2652, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 31.0\n",
      "loss是： tensor(79.7480, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 33.54838709677419\n",
      "loss是： tensor(92.0714, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 34.59016393442623\n",
      "loss是： tensor(90.9085, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 29.393939393939394\n",
      "loss是： tensor(67.9629, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "290/300\n",
      "bert计算的loss是： 29.83050847457627\n",
      "loss是： tensor(71.7492, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 28.360655737704917\n",
      "loss是： tensor(90.0505, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 36.3013698630137\n",
      "loss是： tensor(97.2180, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 34.92957746478873\n",
      "loss是： tensor(95.1219, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 28.75\n",
      "loss是： tensor(81.7143, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 36.09375\n",
      "loss是： tensor(94.2927, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 35.96774193548387\n",
      "loss是： tensor(87.1168, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 40.0\n",
      "loss是： tensor(105.2506, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 44.794520547945204\n",
      "loss是： tensor(99.8643, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 32.22222222222222\n",
      "loss是： tensor(88.8861, device='cuda:0', grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert计算的loss是： 25.5\n",
      "loss是： tensor(64.8048, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 25.084745762711865\n",
      "loss是： tensor(68.2457, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 39.0\n",
      "loss是： tensor(114.6081, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 29.82758620689655\n",
      "loss是： tensor(82.6216, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 52.77777777777778\n",
      "loss是： tensor(108.0944, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 27.27272727272727\n",
      "loss是： tensor(70.8130, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 27.454545454545457\n",
      "loss是： tensor(75.9863, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 36.46153846153846\n",
      "loss是： tensor(84.6718, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 41.38888888888889\n",
      "loss是： tensor(116.7766, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 31.52777777777778\n",
      "loss是： tensor(87.4742, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 32.41935483870967\n",
      "loss是： tensor(90.7934, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 27.460317460317462\n",
      "loss是： tensor(89.2960, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 40.256410256410255\n",
      "loss是： tensor(81.7355, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 36.39344262295082\n",
      "loss是： tensor(80.2551, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 39.523809523809526\n",
      "loss是： tensor(104.4658, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 21.186440677966104\n",
      "loss是： tensor(89.6128, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 41.45454545454545\n",
      "loss是： tensor(98.5740, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 47.46268656716418\n",
      "loss是： tensor(101.9854, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 30.0\n",
      "loss是： tensor(86.1602, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 42.03125\n",
      "loss是： tensor(107.3409, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 30.0\n",
      "loss是： tensor(70.7881, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 30.806451612903224\n",
      "loss是： tensor(94.9690, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 31.5\n",
      "loss是： tensor(95.7165, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 40.54545454545455\n",
      "loss是： tensor(86.1403, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 23.666666666666668\n",
      "loss是： tensor(62.5179, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 8.275862068965518\n",
      "loss是： tensor(51.3094, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 37.2463768115942\n",
      "loss是： tensor(81.6726, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 27.8125\n",
      "loss是： tensor(78.5455, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 30.441176470588232\n",
      "loss是： tensor(83.9387, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 27.36842105263158\n",
      "loss是： tensor(76.7727, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 31.159420289855074\n",
      "loss是： tensor(79.2184, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 29.508196721311478\n",
      "loss是： tensor(90.4763, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 24.393939393939394\n",
      "loss是： tensor(69.9121, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 30.294117647058822\n",
      "loss是： tensor(91.6229, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 61.911764705882355\n",
      "loss是： tensor(116.2805, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 5.263157894736842\n",
      "loss是： tensor(29.3878, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 36.785714285714285\n",
      "loss是： tensor(80.2340, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 31.311475409836067\n",
      "loss是： tensor(80.4225, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 23.728813559322035\n",
      "loss是： tensor(61.5895, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 34.920634920634924\n",
      "loss是： tensor(95.4582, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 30.0\n",
      "loss是： tensor(80.5830, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 29.333333333333332\n",
      "loss是： tensor(74.7993, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 15.806451612903224\n",
      "loss是： tensor(77.3543, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 40.14492753623188\n",
      "loss是： tensor(113.5070, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 52.857142857142854\n",
      "loss是： tensor(94.1327, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 35.69230769230769\n",
      "loss是： tensor(108.2719, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 19.090909090909093\n",
      "loss是： tensor(60.1732, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 36.61764705882353\n",
      "loss是： tensor(105.6238, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 5.0\n",
      "loss是： tensor(62.3597, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 32.93103448275862\n",
      "loss是： tensor(105.1759, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 33.75\n",
      "loss是： tensor(103.8909, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 5.689655172413794\n",
      "loss是： tensor(58.7322, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 39.1044776119403\n",
      "loss是： tensor(112.5644, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 30.151515151515152\n",
      "loss是： tensor(59.7903, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 32.59259259259259\n",
      "loss是： tensor(73.4984, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 30.434782608695656\n",
      "loss是： tensor(75.7568, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 37.63636363636364\n",
      "loss是： tensor(76.9671, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 35.39682539682539\n",
      "loss是： tensor(102.5131, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 28.0\n",
      "loss是： tensor(63.6123, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 30.9375\n",
      "loss是： tensor(90.2315, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 25.47945205479452\n",
      "loss是： tensor(103.7480, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 33.88059701492537\n",
      "loss是： tensor(112.7924, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 31.323529411764707\n",
      "loss是： tensor(83.3183, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 37.32394366197183\n",
      "loss是： tensor(67.4965, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 31.846153846153843\n",
      "loss是： tensor(75.2477, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 34.21875\n",
      "loss是： tensor(98.9717, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 26.774193548387096\n",
      "loss是： tensor(69.7070, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 26.825396825396822\n",
      "loss是： tensor(57.3640, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 35.64516129032258\n",
      "loss是： tensor(69.3716, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 43.28125\n",
      "loss是： tensor(108.0005, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 18.94736842105263\n",
      "loss是： tensor(64.4079, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 22.424242424242422\n",
      "loss是： tensor(71.2044, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 44.33962264150943\n",
      "loss是： tensor(103.6068, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 30.877192982456137\n",
      "loss是： tensor(84.8600, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 32.80701754385965\n",
      "loss是： tensor(69.4186, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 41.714285714285715\n",
      "loss是： tensor(121.6995, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 42.698412698412696\n",
      "loss是： tensor(79.0228, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 36.5\n",
      "loss是： tensor(89.9470, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 40.3125\n",
      "loss是： tensor(98.1866, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 24.56140350877193\n",
      "loss是： tensor(54.1507, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 36.0\n",
      "loss是： tensor(80.1091, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 23.333333333333336\n",
      "loss是： tensor(45.2911, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 33.92857142857143\n",
      "loss是： tensor(74.8500, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 26.11111111111111\n",
      "loss是： tensor(82.2590, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 31.38888888888889\n",
      "loss是： tensor(67.3090, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 18.095238095238095\n",
      "loss是： tensor(77.9587, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 30.28985507246377\n",
      "loss是： tensor(94.5977, device='cuda:0', grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert计算的loss是： 28.19672131147541\n",
      "loss是： tensor(77.7369, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 37.5\n",
      "loss是： tensor(116.7274, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 27.0\n",
      "loss是： tensor(85.4936, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 33.69230769230769\n",
      "loss是： tensor(79.4017, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 12.580645161290322\n",
      "loss是： tensor(80.1784, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 28.620689655172416\n",
      "loss是： tensor(74.2699, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 32.72727272727273\n",
      "loss是： tensor(79.6352, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 32.72727272727273\n",
      "loss是： tensor(85.5757, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 35.166666666666664\n",
      "loss是： tensor(108.4663, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 29.48275862068965\n",
      "loss是： tensor(85.5776, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 36.18181818181818\n",
      "loss是： tensor(92.7444, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 31.83098591549296\n",
      "loss是： tensor(82.9973, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 32.0\n",
      "loss是： tensor(80.6842, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 37.6271186440678\n",
      "loss是： tensor(93.9107, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 37.16666666666667\n",
      "loss是： tensor(107.6344, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 45.0\n",
      "loss是： tensor(133.9178, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 42.22222222222222\n",
      "loss是： tensor(107.3165, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 29.714285714285715\n",
      "loss是： tensor(58.1794, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 34.42622950819672\n",
      "loss是： tensor(82.4036, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 37.27272727272727\n",
      "loss是： tensor(102.3942, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 23.43283582089552\n",
      "loss是： tensor(80.2395, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 31.967213114754102\n",
      "loss是： tensor(96.8468, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 31.19402985074627\n",
      "loss是： tensor(75.8184, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 17.272727272727273\n",
      "loss是： tensor(64.9312, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 38.54545454545455\n",
      "loss是： tensor(81.6369, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 32.96296296296296\n",
      "loss是： tensor(80.0066, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 32.5\n",
      "loss是： tensor(80.9865, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 36.911764705882355\n",
      "loss是： tensor(89.4379, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 33.75\n",
      "loss是： tensor(97.5455, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 23.89830508474576\n",
      "loss是： tensor(76.8223, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 40.42857142857143\n",
      "loss是： tensor(104.9339, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 30.892857142857146\n",
      "loss是： tensor(74.4397, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 38.59375\n",
      "loss是： tensor(82.6308, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 28.135593220338983\n",
      "loss是： tensor(76.7915, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 35.37313432835821\n",
      "loss是： tensor(84.4161, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 59.516129032258064\n",
      "loss是： tensor(110.7141, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 27.464788732394364\n",
      "loss是： tensor(73.6347, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 36.15384615384615\n",
      "loss是： tensor(97.9769, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 32.53968253968254\n",
      "loss是： tensor(103.6212, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 23.333333333333336\n",
      "loss是： tensor(81.3570, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 48.97435897435898\n",
      "loss是： tensor(72.4898, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 44.75409836065574\n",
      "loss是： tensor(84.6163, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 28.620689655172416\n",
      "loss是： tensor(62.4221, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 19.11764705882353\n",
      "loss是： tensor(74.7692, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 33.44827586206896\n",
      "loss是： tensor(70.5593, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 37.16417910447761\n",
      "loss是： tensor(96.0857, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 32.76923076923077\n",
      "loss是： tensor(84.1523, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 21.38888888888889\n",
      "loss是： tensor(69.2554, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 29.206349206349206\n",
      "loss是： tensor(79.8092, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 52.10526315789473\n",
      "loss是： tensor(105.4871, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 28.275862068965516\n",
      "loss是： tensor(59.7103, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 32.714285714285715\n",
      "loss是： tensor(83.6956, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 5.319148936170213\n",
      "loss是： tensor(54.0345, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 32.46153846153846\n",
      "loss是： tensor(87.4688, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 29.70149253731343\n",
      "loss是： tensor(82.2568, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 34.92307692307692\n",
      "loss是： tensor(125.3277, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 31.911764705882355\n",
      "loss是： tensor(98.6956, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 31.80327868852459\n",
      "loss是： tensor(87.7469, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 35.87301587301587\n",
      "loss是： tensor(83.5855, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 34.0\n",
      "loss是： tensor(77.8501, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 54.920634920634924\n",
      "loss是： tensor(102.3508, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 25.737704918032787\n",
      "loss是： tensor(63.0493, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 35.08474576271186\n",
      "loss是： tensor(76.2018, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 36.92307692307693\n",
      "loss是： tensor(60.4182, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 30.701754385964914\n",
      "loss是： tensor(61.4710, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 29.322033898305087\n",
      "loss是： tensor(73.6930, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 39.743589743589745\n",
      "loss是： tensor(115.8851, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 26.26865671641791\n",
      "loss是： tensor(80.4440, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 30.793650793650794\n",
      "loss是： tensor(100.0605, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 27.5\n",
      "loss是： tensor(83.2231, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 30.158730158730158\n",
      "loss是： tensor(85.2566, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 30.625\n",
      "loss是： tensor(83.5915, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 39.047619047619044\n",
      "loss是： tensor(84.6812, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 47.5\n",
      "loss是： tensor(93.1023, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 27.424242424242422\n",
      "loss是： tensor(79.7415, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 29.384615384615387\n",
      "loss是： tensor(89.4221, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 5.797101449275362\n",
      "loss是： tensor(69.8011, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 34.59016393442623\n",
      "loss是： tensor(84.3856, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 35.0\n",
      "loss是： tensor(103.3677, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 31.5625\n",
      "loss是： tensor(84.7841, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 31.492537313432834\n",
      "loss是： tensor(91.3942, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 28.4375\n",
      "loss是： tensor(74.9961, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 17.5\n",
      "loss是： tensor(107.8330, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 28.923076923076923\n",
      "loss是： tensor(75.5961, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 35.294117647058826\n",
      "loss是： tensor(86.8077, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 47.96296296296297\n",
      "loss是： tensor(95.3713, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 35.27777777777778\n",
      "loss是： tensor(66.5516, device='cuda:0', grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert计算的loss是： 41.96969696969697\n",
      "loss是： tensor(83.8932, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 41.15942028985507\n",
      "loss是： tensor(95.0325, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 34.629629629629626\n",
      "loss是： tensor(90.2284, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 26.46153846153846\n",
      "loss是： tensor(73.4336, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 35.49295774647887\n",
      "loss是： tensor(81.4244, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 33.088235294117645\n",
      "loss是： tensor(76.5756, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 33.90625\n",
      "loss是： tensor(74.4837, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 21.93548387096774\n",
      "loss是： tensor(65.6601, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 21.029411764705884\n",
      "loss是： tensor(63.7271, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 45.07692307692308\n",
      "loss是： tensor(125.7689, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 33.96551724137931\n",
      "loss是： tensor(74.7368, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 33.278688524590166\n",
      "loss是： tensor(91.7074, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 43.793103448275865\n",
      "loss是： tensor(107.2897, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 57.23404255319149\n",
      "loss是： tensor(97.7691, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 24.210526315789473\n",
      "loss是： tensor(66.0746, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 23.571428571428573\n",
      "loss是： tensor(74.4859, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 29.696969696969695\n",
      "loss是： tensor(87.1969, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 33.23529411764706\n",
      "loss是： tensor(95.8134, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 53.75\n",
      "loss是： tensor(96.4218, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 28.518518518518515\n",
      "loss是： tensor(86.3050, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 29.411764705882355\n",
      "loss是： tensor(97.4667, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 27.246376811594203\n",
      "loss是： tensor(87.1404, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 25.128205128205128\n",
      "loss是： tensor(76.9989, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "正在训练第 2轮\n",
      "Model(\n",
      "  (gen_embedding): Embedding(8518, 300)\n",
      "  (conv1): Conv1d(300, 128, kernel_size=(5,), stride=(1,), padding=(2,))\n",
      "  (conv2): Conv1d(300, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "  (dropout): Dropout(p=0.55, inplace=False)\n",
      "  (conv3): Conv1d(256, 256, kernel_size=(5,), stride=(1,), padding=(2,))\n",
      "  (conv4): Conv1d(256, 256, kernel_size=(5,), stride=(1,), padding=(2,))\n",
      "  (conv5): Conv1d(256, 256, kernel_size=(5,), stride=(1,), padding=(2,))\n",
      "  (linear_ae): Linear(in_features=256, out_features=3, bias=True)\n",
      "  (crf): ConditionalRandomField()\n",
      ")\n",
      "0/300\n",
      "10/300\n",
      "20/300\n",
      "30/300\n",
      "40/300\n",
      "50/300\n",
      "60/300\n",
      "70/300\n",
      "80/300\n",
      "90/300\n",
      "100/300\n",
      "110/300\n",
      "120/300\n",
      "130/300\n",
      "140/300\n",
      "150/300\n",
      "160/300\n",
      "170/300\n",
      "180/300\n",
      "190/300\n",
      "200/300\n",
      "210/300\n",
      "220/300\n",
      "230/300\n",
      "240/300\n",
      "bert计算的loss是： 27.28813559322034\n",
      "loss是： tensor(87.7402, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 33.114754098360656\n",
      "loss是： tensor(100.7190, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 24.84375\n",
      "loss是： tensor(81.4694, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 39.1044776119403\n",
      "loss是： tensor(133.1101, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 42.075471698113205\n",
      "loss是： tensor(112.0908, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 35.079365079365076\n",
      "loss是： tensor(100.1679, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 28.70967741935484\n",
      "loss是： tensor(108.8715, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 74.3076923076923\n",
      "loss是： tensor(179.6328, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 34.32835820895522\n",
      "loss是： tensor(140.2812, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 20.64516129032258\n",
      "loss是： tensor(74.4234, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 27.857142857142854\n",
      "loss是： tensor(165.6033, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 44.46153846153847\n",
      "loss是： tensor(121.1956, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 23.846153846153847\n",
      "loss是： tensor(91.2277, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 31.311475409836067\n",
      "loss是： tensor(107.8981, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 30.606060606060606\n",
      "loss是： tensor(93.2008, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 30.508474576271183\n",
      "loss是： tensor(104.9936, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 25.076923076923077\n",
      "loss是： tensor(106.4391, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 40.19230769230769\n",
      "loss是： tensor(99.6915, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 35.87301587301587\n",
      "loss是： tensor(94.4019, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 26.25\n",
      "loss是： tensor(81.7919, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 49.824561403508774\n",
      "loss是： tensor(106.5259, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 28.461538461538463\n",
      "loss是： tensor(84.8820, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 42.30769230769231\n",
      "loss是： tensor(108.9534, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "250/300\n",
      "bert计算的loss是： 2.380952380952381\n",
      "loss是： tensor(83.8354, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 51.5625\n",
      "loss是： tensor(140.1279, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 33.09090909090909\n",
      "loss是： tensor(93.9398, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 35.0\n",
      "loss是： tensor(108.6366, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 31.636363636363637\n",
      "loss是： tensor(92.9317, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 35.3448275862069\n",
      "loss是： tensor(112.0579, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 13.709677419354838\n",
      "loss是： tensor(101.8541, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 26.440677966101696\n",
      "loss是： tensor(97.0164, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 33.87096774193549\n",
      "loss是： tensor(102.1772, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 25.32258064516129\n",
      "loss是： tensor(85.1044, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 31.756756756756758\n",
      "loss是： tensor(124.5757, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 45.07462686567165\n",
      "loss是： tensor(159.0037, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 11.692307692307693\n",
      "loss是： tensor(87.0752, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 30.0\n",
      "loss是： tensor(109.0840, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 28.4375\n",
      "loss是： tensor(124.2337, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 63.28125\n",
      "loss是： tensor(132.5073, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 30.714285714285715\n",
      "loss是： tensor(94.8089, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 28.23529411764706\n",
      "loss是： tensor(104.7418, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 30.51724137931035\n",
      "loss是： tensor(106.4272, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 26.451612903225804\n",
      "loss是： tensor(118.6669, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 47.301587301587304\n",
      "loss是： tensor(121.7549, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 13.442622950819672\n",
      "loss是： tensor(79.7913, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 31.136363636363637\n",
      "loss是： tensor(98.5028, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 40.53571428571429\n",
      "loss是： tensor(133.2343, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 40.84507042253521\n",
      "loss是： tensor(99.0646, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 25.32258064516129\n",
      "loss是： tensor(90.5135, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 28.309859154929576\n",
      "loss是： tensor(97.5998, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 33.28125\n",
      "loss是： tensor(125.0742, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 24.25925925925926\n",
      "loss是： tensor(100.5829, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 37.03125\n",
      "loss是： tensor(127.0980, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 36.0655737704918\n",
      "loss是： tensor(100.5898, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 31.69230769230769\n",
      "loss是： tensor(117.5863, device='cuda:0', grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert计算的loss是： 38.5\n",
      "loss是： tensor(106.5049, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 33.013698630136986\n",
      "loss是： tensor(112.3630, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 43.71428571428571\n",
      "loss是： tensor(103.7194, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 61.04477611940298\n",
      "loss是： tensor(155.5894, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 15.857142857142856\n",
      "loss是： tensor(97.4113, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 36.34615384615385\n",
      "loss是： tensor(90.0054, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 21.29032258064516\n",
      "loss是： tensor(98.1524, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 24.92957746478873\n",
      "loss是： tensor(116.0994, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 15.660377358490567\n",
      "loss是： tensor(84.2691, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 30.625\n",
      "loss是： tensor(118.0525, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 7.241379310344827\n",
      "loss是： tensor(42.2431, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 36.724137931034484\n",
      "loss是： tensor(100.2155, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 29.393939393939394\n",
      "loss是： tensor(86.5474, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 33.24324324324324\n",
      "loss是： tensor(78.6341, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 37.213114754098356\n",
      "loss是： tensor(110.9624, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 20.163934426229506\n",
      "loss是： tensor(115.4991, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 30.158730158730158\n",
      "loss是： tensor(102.8452, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 28.676470588235293\n",
      "loss是： tensor(107.4927, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 10.9375\n",
      "loss是： tensor(68.8074, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 34.36363636363637\n",
      "loss是： tensor(110.5394, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 26.865671641791046\n",
      "loss是： tensor(98.1392, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 34.09090909090909\n",
      "loss是： tensor(113.3799, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 32.93103448275862\n",
      "loss是： tensor(120.8720, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 24.26470588235294\n",
      "loss是： tensor(88.2981, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 35.09803921568627\n",
      "loss是： tensor(86.7880, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 34.84375\n",
      "loss是： tensor(117.1040, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 37.01492537313433\n",
      "loss是： tensor(113.1003, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 59.53846153846154\n",
      "loss是： tensor(141.2992, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 35.87301587301587\n",
      "loss是： tensor(107.8698, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 57.69230769230769\n",
      "loss是： tensor(137.4744, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 36.09375\n",
      "loss是： tensor(118.3563, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 43.28358208955223\n",
      "loss是： tensor(109.6268, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 25.757575757575758\n",
      "loss是： tensor(81.0948, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 30.88235294117647\n",
      "loss是： tensor(109.6822, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 33.492063492063494\n",
      "loss是： tensor(95.5617, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 28.656716417910445\n",
      "loss是： tensor(103.6270, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 23.548387096774196\n",
      "loss是： tensor(56.2676, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 33.666666666666664\n",
      "loss是： tensor(87.5217, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 36.10169491525424\n",
      "loss是： tensor(99.3208, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 45.094339622641506\n",
      "loss是： tensor(110.0829, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 41.48148148148148\n",
      "loss是： tensor(125.3353, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 14.426229508196721\n",
      "loss是： tensor(78.1578, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 29.23076923076923\n",
      "loss是： tensor(111.1729, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 30.27777777777778\n",
      "loss是： tensor(99.7412, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 29.18032786885246\n",
      "loss是： tensor(97.3685, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 25.692307692307693\n",
      "loss是： tensor(71.2870, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 37.1875\n",
      "loss是： tensor(108.0822, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 25.40983606557377\n",
      "loss是： tensor(106.2606, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 35.46875\n",
      "loss是： tensor(109.9054, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 33.15789473684211\n",
      "loss是： tensor(97.2732, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 35.303030303030305\n",
      "loss是： tensor(98.0882, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 27.692307692307693\n",
      "loss是： tensor(83.5694, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 34.73684210526316\n",
      "loss是： tensor(82.7348, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 43.33333333333333\n",
      "loss是： tensor(124.8851, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 36.71641791044776\n",
      "loss是： tensor(120.9055, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 36.81818181818181\n",
      "loss是： tensor(123.9570, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 29.84375\n",
      "loss是： tensor(89.4908, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 27.8125\n",
      "loss是： tensor(92.8872, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 35.08771929824561\n",
      "loss是： tensor(113.5166, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 35.75\n",
      "loss是： tensor(77.4813, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 63.846153846153854\n",
      "loss是： tensor(124.4938, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 33.728813559322035\n",
      "loss是： tensor(91.1972, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 37.23076923076923\n",
      "loss是： tensor(119.6024, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 30.84507042253521\n",
      "loss是： tensor(85.8624, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 41.45454545454545\n",
      "loss是： tensor(114.6288, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 33.484848484848484\n",
      "loss是： tensor(116.5734, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 30.833333333333336\n",
      "loss是： tensor(95.5295, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 39.53125\n",
      "loss是： tensor(112.6662, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 26.0\n",
      "loss是： tensor(126.7523, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 37.05882352941177\n",
      "loss是： tensor(144.0580, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 29.577464788732396\n",
      "loss是： tensor(106.1517, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 35.14285714285714\n",
      "loss是： tensor(103.9643, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 37.666666666666664\n",
      "loss是： tensor(116.3763, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 25.192307692307693\n",
      "loss是： tensor(66.7875, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 37.0\n",
      "loss是： tensor(138.1958, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 42.333333333333336\n",
      "loss是： tensor(105.0533, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 25.087719298245613\n",
      "loss是： tensor(79.3018, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 36.875\n",
      "loss是： tensor(117.4400, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 24.72222222222222\n",
      "loss是： tensor(108.5828, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 31.733333333333334\n",
      "loss是： tensor(106.9864, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 37.83333333333333\n",
      "loss是： tensor(98.0871, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 29.056603773584904\n",
      "loss是： tensor(96.0404, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 36.04651162790698\n",
      "loss是： tensor(89.4972, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 31.11111111111111\n",
      "loss是： tensor(100.4888, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 59.19354838709677\n",
      "loss是： tensor(123.0508, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 40.3125\n",
      "loss是： tensor(92.0943, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 33.50877192982456\n",
      "loss是： tensor(127.6241, device='cuda:0', grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert计算的loss是： 26.779661016949152\n",
      "loss是： tensor(87.2351, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 34.57142857142857\n",
      "loss是： tensor(137.0072, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 31.694915254237287\n",
      "loss是： tensor(82.2227, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 24.383561643835616\n",
      "loss是： tensor(79.2757, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 40.61538461538461\n",
      "loss是： tensor(113.1601, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 40.3125\n",
      "loss是： tensor(146.3810, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 26.875\n",
      "loss是： tensor(93.1754, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 30.64516129032258\n",
      "loss是： tensor(110.2287, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 28.548387096774196\n",
      "loss是： tensor(102.6593, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 40.15384615384615\n",
      "loss是： tensor(116.8369, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 36.81818181818181\n",
      "loss是： tensor(90.5848, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 30.76923076923077\n",
      "loss是： tensor(143.6992, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 29.0\n",
      "loss是： tensor(81.0420, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 35.86206896551724\n",
      "loss是： tensor(91.4139, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 41.30434782608695\n",
      "loss是： tensor(145.6432, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 28.70967741935484\n",
      "loss是： tensor(103.3810, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 20.847457627118647\n",
      "loss是： tensor(87.6357, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 31.35593220338983\n",
      "loss是： tensor(90.1327, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 32.0\n",
      "loss是： tensor(83.7721, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 19.090909090909093\n",
      "loss是： tensor(79.5117, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 66.43835616438356\n",
      "loss是： tensor(154.9170, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 9.272727272727273\n",
      "loss是： tensor(57.9211, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 28.421052631578945\n",
      "loss是： tensor(88.6667, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 34.375\n",
      "loss是： tensor(97.3070, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 34.15384615384615\n",
      "loss是： tensor(84.1726, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 40.32786885245901\n",
      "loss是： tensor(93.5119, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 42.90322580645161\n",
      "loss是： tensor(146.2421, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 29.545454545454547\n",
      "loss是： tensor(117.5608, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 32.07547169811321\n",
      "loss是： tensor(88.6309, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 29.718309859154928\n",
      "loss是： tensor(104.0967, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 29.1044776119403\n",
      "loss是： tensor(79.7784, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 41.11111111111111\n",
      "loss是： tensor(103.2204, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 27.599999999999998\n",
      "loss是： tensor(103.9026, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 39.53125\n",
      "loss是： tensor(100.5384, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 40.53571428571429\n",
      "loss是： tensor(93.7891, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 31.515151515151516\n",
      "loss是： tensor(113.4746, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 29.464285714285715\n",
      "loss是： tensor(79.9091, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 31.818181818181817\n",
      "loss是： tensor(105.6403, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 34.705882352941174\n",
      "loss是： tensor(126.1014, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 36.60377358490566\n",
      "loss是： tensor(100.1239, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 21.29032258064516\n",
      "loss是： tensor(78.1526, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 12.051282051282051\n",
      "loss是： tensor(45.4441, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 27.017543859649123\n",
      "loss是： tensor(96.3447, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 25.97222222222222\n",
      "loss是： tensor(112.2533, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 40.96153846153846\n",
      "loss是： tensor(74.3645, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 35.9016393442623\n",
      "loss是： tensor(106.6724, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 29.661016949152543\n",
      "loss是： tensor(92.4014, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 47.90322580645161\n",
      "loss是： tensor(112.9836, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 37.94117647058823\n",
      "loss是： tensor(111.6415, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 35.73770491803279\n",
      "loss是： tensor(134.6385, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 23.650793650793652\n",
      "loss是： tensor(70.4430, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 27.10144927536232\n",
      "loss是： tensor(126.1745, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 38.46153846153846\n",
      "loss是： tensor(122.8180, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 20.625\n",
      "loss是： tensor(90.0416, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 36.83333333333333\n",
      "loss是： tensor(115.5888, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 29.166666666666664\n",
      "loss是： tensor(79.9116, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 6.551724137931034\n",
      "loss是： tensor(85.7549, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 31.666666666666664\n",
      "loss是： tensor(101.0186, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 57.72727272727273\n",
      "loss是： tensor(116.0147, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 35.820895522388064\n",
      "loss是： tensor(110.9570, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 41.228070175438596\n",
      "loss是： tensor(111.1558, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 30.588235294117645\n",
      "loss是： tensor(114.5500, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 34.8051948051948\n",
      "loss是： tensor(120.7327, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 32.0\n",
      "loss是： tensor(104.6143, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 13.157894736842106\n",
      "loss是： tensor(47.8404, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 30.689655172413794\n",
      "loss是： tensor(108.3575, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 50.317460317460316\n",
      "loss是： tensor(135.3272, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 33.0\n",
      "loss是： tensor(87.3078, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 65.90163934426229\n",
      "loss是： tensor(118.4143, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 33.088235294117645\n",
      "loss是： tensor(110.9067, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 27.460317460317462\n",
      "loss是： tensor(96.3010, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 47.45454545454545\n",
      "loss是： tensor(115.6362, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 25.833333333333336\n",
      "loss是： tensor(95.2383, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 26.307692307692307\n",
      "loss是： tensor(88.8890, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 26.666666666666664\n",
      "loss是： tensor(74.4342, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 30.0\n",
      "loss是： tensor(94.2677, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 24.754098360655735\n",
      "loss是： tensor(86.8311, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 8.7012987012987\n",
      "loss是： tensor(95.5315, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 39.67741935483871\n",
      "loss是： tensor(107.0454, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 30.535714285714285\n",
      "loss是： tensor(109.7691, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 9.672131147540984\n",
      "loss是： tensor(84.3061, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 25.689655172413794\n",
      "loss是： tensor(84.5097, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 34.696969696969695\n",
      "loss是： tensor(125.2804, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 37.333333333333336\n",
      "loss是： tensor(98.2223, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 27.037037037037038\n",
      "loss是： tensor(74.9910, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 23.56164383561644\n",
      "loss是： tensor(114.4286, device='cuda:0', grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert计算的loss是： 28.243243243243242\n",
      "loss是： tensor(92.2942, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 31.904761904761905\n",
      "loss是： tensor(76.7003, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 37.205882352941174\n",
      "loss是： tensor(108.0634, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 7.910447761194029\n",
      "loss是： tensor(84.3302, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 35.44117647058823\n",
      "loss是： tensor(101.6440, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 35.6140350877193\n",
      "loss是： tensor(107.7024, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 34.40677966101695\n",
      "loss是： tensor(118.1555, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 36.81159420289855\n",
      "loss是： tensor(129.1801, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 37.719298245614034\n",
      "loss是： tensor(107.6233, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 28.5\n",
      "loss是： tensor(85.6372, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 33.389830508474574\n",
      "loss是： tensor(98.1909, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 28.78787878787879\n",
      "loss是： tensor(95.7420, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 33.529411764705884\n",
      "loss是： tensor(128.1466, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 17.460317460317462\n",
      "loss是： tensor(89.5344, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 68.4126984126984\n",
      "loss是： tensor(125.2162, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 25.294117647058822\n",
      "loss是： tensor(98.8090, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 26.29032258064516\n",
      "loss是： tensor(74.0474, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 32.96875\n",
      "loss是： tensor(110.6440, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 38.70967741935484\n",
      "loss是： tensor(95.0520, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 34.6551724137931\n",
      "loss是： tensor(123.1378, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 33.88059701492537\n",
      "loss是： tensor(112.8590, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 34.85714285714286\n",
      "loss是： tensor(106.3205, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 29.347826086956523\n",
      "loss是： tensor(101.0548, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 25.294117647058822\n",
      "loss是： tensor(102.8884, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 25.897435897435898\n",
      "loss是： tensor(79.5204, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "260/300\n",
      "bert计算的loss是： 35.21126760563381\n",
      "loss是： tensor(109.0000, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 36.724137931034484\n",
      "loss是： tensor(89.0961, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 23.333333333333336\n",
      "loss是： tensor(93.1574, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 26.557377049180324\n",
      "loss是： tensor(93.7561, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 36.5625\n",
      "loss是： tensor(110.6962, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 43.60655737704918\n",
      "loss是： tensor(87.8651, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 39.57142857142857\n",
      "loss是： tensor(128.1795, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 30.153846153846153\n",
      "loss是： tensor(107.6525, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 27.666666666666664\n",
      "loss是： tensor(92.6415, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 28.688524590163933\n",
      "loss是： tensor(110.7192, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 37.096774193548384\n",
      "loss是： tensor(115.3674, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 40.847457627118644\n",
      "loss是： tensor(89.7730, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 47.46031746031746\n",
      "loss是： tensor(123.8051, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 30.0\n",
      "loss是： tensor(110.1449, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 32.30769230769231\n",
      "loss是： tensor(80.4081, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 32.758620689655174\n",
      "loss是： tensor(81.6277, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 62.622950819672134\n",
      "loss是： tensor(146.3510, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 30.13157894736842\n",
      "loss是： tensor(108.6764, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 27.84313725490196\n",
      "loss是： tensor(106.6549, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 30.147058823529413\n",
      "loss是： tensor(113.6255, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 34.46153846153847\n",
      "loss是： tensor(105.6251, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 34.73684210526316\n",
      "loss是： tensor(104.8933, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 27.17391304347826\n",
      "loss是： tensor(79.6256, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 46.3768115942029\n",
      "loss是： tensor(121.9366, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 31.267605633802816\n",
      "loss是： tensor(123.8159, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 24.067796610169488\n",
      "loss是： tensor(80.9952, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 36.0\n",
      "loss是： tensor(121.3756, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 21.833333333333332\n",
      "loss是： tensor(85.4296, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 33.70967741935484\n",
      "loss是： tensor(134.4115, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 37.16666666666667\n",
      "loss是： tensor(102.7871, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 37.37704918032787\n",
      "loss是： tensor(123.9792, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 46.66666666666667\n",
      "loss是： tensor(131.6727, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 18.793103448275865\n",
      "loss是： tensor(87.1523, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 35.52238805970149\n",
      "loss是： tensor(109.0638, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 30.64516129032258\n",
      "loss是： tensor(109.8739, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 29.807692307692307\n",
      "loss是： tensor(90.5112, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 23.103448275862068\n",
      "loss是： tensor(68.5432, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 31.451612903225804\n",
      "loss是： tensor(107.2236, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 34.0\n",
      "loss是： tensor(97.6989, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 34.936708860759495\n",
      "loss是： tensor(136.3314, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 40.9375\n",
      "loss是： tensor(137.8550, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 37.77777777777778\n",
      "loss是： tensor(102.3772, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 22.5\n",
      "loss是： tensor(100.2078, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 28.833333333333332\n",
      "loss是： tensor(99.0511, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 47.12121212121212\n",
      "loss是： tensor(132.1448, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 24.146341463414632\n",
      "loss是： tensor(82.0244, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 25.555555555555554\n",
      "loss是： tensor(69.4457, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 45.15625\n",
      "loss是： tensor(115.1750, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 32.08955223880597\n",
      "loss是： tensor(130.9427, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 26.956521739130434\n",
      "loss是： tensor(74.0458, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 26.451612903225804\n",
      "loss是： tensor(79.7620, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 35.230769230769226\n",
      "loss是： tensor(79.7090, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 56.935483870967744\n",
      "loss是： tensor(148.3209, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 27.049180327868854\n",
      "loss是： tensor(91.7276, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 30.0\n",
      "loss是： tensor(124.2606, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 20.74074074074074\n",
      "loss是： tensor(57.4398, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 29.571428571428573\n",
      "loss是： tensor(134.5255, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 30.78125\n",
      "loss是： tensor(79.6504, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 42.698412698412696\n",
      "loss是： tensor(114.6507, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 35.16129032258065\n",
      "loss是： tensor(101.7112, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 32.10526315789474\n",
      "loss是： tensor(98.2659, device='cuda:0', grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert计算的loss是： 41.0344827586207\n",
      "loss是： tensor(102.1952, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 31.846153846153843\n",
      "loss是： tensor(94.1479, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 29.090909090909093\n",
      "loss是： tensor(79.2196, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 19.838709677419356\n",
      "loss是： tensor(88.6447, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 27.53846153846154\n",
      "loss是： tensor(124.6657, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 69.32203389830508\n",
      "loss是： tensor(136.3479, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 25.666666666666668\n",
      "loss是： tensor(76.9511, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 41.27659574468085\n",
      "loss是： tensor(80.0675, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 19.047619047619047\n",
      "loss是： tensor(78.3985, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 37.37704918032787\n",
      "loss是： tensor(100.0198, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 29.21875\n",
      "loss是： tensor(91.3914, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 41.12903225806452\n",
      "loss是： tensor(110.7700, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 38.15384615384615\n",
      "loss是： tensor(98.3181, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 40.285714285714285\n",
      "loss是： tensor(129.1528, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 67.41379310344827\n",
      "loss是： tensor(142.6127, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 30.0\n",
      "loss是： tensor(82.1748, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 13.56164383561644\n",
      "loss是： tensor(66.1715, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 38.5\n",
      "loss是： tensor(132.7564, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 37.34375\n",
      "loss是： tensor(107.0408, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 41.81818181818181\n",
      "loss是： tensor(123.0786, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 35.23809523809524\n",
      "loss是： tensor(87.9530, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 29.21875\n",
      "loss是： tensor(100.9722, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 27.049180327868854\n",
      "loss是： tensor(74.5786, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 33.278688524590166\n",
      "loss是： tensor(133.9136, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 26.21212121212121\n",
      "loss是： tensor(79.4217, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 28.448275862068964\n",
      "loss是： tensor(72.1119, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 31.833333333333332\n",
      "loss是： tensor(95.5627, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 26.76470588235294\n",
      "loss是： tensor(90.6527, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 40.166666666666664\n",
      "loss是： tensor(143.9808, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 29.23076923076923\n",
      "loss是： tensor(89.0395, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 26.34146341463415\n",
      "loss是： tensor(64.2830, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 30.677966101694913\n",
      "loss是： tensor(106.2998, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 31.06060606060606\n",
      "loss是： tensor(110.2121, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 39.230769230769226\n",
      "loss是： tensor(102.9269, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 34.0625\n",
      "loss是： tensor(106.6080, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 37.54385964912281\n",
      "loss是： tensor(125.9045, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 27.80821917808219\n",
      "loss是： tensor(98.1855, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 26.06060606060606\n",
      "loss是： tensor(115.5564, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 37.333333333333336\n",
      "loss是： tensor(85.1501, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 71.31147540983606\n",
      "loss是： tensor(138.6372, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 29.090909090909093\n",
      "loss是： tensor(104.0463, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 37.53846153846154\n",
      "loss是： tensor(138.0908, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 29.21875\n",
      "loss是： tensor(94.3727, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 31.19402985074627\n",
      "loss是： tensor(130.6362, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 27.647058823529413\n",
      "loss是： tensor(95.4520, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 23.833333333333332\n",
      "loss是： tensor(79.7415, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 45.82089552238806\n",
      "loss是： tensor(118.7876, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 32.241379310344826\n",
      "loss是： tensor(76.8254, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 21.88679245283019\n",
      "loss是： tensor(94.4184, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 29.55223880597015\n",
      "loss是： tensor(97.5613, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 45.32258064516129\n",
      "loss是： tensor(109.8349, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 37.432432432432435\n",
      "loss是： tensor(111.0311, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 23.076923076923073\n",
      "loss是： tensor(87.3080, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 42.0\n",
      "loss是： tensor(94.2196, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 22.121212121212118\n",
      "loss是： tensor(58.1977, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 47.0\n",
      "loss是： tensor(125.9846, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 28.30769230769231\n",
      "loss是： tensor(91.5416, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 42.08955223880597\n",
      "loss是： tensor(108.3596, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 25.862068965517242\n",
      "loss是： tensor(88.0651, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 28.166666666666668\n",
      "loss是： tensor(117.8402, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 30.416666666666664\n",
      "loss是： tensor(110.3029, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 32.028985507246375\n",
      "loss是： tensor(97.7326, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 30.952380952380953\n",
      "loss是： tensor(119.2061, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 42.95774647887324\n",
      "loss是： tensor(134.6529, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 36.875\n",
      "loss是： tensor(83.2413, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 35.66666666666667\n",
      "loss是： tensor(81.1629, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 24.366197183098592\n",
      "loss是： tensor(93.2564, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 6.0\n",
      "loss是： tensor(80.6236, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 34.34782608695652\n",
      "loss是： tensor(112.3481, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 7.575757575757576\n",
      "loss是： tensor(81.0118, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 30.344827586206897\n",
      "loss是： tensor(125.4184, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 43.88059701492537\n",
      "loss是： tensor(152.9357, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 37.45762711864407\n",
      "loss是： tensor(101.5107, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 32.07547169811321\n",
      "loss是： tensor(104.0598, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 15.88235294117647\n",
      "loss是： tensor(87.0107, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 56.0\n",
      "loss是： tensor(144.4792, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 30.731707317073173\n",
      "loss是： tensor(81.6071, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 35.166666666666664\n",
      "loss是： tensor(101.5331, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 28.08219178082192\n",
      "loss是： tensor(94.1684, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 29.8\n",
      "loss是： tensor(89.7202, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 29.076923076923077\n",
      "loss是： tensor(90.0920, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 38.59649122807018\n",
      "loss是： tensor(114.8025, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 54.76190476190476\n",
      "loss是： tensor(129.2530, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 4.5588235294117645\n",
      "loss是： tensor(105.8995, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 27.457627118644066\n",
      "loss是： tensor(74.5124, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 29.242424242424242\n",
      "loss是： tensor(87.1669, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 15.0\n",
      "loss是： tensor(79.4073, device='cuda:0', grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert计算的loss是： 33.90625\n",
      "loss是： tensor(92.1464, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 48.793103448275865\n",
      "loss是： tensor(106.6023, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 38.78787878787879\n",
      "loss是： tensor(105.5643, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 33.275862068965516\n",
      "loss是： tensor(99.0212, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 31.833333333333332\n",
      "loss是： tensor(107.8992, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 37.714285714285715\n",
      "loss是： tensor(135.1404, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 37.8125\n",
      "loss是： tensor(110.6289, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 37.28813559322034\n",
      "loss是： tensor(120.7427, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 29.855072463768117\n",
      "loss是： tensor(99.3490, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 42.786885245901644\n",
      "loss是： tensor(125.4619, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 6.833333333333334\n",
      "loss是： tensor(80.5296, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 17.1875\n",
      "loss是： tensor(76.8730, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 27.551020408163268\n",
      "loss是： tensor(73.6335, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 31.967213114754102\n",
      "loss是： tensor(96.0088, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 9.12280701754386\n",
      "loss是： tensor(61.6800, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 43.6231884057971\n",
      "loss是： tensor(114.7345, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 42.5\n",
      "loss是： tensor(121.2764, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 39.18032786885246\n",
      "loss是： tensor(87.4780, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 28.03030303030303\n",
      "loss是： tensor(86.6185, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 27.462686567164177\n",
      "loss是： tensor(94.0768, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 30.0\n",
      "loss是： tensor(122.9882, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 21.311475409836067\n",
      "loss是： tensor(70.0590, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 35.32258064516129\n",
      "loss是： tensor(85.7570, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 36.029411764705884\n",
      "loss是： tensor(115.1541, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 29.833333333333336\n",
      "loss是： tensor(102.3569, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 30.0\n",
      "loss是： tensor(113.4982, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 27.735849056603776\n",
      "loss是： tensor(109.8952, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 26.923076923076927\n",
      "loss是： tensor(89.1672, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 29.850746268656717\n",
      "loss是： tensor(115.9748, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 35.54054054054054\n",
      "loss是： tensor(106.6834, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 57.61194029850746\n",
      "loss是： tensor(125.2535, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 9.18918918918919\n",
      "loss是： tensor(96.5782, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 31.016949152542374\n",
      "loss是： tensor(83.6988, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 31.285714285714285\n",
      "loss是： tensor(123.4175, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 43.275862068965516\n",
      "loss是： tensor(109.3371, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 34.32432432432432\n",
      "loss是： tensor(70.0991, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 47.74193548387097\n",
      "loss是： tensor(110.2513, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 29.411764705882355\n",
      "loss是： tensor(94.2484, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 32.714285714285715\n",
      "loss是： tensor(112.0523, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 29.107142857142854\n",
      "loss是： tensor(79.0605, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 32.5\n",
      "loss是： tensor(107.4194, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 25.964912280701753\n",
      "loss是： tensor(76.4588, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 34.33333333333333\n",
      "loss是： tensor(115.7589, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 28.529411764705884\n",
      "loss是： tensor(110.4045, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 42.0\n",
      "loss是： tensor(97.9717, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 33.38461538461539\n",
      "loss是： tensor(98.3457, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 23.59375\n",
      "loss是： tensor(86.6169, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 43.01587301587301\n",
      "loss是： tensor(114.6306, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 30.88607594936709\n",
      "loss是： tensor(114.0735, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 29.848484848484848\n",
      "loss是： tensor(100.0762, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 40.3921568627451\n",
      "loss是： tensor(99.6279, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 39.152542372881356\n",
      "loss是： tensor(120.0184, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 31.176470588235293\n",
      "loss是： tensor(85.8941, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 34.91228070175439\n",
      "loss是： tensor(98.1632, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 33.80952380952381\n",
      "loss是： tensor(126.3417, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 69.50819672131148\n",
      "loss是： tensor(145.8520, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 31.904761904761905\n",
      "loss是： tensor(79.5840, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 21.785714285714285\n",
      "loss是： tensor(93.7584, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 31.627906976744185\n",
      "loss是： tensor(77.7709, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 36.71641791044776\n",
      "loss是： tensor(133.0675, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 28.47457627118644\n",
      "loss是： tensor(103.8358, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 38.15384615384615\n",
      "loss是： tensor(87.6034, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 34.03846153846154\n",
      "loss是： tensor(94.1347, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 35.0\n",
      "loss是： tensor(91.2087, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 42.7536231884058\n",
      "loss是： tensor(143.3563, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 36.61971830985915\n",
      "loss是： tensor(112.8835, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 31.451612903225804\n",
      "loss是： tensor(102.3771, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 33.6231884057971\n",
      "loss是： tensor(102.4601, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 32.93103448275862\n",
      "loss是： tensor(115.1663, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 21.666666666666664\n",
      "loss是： tensor(73.7440, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 18.958333333333332\n",
      "loss是： tensor(66.9974, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 33.0\n",
      "loss是： tensor(84.4329, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 39.83870967741935\n",
      "loss是： tensor(116.5837, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 26.984126984126988\n",
      "loss是： tensor(95.4134, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 31.71875\n",
      "loss是： tensor(103.6282, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 19.01639344262295\n",
      "loss是： tensor(95.1951, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 61.54929577464789\n",
      "loss是： tensor(139.3703, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 15.846153846153845\n",
      "loss是： tensor(81.8138, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 30.136986301369863\n",
      "loss是： tensor(104.0470, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 38.14814814814815\n",
      "loss是： tensor(96.7282, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 27.65625\n",
      "loss是： tensor(68.4595, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 42.285714285714285\n",
      "loss是： tensor(85.0384, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "270/300\n",
      "bert计算的loss是： 16.666666666666668\n",
      "loss是： tensor(87.1299, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 37.41379310344827\n",
      "loss是： tensor(128.0111, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 33.214285714285715\n",
      "loss是： tensor(88.8557, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 11.692307692307693\n",
      "loss是： tensor(48.7875, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 55.68627450980392\n",
      "loss是： tensor(126.0052, device='cuda:0', grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert计算的loss是： 26.911764705882355\n",
      "loss是： tensor(106.3199, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 34.83870967741935\n",
      "loss是： tensor(115.9755, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 33.6231884057971\n",
      "loss是： tensor(96.0230, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 37.10144927536232\n",
      "loss是： tensor(94.3757, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 33.54838709677419\n",
      "loss是： tensor(89.2071, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 29.82456140350877\n",
      "loss是： tensor(93.2498, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 29.420289855072465\n",
      "loss是： tensor(105.4640, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 26.607142857142854\n",
      "loss是： tensor(101.8595, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 34.861111111111114\n",
      "loss是： tensor(159.5936, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 28.450704225352116\n",
      "loss是： tensor(96.3825, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 49.26470588235294\n",
      "loss是： tensor(130.1749, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 30.98360655737705\n",
      "loss是： tensor(96.3430, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 9.6875\n",
      "loss是： tensor(83.5753, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 23.666666666666668\n",
      "loss是： tensor(58.7734, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 63.088235294117645\n",
      "loss是： tensor(141.8803, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 26.56716417910448\n",
      "loss是： tensor(81.3960, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 30.35087719298246\n",
      "loss是： tensor(96.3763, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 36.07142857142857\n",
      "loss是： tensor(61.4589, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 28.545454545454547\n",
      "loss是： tensor(100.3893, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 27.5\n",
      "loss是： tensor(70.8544, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 42.05479452054795\n",
      "loss是： tensor(114.1857, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 35.64516129032258\n",
      "loss是： tensor(104.5320, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 34.03846153846154\n",
      "loss是： tensor(110.6487, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 39.682539682539684\n",
      "loss是： tensor(96.1109, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 17.166666666666664\n",
      "loss是： tensor(80.7558, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 35.25423728813559\n",
      "loss是： tensor(95.0702, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 29.242424242424242\n",
      "loss是： tensor(94.2998, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 21.641791044776117\n",
      "loss是： tensor(101.6921, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 29.285714285714285\n",
      "loss是： tensor(85.6281, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 24.02777777777778\n",
      "loss是： tensor(86.1789, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 31.475409836065573\n",
      "loss是： tensor(114.1748, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 40.588235294117645\n",
      "loss是： tensor(105.9174, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 39.48275862068965\n",
      "loss是： tensor(108.6303, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 23.793103448275865\n",
      "loss是： tensor(115.5620, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 37.45762711864407\n",
      "loss是： tensor(115.2907, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 57.04225352112676\n",
      "loss是： tensor(143.8817, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 34.24242424242424\n",
      "loss是： tensor(136.1625, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 31.587301587301585\n",
      "loss是： tensor(96.8779, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 30.526315789473685\n",
      "loss是： tensor(111.0392, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 32.459016393442624\n",
      "loss是： tensor(115.5850, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 35.4054054054054\n",
      "loss是： tensor(70.2782, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 13.174603174603174\n",
      "loss是： tensor(99.1782, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 35.84615384615385\n",
      "loss是： tensor(86.8661, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 36.607142857142854\n",
      "loss是： tensor(91.2223, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 30.73529411764706\n",
      "loss是： tensor(102.3879, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 47.5\n",
      "loss是： tensor(118.2758, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 34.920634920634924\n",
      "loss是： tensor(110.1581, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 10.5\n",
      "loss是： tensor(63.7253, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 23.636363636363637\n",
      "loss是： tensor(88.1626, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 31.48936170212766\n",
      "loss是： tensor(102.4097, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 37.91044776119403\n",
      "loss是： tensor(115.6431, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 35.820895522388064\n",
      "loss是： tensor(113.5697, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 27.6056338028169\n",
      "loss是： tensor(92.5002, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 31.911764705882355\n",
      "loss是： tensor(122.5951, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 60.63492063492063\n",
      "loss是： tensor(129.1541, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 39.35483870967742\n",
      "loss是： tensor(117.0237, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 27.301587301587304\n",
      "loss是： tensor(89.2344, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 28.771929824561404\n",
      "loss是： tensor(77.7405, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 32.714285714285715\n",
      "loss是： tensor(114.0148, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 29.516129032258064\n",
      "loss是： tensor(148.1947, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 30.15625\n",
      "loss是： tensor(84.1532, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 36.231884057971016\n",
      "loss是： tensor(114.8236, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 28.90909090909091\n",
      "loss是： tensor(73.1810, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 37.17948717948718\n",
      "loss是： tensor(87.7420, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 32.74193548387097\n",
      "loss是： tensor(104.1080, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 28.620689655172416\n",
      "loss是： tensor(94.6287, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 30.476190476190474\n",
      "loss是： tensor(101.1952, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 34.02985074626866\n",
      "loss是： tensor(108.7584, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 28.64406779661017\n",
      "loss是： tensor(89.4132, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 38.78787878787879\n",
      "loss是： tensor(130.9311, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 35.0\n",
      "loss是： tensor(116.3606, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 33.54838709677419\n",
      "loss是： tensor(104.5938, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 26.206896551724135\n",
      "loss是： tensor(103.7534, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 27.34375\n",
      "loss是： tensor(101.3910, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 29.672131147540984\n",
      "loss是： tensor(111.0168, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 7.868852459016393\n",
      "loss是： tensor(68.6827, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 36.5\n",
      "loss是： tensor(103.5986, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 30.535714285714285\n",
      "loss是： tensor(90.3106, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 38.03030303030303\n",
      "loss是： tensor(111.9732, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 39.50819672131148\n",
      "loss是： tensor(111.3167, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 56.417910447761194\n",
      "loss是： tensor(121.8933, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 31.791044776119403\n",
      "loss是： tensor(118.2666, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 44.769230769230774\n",
      "loss是： tensor(89.9872, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 35.294117647058826\n",
      "loss是： tensor(97.5002, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 29.84375\n",
      "loss是： tensor(87.1349, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 23.43283582089552\n",
      "loss是： tensor(96.4331, device='cuda:0', grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert计算的loss是： 29.523809523809526\n",
      "loss是： tensor(79.8059, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 12.063492063492063\n",
      "loss是： tensor(88.0487, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 30.454545454545453\n",
      "loss是： tensor(110.2929, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 39.40298507462686\n",
      "loss是： tensor(88.4319, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 31.333333333333332\n",
      "loss是： tensor(120.9814, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 41.449275362318836\n",
      "loss是： tensor(106.1758, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 25.576923076923073\n",
      "loss是： tensor(93.8906, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 32.16666666666667\n",
      "loss是： tensor(72.3506, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 30.88235294117647\n",
      "loss是： tensor(97.9597, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 27.088607594936708\n",
      "loss是： tensor(123.7638, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 27.96875\n",
      "loss是： tensor(101.8905, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 20.909090909090907\n",
      "loss是： tensor(85.1277, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 35.64516129032258\n",
      "loss是： tensor(109.6413, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 28.857142857142858\n",
      "loss是： tensor(98.6526, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 73.75\n",
      "loss是： tensor(142.4716, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 28.852459016393443\n",
      "loss是： tensor(110.3678, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 43.833333333333336\n",
      "loss是： tensor(106.6324, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 29.83050847457627\n",
      "loss是： tensor(104.9603, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 38.064516129032256\n",
      "loss是： tensor(102.9737, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 30.78125\n",
      "loss是： tensor(98.4379, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 38.125\n",
      "loss是： tensor(99.6307, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 33.60655737704918\n",
      "loss是： tensor(96.8173, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 33.970588235294116\n",
      "loss是： tensor(97.3463, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 24.324324324324323\n",
      "loss是： tensor(65.6441, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 32.333333333333336\n",
      "loss是： tensor(77.5459, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 33.07692307692307\n",
      "loss是： tensor(92.3439, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 42.295081967213115\n",
      "loss是： tensor(117.7764, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 35.79710144927536\n",
      "loss是： tensor(114.8032, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 35.93220338983051\n",
      "loss是： tensor(106.5748, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 29.577464788732396\n",
      "loss是： tensor(142.1050, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 29.384615384615387\n",
      "loss是： tensor(86.2369, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 38.888888888888886\n",
      "loss是： tensor(125.5192, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 30.15625\n",
      "loss是： tensor(111.3957, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 22.833333333333332\n",
      "loss是： tensor(91.3016, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 28.923076923076923\n",
      "loss是： tensor(105.2364, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 28.507462686567166\n",
      "loss是： tensor(104.8879, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 31.864406779661017\n",
      "loss是： tensor(87.6668, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 25.82089552238806\n",
      "loss是： tensor(71.1008, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 43.91304347826087\n",
      "loss是： tensor(134.9866, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 36.666666666666664\n",
      "loss是： tensor(89.8709, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 53.93939393939394\n",
      "loss是： tensor(169.8636, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 29.565217391304344\n",
      "loss是： tensor(113.2218, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 34.0625\n",
      "loss是： tensor(96.3664, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 32.888888888888886\n",
      "loss是： tensor(81.6443, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 27.540983606557376\n",
      "loss是： tensor(99.2116, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 27.246376811594203\n",
      "loss是： tensor(90.1603, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 31.5\n",
      "loss是： tensor(64.1842, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 22.799999999999997\n",
      "loss是： tensor(65.3237, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 33.24324324324324\n",
      "loss是： tensor(118.1519, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 33.78787878787879\n",
      "loss是： tensor(120.4704, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 34.237288135593225\n",
      "loss是： tensor(92.2381, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 38.309859154929576\n",
      "loss是： tensor(144.9070, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 31.311475409836067\n",
      "loss是： tensor(88.7872, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 26.333333333333332\n",
      "loss是： tensor(95.9030, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 25.967741935483872\n",
      "loss是： tensor(85.1428, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 35.0\n",
      "loss是： tensor(73.7040, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 40.83333333333333\n",
      "loss是： tensor(88.1459, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 36.98412698412699\n",
      "loss是： tensor(98.1402, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 27.662337662337663\n",
      "loss是： tensor(88.3948, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 28.767123287671232\n",
      "loss是： tensor(94.4511, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 39.66101694915254\n",
      "loss是： tensor(96.4312, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 63.442622950819676\n",
      "loss是： tensor(148.2494, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 37.61904761904762\n",
      "loss是： tensor(98.1845, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 36.55172413793104\n",
      "loss是： tensor(83.1123, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 27.07692307692308\n",
      "loss是： tensor(85.9794, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 35.0\n",
      "loss是： tensor(103.9974, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 30.303030303030305\n",
      "loss是： tensor(99.1770, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 35.07692307692308\n",
      "loss是： tensor(119.9943, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 27.61904761904762\n",
      "loss是： tensor(68.8580, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 43.03030303030303\n",
      "loss是： tensor(69.2661, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 28.653846153846153\n",
      "loss是： tensor(61.8062, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 30.677966101694913\n",
      "loss是： tensor(80.5161, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 15.806451612903224\n",
      "loss是： tensor(69.6784, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 27.323943661971832\n",
      "loss是： tensor(108.0085, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 30.566037735849058\n",
      "loss是： tensor(111.2034, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 32.22222222222222\n",
      "loss是： tensor(68.3343, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 26.470588235294116\n",
      "loss是： tensor(71.3802, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 9.615384615384615\n",
      "loss是： tensor(109.4266, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 29.35483870967742\n",
      "loss是： tensor(107.1017, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 16.779661016949152\n",
      "loss是： tensor(65.2137, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 40.65573770491803\n",
      "loss是： tensor(128.1271, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 30.46875\n",
      "loss是： tensor(109.0260, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 50.42253521126761\n",
      "loss是： tensor(132.5128, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 43.33333333333333\n",
      "loss是： tensor(140.8299, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 26.470588235294116\n",
      "loss是： tensor(93.3684, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 27.166666666666668\n",
      "loss是： tensor(94.9019, device='cuda:0', grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert计算的loss是： 33.859649122807014\n",
      "loss是： tensor(90.1079, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 20.123456790123456\n",
      "loss是： tensor(67.5198, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 62.0\n",
      "loss是： tensor(130.6809, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 31.538461538461537\n",
      "loss是： tensor(79.8673, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 4.827586206896552\n",
      "loss是： tensor(52.4935, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 42.57142857142857\n",
      "loss是： tensor(108.3210, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 8.235294117647058\n",
      "loss是： tensor(45.0009, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 41.42857142857143\n",
      "loss是： tensor(160.9434, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 36.30769230769231\n",
      "loss是： tensor(97.2780, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 18.11320754716981\n",
      "loss是： tensor(99.9559, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 30.3125\n",
      "loss是： tensor(97.4095, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 32.65625\n",
      "loss是： tensor(85.8081, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 31.11111111111111\n",
      "loss是： tensor(106.7708, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 27.868852459016395\n",
      "loss是： tensor(94.8482, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 25.8\n",
      "loss是： tensor(86.7703, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 29.6875\n",
      "loss是： tensor(100.3684, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 32.241379310344826\n",
      "loss是： tensor(79.8316, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 25.81081081081081\n",
      "loss是： tensor(101.0666, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 62.666666666666664\n",
      "loss是： tensor(139.2713, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 21.449275362318843\n",
      "loss是： tensor(76.4690, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 30.5\n",
      "loss是： tensor(124.9425, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 30.555555555555554\n",
      "loss是： tensor(77.5121, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 26.216216216216214\n",
      "loss是： tensor(79.8616, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 37.07692307692308\n",
      "loss是： tensor(119.1382, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 41.06060606060606\n",
      "loss是： tensor(126.3635, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 46.0\n",
      "loss是： tensor(120.3116, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 32.57142857142857\n",
      "loss是： tensor(89.6273, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 50.0\n",
      "loss是： tensor(127.3297, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 30.588235294117645\n",
      "loss是： tensor(106.0438, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 34.11764705882353\n",
      "loss是： tensor(80.1226, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 30.166666666666664\n",
      "loss是： tensor(104.0966, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 52.74193548387097\n",
      "loss是： tensor(126.1094, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 30.51724137931035\n",
      "loss是： tensor(61.5643, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 10.833333333333332\n",
      "loss是： tensor(69.0142, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 56.417910447761194\n",
      "loss是： tensor(126.4457, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 31.052631578947366\n",
      "loss是： tensor(138.1569, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 40.0\n",
      "loss是： tensor(91.9765, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 45.166666666666664\n",
      "loss是： tensor(116.8722, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 36.61538461538461\n",
      "loss是： tensor(105.6496, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 39.285714285714285\n",
      "loss是： tensor(79.2294, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 30.263157894736842\n",
      "loss是： tensor(123.8567, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 27.53846153846154\n",
      "loss是： tensor(116.1396, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 39.02777777777778\n",
      "loss是： tensor(140.8018, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 24.328358208955226\n",
      "loss是： tensor(78.1918, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 39.54545454545455\n",
      "loss是： tensor(112.8794, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 32.61538461538461\n",
      "loss是： tensor(107.6458, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 40.51724137931035\n",
      "loss是： tensor(87.0478, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 26.610169491525426\n",
      "loss是： tensor(88.0564, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 36.166666666666664\n",
      "loss是： tensor(105.2959, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 26.90909090909091\n",
      "loss是： tensor(84.4406, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 32.41379310344827\n",
      "loss是： tensor(107.0833, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 28.70967741935484\n",
      "loss是： tensor(103.5991, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 22.05128205128205\n",
      "loss是： tensor(67.9856, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "280/300\n",
      "bert计算的loss是： 26.0\n",
      "loss是： tensor(105.0033, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 37.971014492753625\n",
      "loss是： tensor(115.9642, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 26.0\n",
      "loss是： tensor(136.1385, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 71.23076923076923\n",
      "loss是： tensor(161.0171, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 35.75757575757576\n",
      "loss是： tensor(98.3957, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 36.271186440677965\n",
      "loss是： tensor(123.2252, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 33.80281690140845\n",
      "loss是： tensor(99.4021, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 38.833333333333336\n",
      "loss是： tensor(87.2533, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 14.827586206896552\n",
      "loss是： tensor(94.7146, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 15.384615384615385\n",
      "loss是： tensor(75.1282, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 30.0\n",
      "loss是： tensor(86.0028, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 35.44117647058823\n",
      "loss是： tensor(83.9745, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 16.470588235294116\n",
      "loss是： tensor(94.6164, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 16.5625\n",
      "loss是： tensor(83.9313, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 25.901639344262296\n",
      "loss是： tensor(65.9884, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 35.3125\n",
      "loss是： tensor(111.3547, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 35.0\n",
      "loss是： tensor(101.5948, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 51.166666666666664\n",
      "loss是： tensor(105.6368, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 32.46376811594203\n",
      "loss是： tensor(111.7277, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 36.15384615384615\n",
      "loss是： tensor(122.3286, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 9.827586206896552\n",
      "loss是： tensor(77.3567, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 26.440677966101696\n",
      "loss是： tensor(82.9822, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 35.13513513513514\n",
      "loss是： tensor(70.5542, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 36.666666666666664\n",
      "loss是： tensor(111.7626, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 27.833333333333332\n",
      "loss是： tensor(102.9788, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 34.0625\n",
      "loss是： tensor(107.9364, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 36.911764705882355\n",
      "loss是： tensor(127.2861, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 41.25\n",
      "loss是： tensor(88.9904, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 13.91891891891892\n",
      "loss是： tensor(71.6070, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 32.08955223880597\n",
      "loss是： tensor(88.9038, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 30.923076923076923\n",
      "loss是： tensor(126.8656, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 27.65625\n",
      "loss是： tensor(117.6121, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 28.902439024390244\n",
      "loss是： tensor(86.5771, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 24.92957746478873\n",
      "loss是： tensor(87.2870, device='cuda:0', grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert计算的loss是： 43.278688524590166\n",
      "loss是： tensor(120.4298, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 36.885245901639344\n",
      "loss是： tensor(108.9615, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 32.07547169811321\n",
      "loss是： tensor(92.8438, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 34.03225806451613\n",
      "loss是： tensor(97.2791, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 3.114754098360656\n",
      "loss是： tensor(71.0641, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 31.296296296296298\n",
      "loss是： tensor(92.3424, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 42.5\n",
      "loss是： tensor(108.3712, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 52.1875\n",
      "loss是： tensor(137.1581, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 33.9622641509434\n",
      "loss是： tensor(87.1270, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 34.12698412698413\n",
      "loss是： tensor(86.8077, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 28.095238095238095\n",
      "loss是： tensor(97.2045, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 32.1875\n",
      "loss是： tensor(68.4474, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 21.016949152542374\n",
      "loss是： tensor(75.2684, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 40.60606060606061\n",
      "loss是： tensor(112.5028, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 26.451612903225804\n",
      "loss是： tensor(103.2592, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 32.12121212121212\n",
      "loss是： tensor(112.6373, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 28.983050847457626\n",
      "loss是： tensor(90.2279, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 30.88235294117647\n",
      "loss是： tensor(95.3535, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 53.49206349206349\n",
      "loss是： tensor(118.3118, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 27.72727272727273\n",
      "loss是： tensor(81.4696, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 35.892857142857146\n",
      "loss是： tensor(79.1594, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 48.125\n",
      "loss是： tensor(119.6046, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 35.96774193548387\n",
      "loss是： tensor(134.9387, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 41.81818181818181\n",
      "loss是： tensor(98.2189, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 45.362318840579704\n",
      "loss是： tensor(122.8235, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 37.05882352941177\n",
      "loss是： tensor(108.9403, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 32.241379310344826\n",
      "loss是： tensor(115.7319, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 17.258064516129032\n",
      "loss是： tensor(61.6849, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 35.714285714285715\n",
      "loss是： tensor(107.5541, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 23.846153846153847\n",
      "loss是： tensor(71.5279, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 22.5\n",
      "loss是： tensor(75.5812, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 22.131147540983605\n",
      "loss是： tensor(69.1601, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 29.23076923076923\n",
      "loss是： tensor(83.5498, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 33.666666666666664\n",
      "loss是： tensor(86.3274, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 38.22222222222222\n",
      "loss是： tensor(68.2505, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 26.29032258064516\n",
      "loss是： tensor(79.9574, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 25.0\n",
      "loss是： tensor(66.8207, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 80.17857142857142\n",
      "loss是： tensor(149.2461, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 13.75\n",
      "loss是： tensor(95.4084, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 44.44444444444444\n",
      "loss是： tensor(97.7436, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 25.692307692307693\n",
      "loss是： tensor(100.9348, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 35.483870967741936\n",
      "loss是： tensor(97.8842, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 34.85294117647059\n",
      "loss是： tensor(104.7927, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 38.333333333333336\n",
      "loss是： tensor(86.0370, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 27.647058823529413\n",
      "loss是： tensor(100.9979, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 34.56140350877193\n",
      "loss是： tensor(116.8074, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 25.59322033898305\n",
      "loss是： tensor(78.0892, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 36.785714285714285\n",
      "loss是： tensor(96.7234, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 40.0\n",
      "loss是： tensor(114.9459, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 30.46875\n",
      "loss是： tensor(85.7868, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 30.3125\n",
      "loss是： tensor(118.3208, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 25.43859649122807\n",
      "loss是： tensor(74.7304, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 30.579710144927535\n",
      "loss是： tensor(110.3610, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 42.608695652173914\n",
      "loss是： tensor(100.3920, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 28.666666666666668\n",
      "loss是： tensor(99.6080, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 31.11111111111111\n",
      "loss是： tensor(105.3522, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 24.307692307692307\n",
      "loss是： tensor(95.7799, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 32.285714285714285\n",
      "loss是： tensor(65.3277, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 33.73134328358209\n",
      "loss是： tensor(93.2228, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 29.64912280701754\n",
      "loss是： tensor(104.5762, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 48.727272727272734\n",
      "loss是： tensor(100.3783, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 41.42857142857143\n",
      "loss是： tensor(118.0979, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 35.84615384615385\n",
      "loss是： tensor(134.9337, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 23.0\n",
      "loss是： tensor(73.8597, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 34.266666666666666\n",
      "loss是： tensor(111.1948, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 36.515151515151516\n",
      "loss是： tensor(104.7222, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 24.246575342465754\n",
      "loss是： tensor(96.9973, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 32.42424242424242\n",
      "loss是： tensor(107.2803, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 22.686567164179102\n",
      "loss是： tensor(109.8481, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 28.688524590163933\n",
      "loss是： tensor(85.2350, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 25.757575757575758\n",
      "loss是： tensor(86.0132, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 45.53571428571429\n",
      "loss是： tensor(108.8806, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 65.53571428571429\n",
      "loss是： tensor(115.9153, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 39.67741935483871\n",
      "loss是： tensor(133.3195, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 34.30769230769231\n",
      "loss是： tensor(106.9294, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 24.915254237288135\n",
      "loss是： tensor(87.2524, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 30.344827586206897\n",
      "loss是： tensor(95.8897, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 31.69230769230769\n",
      "loss是： tensor(94.9721, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 39.40298507462686\n",
      "loss是： tensor(103.2670, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 34.074074074074076\n",
      "loss是： tensor(81.6223, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 35.588235294117645\n",
      "loss是： tensor(66.2654, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 32.608695652173914\n",
      "loss是： tensor(80.7476, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 23.582089552238806\n",
      "loss是： tensor(87.1302, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 18.73015873015873\n",
      "loss是： tensor(80.6503, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 29.130434782608695\n",
      "loss是： tensor(82.4120, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 38.61538461538461\n",
      "loss是： tensor(119.9242, device='cuda:0', grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert计算的loss是： 24.76923076923077\n",
      "loss是： tensor(93.2723, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 25.714285714285715\n",
      "loss是： tensor(64.4163, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 23.015873015873012\n",
      "loss是： tensor(105.3113, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 30.217391304347828\n",
      "loss是： tensor(96.1426, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 41.66666666666667\n",
      "loss是： tensor(94.8715, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 29.63636363636364\n",
      "loss是： tensor(100.9108, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 35.87301587301587\n",
      "loss是： tensor(90.9561, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 28.78787878787879\n",
      "loss是： tensor(84.5640, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 26.825396825396822\n",
      "loss是： tensor(77.7423, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 22.419354838709676\n",
      "loss是： tensor(80.3739, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 0.0\n",
      "loss是： tensor(61.4382, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 37.41379310344827\n",
      "loss是： tensor(112.7270, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 39.152542372881356\n",
      "loss是： tensor(94.1897, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 44.42857142857143\n",
      "loss是： tensor(112.9848, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 48.771929824561404\n",
      "loss是： tensor(109.4907, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 63.692307692307686\n",
      "loss是： tensor(136.5359, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 28.253968253968257\n",
      "loss是： tensor(83.6532, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 42.10526315789473\n",
      "loss是： tensor(94.7748, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 43.389830508474574\n",
      "loss是： tensor(110.1430, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 25.714285714285715\n",
      "loss是： tensor(71.8024, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 28.666666666666668\n",
      "loss是： tensor(91.4560, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 30.144927536231883\n",
      "loss是： tensor(91.9276, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 21.515151515151516\n",
      "loss是： tensor(59.2515, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 28.985507246376812\n",
      "loss是： tensor(86.6189, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 35.08474576271186\n",
      "loss是： tensor(105.9792, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 35.26315789473684\n",
      "loss是： tensor(115.6750, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 26.769230769230766\n",
      "loss是： tensor(100.0366, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 37.11538461538461\n",
      "loss是： tensor(74.1610, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 35.08196721311475\n",
      "loss是： tensor(105.3089, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 25.238095238095237\n",
      "loss是： tensor(106.3258, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 30.29850746268657\n",
      "loss是： tensor(89.1736, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 30.869565217391305\n",
      "loss是： tensor(104.7375, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 30.8955223880597\n",
      "loss是： tensor(104.4359, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 34.516129032258064\n",
      "loss是： tensor(92.0157, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 41.34328358208955\n",
      "loss是： tensor(102.1134, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 29.275362318840582\n",
      "loss是： tensor(93.5304, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 32.41935483870967\n",
      "loss是： tensor(99.5190, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 40.72727272727272\n",
      "loss是： tensor(101.5947, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 27.39130434782609\n",
      "loss是： tensor(122.6044, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 7.272727272727273\n",
      "loss是： tensor(66.4873, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 22.439024390243905\n",
      "loss是： tensor(69.6383, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 26.923076923076927\n",
      "loss是： tensor(77.0232, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 45.892857142857146\n",
      "loss是： tensor(109.9562, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 33.970588235294116\n",
      "loss是： tensor(103.0405, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 36.39344262295082\n",
      "loss是： tensor(106.5485, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 29.55223880597015\n",
      "loss是： tensor(90.0813, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 42.75862068965517\n",
      "loss是： tensor(96.4231, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 7.4576271186440675\n",
      "loss是： tensor(59.9861, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 32.25352112676057\n",
      "loss是： tensor(105.0776, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 32.42857142857143\n",
      "loss是： tensor(120.9799, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 37.0\n",
      "loss是： tensor(100.9881, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 46.37931034482759\n",
      "loss是： tensor(118.0937, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 33.38235294117647\n",
      "loss是： tensor(118.7297, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 19.491525423728813\n",
      "loss是： tensor(83.3955, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 31.666666666666664\n",
      "loss是： tensor(85.6649, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 34.47761194029851\n",
      "loss是： tensor(107.7538, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 2.1875\n",
      "loss是： tensor(85.9655, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 26.315789473684212\n",
      "loss是： tensor(71.4187, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 33.96551724137931\n",
      "loss是： tensor(122.0080, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 31.186440677966104\n",
      "loss是： tensor(83.7261, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 34.179104477611936\n",
      "loss是： tensor(120.2878, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 33.0188679245283\n",
      "loss是： tensor(87.7670, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 26.538461538461537\n",
      "loss是： tensor(101.3061, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 10.857142857142856\n",
      "loss是： tensor(39.2136, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 32.3943661971831\n",
      "loss是： tensor(108.8794, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 38.13559322033898\n",
      "loss是： tensor(78.1363, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 30.0\n",
      "loss是： tensor(93.2198, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 40.87719298245614\n",
      "loss是： tensor(114.0573, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 28.823529411764707\n",
      "loss是： tensor(113.5557, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 13.866666666666667\n",
      "loss是： tensor(90.2730, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 32.3728813559322\n",
      "loss是： tensor(89.7317, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 15.964912280701755\n",
      "loss是： tensor(86.9975, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 29.821428571428573\n",
      "loss是： tensor(78.5876, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 28.412698412698415\n",
      "loss是： tensor(88.6917, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 44.0\n",
      "loss是： tensor(111.4389, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 24.035087719298247\n",
      "loss是： tensor(57.8188, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 32.76923076923077\n",
      "loss是： tensor(105.3037, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 31.11111111111111\n",
      "loss是： tensor(92.3864, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 25.57377049180328\n",
      "loss是： tensor(92.5382, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 14.411764705882353\n",
      "loss是： tensor(70.6758, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 24.153846153846153\n",
      "loss是： tensor(63.0085, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 31.166666666666668\n",
      "loss是： tensor(98.5951, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 39.629629629629626\n",
      "loss是： tensor(86.0965, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 41.0\n",
      "loss是： tensor(103.1854, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 38.37837837837838\n",
      "loss是： tensor(103.3639, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 78.38709677419355\n",
      "loss是： tensor(147.7707, device='cuda:0', grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert计算的loss是： 53.5\n",
      "loss是： tensor(102.2705, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 24.0\n",
      "loss是： tensor(88.7428, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 19.43661971830986\n",
      "loss是： tensor(100.5318, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 37.23076923076923\n",
      "loss是： tensor(125.5832, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 33.63636363636364\n",
      "loss是： tensor(99.7075, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 27.61904761904762\n",
      "loss是： tensor(96.1893, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 40.49180327868853\n",
      "loss是： tensor(103.1863, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 24.307692307692307\n",
      "loss是： tensor(90.5470, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 38.05555555555556\n",
      "loss是： tensor(115.7387, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 28.07017543859649\n",
      "loss是： tensor(91.9039, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 17.258064516129032\n",
      "loss是： tensor(72.3103, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 39.2063492063492\n",
      "loss是： tensor(97.4003, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 28.732394366197184\n",
      "loss是： tensor(97.9014, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 40.81967213114754\n",
      "loss是： tensor(91.6216, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 44.92753623188406\n",
      "loss是： tensor(107.9538, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 54.48275862068965\n",
      "loss是： tensor(110.6832, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 27.82608695652174\n",
      "loss是： tensor(66.2220, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 26.779661016949152\n",
      "loss是： tensor(88.2869, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 15.0\n",
      "loss是： tensor(79.6248, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 44.73684210526316\n",
      "loss是： tensor(117.5750, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 42.83333333333333\n",
      "loss是： tensor(99.0410, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 28.548387096774196\n",
      "loss是： tensor(84.9864, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 35.15151515151515\n",
      "loss是： tensor(105.1322, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 38.86363636363636\n",
      "loss是： tensor(113.0728, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "290/300\n",
      "bert计算的loss是： 29.23076923076923\n",
      "loss是： tensor(89.3452, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 39.682539682539684\n",
      "loss是： tensor(91.3184, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 39.861111111111114\n",
      "loss是： tensor(106.0905, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 24.76923076923077\n",
      "loss是： tensor(98.6204, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 23.636363636363637\n",
      "loss是： tensor(83.2703, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 35.535714285714285\n",
      "loss是： tensor(86.8458, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 25.245901639344265\n",
      "loss是： tensor(75.2653, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 33.392857142857146\n",
      "loss是： tensor(88.0031, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 28.275862068965516\n",
      "loss是： tensor(86.8343, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 57.84615384615385\n",
      "loss是： tensor(169.5058, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 36.86274509803921\n",
      "loss是： tensor(108.9576, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 34.34210526315789\n",
      "loss是： tensor(107.9895, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 38.38235294117647\n",
      "loss是： tensor(92.3703, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 36.77966101694915\n",
      "loss是： tensor(126.3848, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 32.98245614035088\n",
      "loss是： tensor(93.6939, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 24.074074074074076\n",
      "loss是： tensor(71.7456, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 48.59154929577465\n",
      "loss是： tensor(146.0938, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 28.771929824561404\n",
      "loss是： tensor(91.1329, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 19.821428571428573\n",
      "loss是： tensor(79.5253, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 43.28358208955223\n",
      "loss是： tensor(113.2161, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 34.54545454545455\n",
      "loss是： tensor(100.7826, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 12.307692307692308\n",
      "loss是： tensor(77.4293, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 24.04255319148936\n",
      "loss是： tensor(49.5869, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 37.32142857142857\n",
      "loss是： tensor(95.4244, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 28.450704225352116\n",
      "loss是： tensor(77.6211, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 55.45454545454546\n",
      "loss是： tensor(124.0356, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 35.88235294117647\n",
      "loss是： tensor(87.5681, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 23.88059701492537\n",
      "loss是： tensor(102.2051, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 39.54545454545455\n",
      "loss是： tensor(90.4693, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 28.181818181818183\n",
      "loss是： tensor(89.8848, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 21.88679245283019\n",
      "loss是： tensor(63.4783, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 41.76470588235294\n",
      "loss是： tensor(113.0256, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 26.52777777777778\n",
      "loss是： tensor(96.8548, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 32.03389830508475\n",
      "loss是： tensor(81.0969, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 29.692307692307693\n",
      "loss是： tensor(84.9310, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 34.83870967741935\n",
      "loss是： tensor(104.9842, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 32.786885245901644\n",
      "loss是： tensor(77.4972, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 36.62162162162162\n",
      "loss是： tensor(96.3923, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 40.32786885245901\n",
      "loss是： tensor(114.5821, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 51.52173913043478\n",
      "loss是： tensor(115.4320, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 40.317460317460316\n",
      "loss是： tensor(106.5870, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 28.666666666666668\n",
      "loss是： tensor(78.0437, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 34.411764705882355\n",
      "loss是： tensor(84.6838, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 29.661016949152543\n",
      "loss是： tensor(107.4349, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 38.30769230769231\n",
      "loss是： tensor(93.3231, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 27.38095238095238\n",
      "loss是： tensor(82.2051, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 37.916666666666664\n",
      "loss是： tensor(83.6465, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 39.03225806451613\n",
      "loss是： tensor(121.7454, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 24.393939393939394\n",
      "loss是： tensor(93.8062, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 27.205882352941178\n",
      "loss是： tensor(80.7638, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 34.285714285714285\n",
      "loss是： tensor(109.3352, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 26.619718309859156\n",
      "loss是： tensor(96.2810, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 31.52542372881356\n",
      "loss是： tensor(86.9180, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 32.96296296296296\n",
      "loss是： tensor(76.3986, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 39.64912280701754\n",
      "loss是： tensor(123.1544, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 46.02941176470588\n",
      "loss是： tensor(97.3285, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 28.636363636363637\n",
      "loss是： tensor(96.9159, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 36.26865671641791\n",
      "loss是： tensor(106.9230, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 9.242424242424242\n",
      "loss是： tensor(71.4115, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 31.578947368421055\n",
      "loss是： tensor(109.2977, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 21.71875\n",
      "loss是： tensor(104.0646, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 41.31147540983606\n",
      "loss是： tensor(106.7450, device='cuda:0', grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert计算的loss是： 30.0\n",
      "loss是： tensor(110.8630, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 38.26086956521739\n",
      "loss是： tensor(87.1143, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 25.833333333333336\n",
      "loss是： tensor(73.5176, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 32.10526315789474\n",
      "loss是： tensor(109.5245, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 39.2063492063492\n",
      "loss是： tensor(98.8047, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 26.666666666666664\n",
      "loss是： tensor(110.0938, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 32.093023255813954\n",
      "loss是： tensor(66.6059, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 38.19672131147541\n",
      "loss是： tensor(109.5546, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 40.166666666666664\n",
      "loss是： tensor(96.9393, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 34.60317460317461\n",
      "loss是： tensor(109.7355, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 25.084745762711865\n",
      "loss是： tensor(73.7913, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 46.19047619047619\n",
      "loss是： tensor(109.0622, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 30.158730158730158\n",
      "loss是： tensor(81.6085, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 24.328358208955226\n",
      "loss是： tensor(69.4668, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 28.87323943661972\n",
      "loss是： tensor(100.8493, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 18.225806451612904\n",
      "loss是： tensor(70.9111, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 41.07692307692307\n",
      "loss是： tensor(123.2879, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 37.74647887323943\n",
      "loss是： tensor(106.5615, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 24.48275862068965\n",
      "loss是： tensor(80.5415, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 9.545454545454547\n",
      "loss是： tensor(81.4798, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 63.11475409836065\n",
      "loss是： tensor(124.8356, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 37.53846153846154\n",
      "loss是： tensor(92.6617, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 26.875\n",
      "loss是： tensor(126.1611, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 35.93220338983051\n",
      "loss是： tensor(107.2127, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 31.052631578947366\n",
      "loss是： tensor(92.5577, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 25.40983606557377\n",
      "loss是： tensor(78.5772, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 36.11940298507463\n",
      "loss是： tensor(103.5411, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 34.791666666666664\n",
      "loss是： tensor(78.9570, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 29.508196721311478\n",
      "loss是： tensor(71.0851, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 41.81818181818181\n",
      "loss是： tensor(92.9384, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 27.833333333333332\n",
      "loss是： tensor(85.6490, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 37.857142857142854\n",
      "loss是： tensor(91.8116, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 31.296296296296298\n",
      "loss是： tensor(105.5053, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 42.857142857142854\n",
      "loss是： tensor(107.7181, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 36.03448275862069\n",
      "loss是： tensor(93.6843, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 28.0\n",
      "loss是： tensor(72.9811, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 41.86046511627907\n",
      "loss是： tensor(85.7860, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 36.09375\n",
      "loss是： tensor(97.5194, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 26.71875\n",
      "loss是： tensor(101.1611, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 38.07017543859649\n",
      "loss是： tensor(130.0481, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 31.774193548387096\n",
      "loss是： tensor(74.8602, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 25.074626865671643\n",
      "loss是： tensor(76.7761, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 39.09090909090909\n",
      "loss是： tensor(103.6750, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 25.416666666666664\n",
      "loss是： tensor(94.6537, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 67.27272727272728\n",
      "loss是： tensor(134.1235, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 28.985507246376812\n",
      "loss是： tensor(82.9619, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 34.20289855072464\n",
      "loss是： tensor(96.9398, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 30.615384615384613\n",
      "loss是： tensor(98.1619, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 29.137931034482758\n",
      "loss是： tensor(76.4900, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 36.5625\n",
      "loss是： tensor(90.7469, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 33.23529411764706\n",
      "loss是： tensor(100.4098, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 32.5\n",
      "loss是： tensor(91.5748, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 10.869565217391305\n",
      "loss是： tensor(46.2904, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 28.24561403508772\n",
      "loss是： tensor(90.5909, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 24.915254237288135\n",
      "loss是： tensor(73.3354, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 25.857142857142858\n",
      "loss是： tensor(76.6246, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 29.444444444444446\n",
      "loss是： tensor(82.2776, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 45.78125\n",
      "loss是： tensor(111.4564, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 26.21212121212121\n",
      "loss是： tensor(83.3306, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 33.559322033898304\n",
      "loss是： tensor(120.0759, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 28.166666666666668\n",
      "loss是： tensor(93.7134, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 36.46153846153846\n",
      "loss是： tensor(122.1550, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 29.333333333333332\n",
      "loss是： tensor(74.9318, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 37.05882352941177\n",
      "loss是： tensor(92.5021, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 34.02985074626866\n",
      "loss是： tensor(103.0954, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 14.6875\n",
      "loss是： tensor(66.0165, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 40.8\n",
      "loss是： tensor(104.0641, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 15.517241379310345\n",
      "loss是： tensor(78.8170, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 23.333333333333336\n",
      "loss是： tensor(90.0054, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 62.38095238095238\n",
      "loss是： tensor(121.9684, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 29.076923076923077\n",
      "loss是： tensor(78.6102, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 41.34328358208955\n",
      "loss是： tensor(109.2642, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 15.217391304347828\n",
      "loss是： tensor(82.7765, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 29.411764705882355\n",
      "loss是： tensor(97.4992, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 49.32203389830509\n",
      "loss是： tensor(114.4826, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 28.4375\n",
      "loss是： tensor(55.8441, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 25.52238805970149\n",
      "loss是： tensor(106.3610, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 31.71428571428571\n",
      "loss是： tensor(93.0525, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 33.28125\n",
      "loss是： tensor(97.2848, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 73.66666666666666\n",
      "loss是： tensor(125.5111, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 33.529411764705884\n",
      "loss是： tensor(100.5288, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 29.726027397260275\n",
      "loss是： tensor(75.2706, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 46.22641509433962\n",
      "loss是： tensor(118.6548, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 38.59649122807018\n",
      "loss是： tensor(118.5822, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 36.666666666666664\n",
      "loss是： tensor(87.4006, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 26.62162162162162\n",
      "loss是： tensor(104.0680, device='cuda:0', grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert计算的loss是： 33.529411764705884\n",
      "loss是： tensor(108.5231, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 37.540983606557376\n",
      "loss是： tensor(105.3879, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 34.30769230769231\n",
      "loss是： tensor(103.7233, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 31.29032258064516\n",
      "loss是： tensor(110.9715, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 21.515151515151516\n",
      "loss是： tensor(90.7600, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 44.91525423728813\n",
      "loss是： tensor(122.4815, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 35.14705882352941\n",
      "loss是： tensor(111.6702, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 31.578947368421055\n",
      "loss是： tensor(99.4149, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 28.666666666666668\n",
      "loss是： tensor(69.9974, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 30.925925925925924\n",
      "loss是： tensor(93.4792, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 3.4848484848484853\n",
      "loss是： tensor(85.0387, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 23.076923076923073\n",
      "loss是： tensor(75.3625, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 26.666666666666664\n",
      "loss是： tensor(72.9829, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 29.193548387096776\n",
      "loss是： tensor(91.7383, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 27.183098591549296\n",
      "loss是： tensor(85.5872, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 54.39393939393939\n",
      "loss是： tensor(98.7486, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 38.888888888888886\n",
      "loss是： tensor(102.7776, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 25.689655172413794\n",
      "loss是： tensor(83.9554, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 29.523809523809526\n",
      "loss是： tensor(99.9526, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 7.704918032786885\n",
      "loss是： tensor(78.8337, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 2.112676056338028\n",
      "loss是： tensor(82.0705, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 50.60606060606061\n",
      "loss是： tensor(134.6366, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 26.81159420289855\n",
      "loss是： tensor(95.5097, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 37.16981132075472\n",
      "loss是： tensor(108.6419, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 44.82142857142857\n",
      "loss是： tensor(117.1067, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 38.421052631578945\n",
      "loss是： tensor(91.2465, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 33.75\n",
      "loss是： tensor(83.5994, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 26.615384615384613\n",
      "loss是： tensor(71.9633, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 8.225806451612904\n",
      "loss是： tensor(56.5705, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 10.56338028169014\n",
      "loss是： tensor(87.4481, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 21.818181818181817\n",
      "loss是： tensor(62.6785, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 37.5\n",
      "loss是： tensor(70.3735, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 29.0\n",
      "loss是： tensor(99.5422, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 33.888888888888886\n",
      "loss是： tensor(70.1112, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 33.114754098360656\n",
      "loss是： tensor(81.6980, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 28.571428571428573\n",
      "loss是： tensor(58.5492, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 15.272727272727273\n",
      "loss是： tensor(65.8063, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 38.92307692307692\n",
      "loss是： tensor(97.6161, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 60.5\n",
      "loss是： tensor(138.8635, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 29.6875\n",
      "loss是： tensor(100.2337, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 37.27272727272727\n",
      "loss是： tensor(106.7264, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 31.492537313432834\n",
      "loss是： tensor(97.5091, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 24.21875\n",
      "loss是： tensor(81.4088, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 26.94915254237288\n",
      "loss是： tensor(88.9097, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 26.176470588235293\n",
      "loss是： tensor(70.5812, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 33.89830508474576\n",
      "loss是： tensor(85.9314, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 31.904761904761905\n",
      "loss是： tensor(92.8145, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 38.42857142857143\n",
      "loss是： tensor(104.6147, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 39.55882352941177\n",
      "loss是： tensor(123.4058, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 28.771929824561404\n",
      "loss是： tensor(94.1481, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 49.848484848484844\n",
      "loss是： tensor(126.6967, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 35.53846153846153\n",
      "loss是： tensor(70.1916, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 2.8571428571428568\n",
      "loss是： tensor(56.3075, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 31.346153846153847\n",
      "loss是： tensor(82.3781, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 38.70967741935484\n",
      "loss是： tensor(80.1139, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 37.903225806451616\n",
      "loss是： tensor(125.5867, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 31.85185185185185\n",
      "loss是： tensor(94.8251, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 26.885245901639344\n",
      "loss是： tensor(64.7719, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 34.90196078431373\n",
      "loss是： tensor(72.8241, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "正在训练第 3轮\n",
      "Model(\n",
      "  (gen_embedding): Embedding(8518, 300)\n",
      "  (conv1): Conv1d(300, 128, kernel_size=(5,), stride=(1,), padding=(2,))\n",
      "  (conv2): Conv1d(300, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "  (dropout): Dropout(p=0.55, inplace=False)\n",
      "  (conv3): Conv1d(256, 256, kernel_size=(5,), stride=(1,), padding=(2,))\n",
      "  (conv4): Conv1d(256, 256, kernel_size=(5,), stride=(1,), padding=(2,))\n",
      "  (conv5): Conv1d(256, 256, kernel_size=(5,), stride=(1,), padding=(2,))\n",
      "  (linear_ae): Linear(in_features=256, out_features=3, bias=True)\n",
      "  (crf): ConditionalRandomField()\n",
      ")\n",
      "0/300\n",
      "10/300\n",
      "20/300\n",
      "30/300\n",
      "40/300\n",
      "50/300\n",
      "60/300\n",
      "70/300\n",
      "80/300\n",
      "90/300\n",
      "100/300\n",
      "110/300\n",
      "120/300\n",
      "130/300\n",
      "140/300\n",
      "150/300\n",
      "160/300\n",
      "170/300\n",
      "180/300\n",
      "190/300\n",
      "200/300\n",
      "210/300\n",
      "220/300\n",
      "230/300\n",
      "240/300\n",
      "bert计算的loss是： 36.166666666666664\n",
      "loss是： tensor(86.0052, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 29.15492957746479\n",
      "loss是： tensor(97.6629, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 20.694444444444446\n",
      "loss是： tensor(101.4574, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 52.622950819672134\n",
      "loss是： tensor(95.0670, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 35.714285714285715\n",
      "loss是： tensor(97.7651, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 31.884057971014492\n",
      "loss是： tensor(125.7719, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 28.90625\n",
      "loss是： tensor(111.9749, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 39.8360655737705\n",
      "loss是： tensor(105.7438, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 30.925925925925924\n",
      "loss是： tensor(74.1136, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 31.147540983606557\n",
      "loss是： tensor(80.7666, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 25.087719298245613\n",
      "loss是： tensor(81.3299, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 27.63157894736842\n",
      "loss是： tensor(89.6356, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 33.59375\n",
      "loss是： tensor(86.4530, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 35.535714285714285\n",
      "loss是： tensor(72.6424, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 19.29824561403509\n",
      "loss是： tensor(81.3755, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 18.970588235294116\n",
      "loss是： tensor(74.3570, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 25.57142857142857\n",
      "loss是： tensor(83.5382, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 28.615384615384617\n",
      "loss是： tensor(102.4427, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 37.8125\n",
      "loss是： tensor(117.8715, device='cuda:0', grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert计算的loss是： 22.878787878787882\n",
      "loss是： tensor(83.5034, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 10.0\n",
      "loss是： tensor(52.1123, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 41.61290322580645\n",
      "loss是： tensor(99.2023, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 51.0344827586207\n",
      "loss是： tensor(93.6181, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "250/300\n",
      "bert计算的loss是： 35.96491228070175\n",
      "loss是： tensor(86.1263, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 46.849315068493155\n",
      "loss是： tensor(118.1313, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 31.400000000000002\n",
      "loss是： tensor(88.2551, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 28.421052631578945\n",
      "loss是： tensor(72.8125, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 28.07017543859649\n",
      "loss是： tensor(91.7441, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 34.90566037735849\n",
      "loss是： tensor(69.5533, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 28.5\n",
      "loss是： tensor(76.5830, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 21.73913043478261\n",
      "loss是： tensor(96.5677, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 41.0\n",
      "loss是： tensor(106.4375, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 29.71014492753623\n",
      "loss是： tensor(91.0572, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 35.08474576271186\n",
      "loss是： tensor(87.9496, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 16.901408450704224\n",
      "loss是： tensor(76.9406, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 43.07692307692307\n",
      "loss是： tensor(106.2212, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 31.818181818181817\n",
      "loss是： tensor(113.0200, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 36.83333333333333\n",
      "loss是： tensor(98.0123, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 33.0188679245283\n",
      "loss是： tensor(81.8922, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 16.81159420289855\n",
      "loss是： tensor(85.2104, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 47.63636363636364\n",
      "loss是： tensor(98.5884, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 33.57142857142857\n",
      "loss是： tensor(66.9547, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 30.909090909090907\n",
      "loss是： tensor(106.9849, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 35.59322033898305\n",
      "loss是： tensor(101.8932, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 0.3076923076923077\n",
      "loss是： tensor(80.3518, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 37.111111111111114\n",
      "loss是： tensor(79.4806, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 29.6875\n",
      "loss是： tensor(115.9088, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 27.37704918032787\n",
      "loss是： tensor(78.4054, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 28.703703703703702\n",
      "loss是： tensor(69.3016, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 33.28358208955224\n",
      "loss是： tensor(96.5353, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 35.762711864406775\n",
      "loss是： tensor(111.3420, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 38.8235294117647\n",
      "loss是： tensor(94.1783, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 30.0\n",
      "loss是： tensor(68.5861, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 34.56140350877193\n",
      "loss是： tensor(96.0850, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 27.540983606557376\n",
      "loss是： tensor(81.4864, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 37.241379310344826\n",
      "loss是： tensor(80.2533, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 28.0\n",
      "loss是： tensor(83.4399, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 18.4375\n",
      "loss是： tensor(66.3180, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 30.29850746268657\n",
      "loss是： tensor(91.5648, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 42.22222222222222\n",
      "loss是： tensor(119.3036, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 33.896103896103895\n",
      "loss是： tensor(114.2931, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 41.09589041095891\n",
      "loss是： tensor(120.4576, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 32.03125\n",
      "loss是： tensor(93.3809, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 28.28125\n",
      "loss是： tensor(80.3383, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 37.868852459016395\n",
      "loss是： tensor(88.2868, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 23.103448275862068\n",
      "loss是： tensor(76.5939, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 29.565217391304344\n",
      "loss是： tensor(94.0536, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 24.74576271186441\n",
      "loss是： tensor(61.7498, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 27.878787878787882\n",
      "loss是： tensor(50.4452, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 42.53731343283582\n",
      "loss是： tensor(105.3357, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 28.484848484848484\n",
      "loss是： tensor(95.0658, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 46.36363636363637\n",
      "loss是： tensor(93.6533, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 37.96610169491525\n",
      "loss是： tensor(101.8672, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 40.33898305084746\n",
      "loss是： tensor(97.8226, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 29.661016949152543\n",
      "loss是： tensor(86.1943, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 34.67741935483871\n",
      "loss是： tensor(101.8897, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 31.80327868852459\n",
      "loss是： tensor(103.8234, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 30.793650793650794\n",
      "loss是： tensor(90.4998, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 23.78787878787879\n",
      "loss是： tensor(87.0079, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 36.12903225806451\n",
      "loss是： tensor(93.8645, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 28.21917808219178\n",
      "loss是： tensor(100.5235, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 30.535714285714285\n",
      "loss是： tensor(125.7983, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 33.392857142857146\n",
      "loss是： tensor(96.1363, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 33.18840579710145\n",
      "loss是： tensor(104.6356, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 24.92063492063492\n",
      "loss是： tensor(116.1497, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 37.54385964912281\n",
      "loss是： tensor(81.7869, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 26.545454545454547\n",
      "loss是： tensor(88.7972, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 26.52173913043478\n",
      "loss是： tensor(80.3279, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 35.39682539682539\n",
      "loss是： tensor(86.8907, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 50.294117647058826\n",
      "loss是： tensor(112.6226, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 28.225806451612904\n",
      "loss是： tensor(103.6805, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 38.666666666666664\n",
      "loss是： tensor(76.7461, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 28.90909090909091\n",
      "loss是： tensor(82.7242, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 35.0\n",
      "loss是： tensor(84.7004, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 31.5625\n",
      "loss是： tensor(83.8677, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 32.08955223880597\n",
      "loss是： tensor(98.6717, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 45.0\n",
      "loss是： tensor(122.6518, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 31.698113207547173\n",
      "loss是： tensor(74.6123, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 30.74074074074074\n",
      "loss是： tensor(91.0504, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 27.432432432432435\n",
      "loss是： tensor(81.6594, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 36.30769230769231\n",
      "loss是： tensor(107.2020, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 26.451612903225804\n",
      "loss是： tensor(92.0251, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 30.0\n",
      "loss是： tensor(94.2293, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 34.12698412698413\n",
      "loss是： tensor(111.7505, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 27.741935483870968\n",
      "loss是： tensor(76.4924, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 56.32352941176471\n",
      "loss是： tensor(121.3073, device='cuda:0', grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert计算的loss是： 34.09090909090909\n",
      "loss是： tensor(86.7223, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 24.285714285714285\n",
      "loss是： tensor(94.6894, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 50.149253731343286\n",
      "loss是： tensor(141.7996, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 16.08108108108108\n",
      "loss是： tensor(83.2196, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 32.74193548387097\n",
      "loss是： tensor(88.2793, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 9.84126984126984\n",
      "loss是： tensor(77.7008, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 32.35294117647059\n",
      "loss是： tensor(112.0238, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 8.571428571428571\n",
      "loss是： tensor(81.3927, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 27.5\n",
      "loss是： tensor(63.0509, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 60.0\n",
      "loss是： tensor(104.6115, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 1.206896551724138\n",
      "loss是： tensor(67.7775, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 38.333333333333336\n",
      "loss是： tensor(81.5078, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 35.230769230769226\n",
      "loss是： tensor(138.0158, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 34.46153846153847\n",
      "loss是： tensor(114.8541, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 50.13698630136986\n",
      "loss是： tensor(111.8666, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 23.73134328358209\n",
      "loss是： tensor(83.9520, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 31.506849315068493\n",
      "loss是： tensor(106.7098, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 31.549295774647884\n",
      "loss是： tensor(97.9272, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 39.6875\n",
      "loss是： tensor(103.2525, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 36.3768115942029\n",
      "loss是： tensor(90.7646, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 23.47826086956522\n",
      "loss是： tensor(82.8844, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 26.285714285714285\n",
      "loss是： tensor(101.9579, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 28.518518518518515\n",
      "loss是： tensor(73.2162, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 32.264150943396224\n",
      "loss是： tensor(82.0399, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 32.96875\n",
      "loss是： tensor(101.2705, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 31.698113207547173\n",
      "loss是： tensor(71.3557, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 29.130434782608695\n",
      "loss是： tensor(109.8119, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 8.75\n",
      "loss是： tensor(61.1721, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 45.0\n",
      "loss是： tensor(151.2099, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 25.849056603773587\n",
      "loss是： tensor(88.3743, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 29.21875\n",
      "loss是： tensor(79.1365, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 53.61111111111111\n",
      "loss是： tensor(100.7433, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 28.03921568627451\n",
      "loss是： tensor(82.7569, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 23.103448275862068\n",
      "loss是： tensor(79.6678, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 33.22033898305085\n",
      "loss是： tensor(90.2289, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 38.69565217391304\n",
      "loss是： tensor(101.0621, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 34.06779661016949\n",
      "loss是： tensor(96.5887, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 50.84507042253521\n",
      "loss是： tensor(98.3144, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 30.869565217391305\n",
      "loss是： tensor(75.1741, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 40.588235294117645\n",
      "loss是： tensor(117.3678, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 40.98039215686274\n",
      "loss是： tensor(113.6121, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 28.10810810810811\n",
      "loss是： tensor(94.3105, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 36.15384615384615\n",
      "loss是： tensor(83.1091, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 41.59420289855072\n",
      "loss是： tensor(102.7004, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 28.03921568627451\n",
      "loss是： tensor(83.9622, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 10.526315789473683\n",
      "loss是： tensor(114.3091, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 49.64912280701754\n",
      "loss是： tensor(97.0955, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 29.333333333333332\n",
      "loss是： tensor(78.7856, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 37.01754385964912\n",
      "loss是： tensor(88.7760, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 31.142857142857142\n",
      "loss是： tensor(98.2768, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 34.78260869565217\n",
      "loss是： tensor(113.0629, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 37.5\n",
      "loss是： tensor(92.6186, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 26.19047619047619\n",
      "loss是： tensor(90.6916, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 28.75\n",
      "loss是： tensor(101.3669, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 26.956521739130434\n",
      "loss是： tensor(53.9594, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 30.0\n",
      "loss是： tensor(101.9118, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 43.63636363636363\n",
      "loss是： tensor(95.7544, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 30.98360655737705\n",
      "loss是： tensor(95.1496, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 39.25373134328358\n",
      "loss是： tensor(89.5687, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 34.15384615384615\n",
      "loss是： tensor(106.7408, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 32.0\n",
      "loss是： tensor(82.0859, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 35.0\n",
      "loss是： tensor(120.0767, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 35.6140350877193\n",
      "loss是： tensor(82.4951, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 33.54838709677419\n",
      "loss是： tensor(111.1255, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 24.262295081967213\n",
      "loss是： tensor(74.2659, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 28.571428571428573\n",
      "loss是： tensor(83.2456, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 31.833333333333332\n",
      "loss是： tensor(103.6456, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 34.59016393442623\n",
      "loss是： tensor(73.6735, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 28.955223880597014\n",
      "loss是： tensor(75.1908, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 42.32142857142857\n",
      "loss是： tensor(84.6732, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 31.92982456140351\n",
      "loss是： tensor(90.0641, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 36.21621621621622\n",
      "loss是： tensor(112.0968, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 31.80327868852459\n",
      "loss是： tensor(97.5139, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 36.45161290322581\n",
      "loss是： tensor(111.4055, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 27.936507936507937\n",
      "loss是： tensor(81.7659, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 35.294117647058826\n",
      "loss是： tensor(80.1123, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 34.90566037735849\n",
      "loss是： tensor(93.9121, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 32.083333333333336\n",
      "loss是： tensor(66.0585, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 35.079365079365076\n",
      "loss是： tensor(112.2445, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 40.0\n",
      "loss是： tensor(123.8013, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 13.859649122807019\n",
      "loss是： tensor(71.7157, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 62.075471698113205\n",
      "loss是： tensor(129.2455, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 28.166666666666668\n",
      "loss是： tensor(68.4219, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 25.0\n",
      "loss是： tensor(83.6154, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 31.724137931034484\n",
      "loss是： tensor(80.7417, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 18.309859154929576\n",
      "loss是： tensor(61.9621, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 36.111111111111114\n",
      "loss是： tensor(102.9004, device='cuda:0', grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert计算的loss是： 30.151515151515152\n",
      "loss是： tensor(91.5860, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 32.03125\n",
      "loss是： tensor(89.4404, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 34.13793103448276\n",
      "loss是： tensor(69.3738, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 26.60377358490566\n",
      "loss是： tensor(92.2126, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 32.54237288135593\n",
      "loss是： tensor(93.4682, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 48.10344827586207\n",
      "loss是： tensor(128.8796, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 26.911764705882355\n",
      "loss是： tensor(87.1253, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 25.694444444444446\n",
      "loss是： tensor(83.7963, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 25.833333333333336\n",
      "loss是： tensor(81.9966, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 32.15384615384615\n",
      "loss是： tensor(100.5072, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 34.193548387096776\n",
      "loss是： tensor(85.5649, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 33.88059701492537\n",
      "loss是： tensor(93.1229, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 46.19047619047619\n",
      "loss是： tensor(123.2067, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 28.64864864864865\n",
      "loss是： tensor(72.4372, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 30.163934426229506\n",
      "loss是： tensor(113.1113, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 40.1639344262295\n",
      "loss是： tensor(96.4608, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 28.305084745762713\n",
      "loss是： tensor(57.5536, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 26.393442622950822\n",
      "loss是： tensor(85.2465, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 31.75438596491228\n",
      "loss是： tensor(70.6463, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 32.35294117647059\n",
      "loss是： tensor(116.3192, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 23.28125\n",
      "loss是： tensor(82.2860, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 45.07042253521127\n",
      "loss是： tensor(110.6333, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 30.41095890410959\n",
      "loss是： tensor(106.2690, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 32.94117647058823\n",
      "loss是： tensor(102.6425, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 29.166666666666664\n",
      "loss是： tensor(73.3026, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 40.61538461538461\n",
      "loss是： tensor(117.9018, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 30.0\n",
      "loss是： tensor(81.7381, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 39.84615384615385\n",
      "loss是： tensor(106.4833, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 31.052631578947366\n",
      "loss是： tensor(86.4608, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 32.264150943396224\n",
      "loss是： tensor(86.4475, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 41.42857142857143\n",
      "loss是： tensor(114.0743, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 34.375\n",
      "loss是： tensor(109.3127, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 54.08450704225352\n",
      "loss是： tensor(117.9067, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 28.03030303030303\n",
      "loss是： tensor(109.8315, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 38.70967741935484\n",
      "loss是： tensor(94.2056, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 34.30769230769231\n",
      "loss是： tensor(92.8743, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 28.71794871794872\n",
      "loss是： tensor(58.2617, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 51.32352941176471\n",
      "loss是： tensor(100.5992, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 32.74193548387097\n",
      "loss是： tensor(100.2942, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 30.526315789473685\n",
      "loss是： tensor(74.2363, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 29.848484848484848\n",
      "loss是： tensor(86.8165, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 43.75\n",
      "loss是： tensor(120.0830, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 34.03508771929825\n",
      "loss是： tensor(90.0060, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 25.694444444444446\n",
      "loss是： tensor(76.7334, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 43.53846153846153\n",
      "loss是： tensor(99.6049, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 24.21875\n",
      "loss是： tensor(78.4592, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 40.96774193548387\n",
      "loss是： tensor(100.4794, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 33.333333333333336\n",
      "loss是： tensor(79.6850, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 25.25423728813559\n",
      "loss是： tensor(76.7160, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 24.385964912280702\n",
      "loss是： tensor(75.8475, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 40.3125\n",
      "loss是： tensor(115.4723, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 37.58064516129033\n",
      "loss是： tensor(98.5945, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 30.158730158730158\n",
      "loss是： tensor(92.6942, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 38.627450980392155\n",
      "loss是： tensor(99.3808, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 45.84615384615384\n",
      "loss是： tensor(114.6655, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 25.454545454545453\n",
      "loss是： tensor(69.1218, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 25.084745762711865\n",
      "loss是： tensor(84.9916, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 23.59375\n",
      "loss是： tensor(75.4593, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 39.2063492063492\n",
      "loss是： tensor(121.7248, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 44.21052631578948\n",
      "loss是： tensor(85.7849, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "260/300\n",
      "bert计算的loss是： 32.74193548387097\n",
      "loss是： tensor(95.8559, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 26.923076923076927\n",
      "loss是： tensor(92.8027, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 27.647058823529413\n",
      "loss是： tensor(86.4442, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 36.57142857142857\n",
      "loss是： tensor(106.1314, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 28.142857142857146\n",
      "loss是： tensor(113.6816, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 35.714285714285715\n",
      "loss是： tensor(79.9560, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 27.230769230769234\n",
      "loss是： tensor(77.6783, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 41.30434782608695\n",
      "loss是： tensor(120.2588, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 28.88888888888889\n",
      "loss是： tensor(87.8577, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 52.857142857142854\n",
      "loss是： tensor(124.6541, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 29.82758620689655\n",
      "loss是： tensor(88.9195, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 47.54716981132076\n",
      "loss是： tensor(97.8077, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 37.53846153846154\n",
      "loss是： tensor(130.9151, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 34.02985074626866\n",
      "loss是： tensor(105.1068, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 29.322033898305087\n",
      "loss是： tensor(80.2787, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 2.7586206896551726\n",
      "loss是： tensor(49.1326, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 38.857142857142854\n",
      "loss是： tensor(102.7080, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 31.481481481481485\n",
      "loss是： tensor(92.5992, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 56.95652173913044\n",
      "loss是： tensor(136.0068, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 30.78125\n",
      "loss是： tensor(71.7362, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 16.379310344827587\n",
      "loss是： tensor(55.1233, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 27.586206896551726\n",
      "loss是： tensor(59.0857, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 38.611111111111114\n",
      "loss是： tensor(72.8600, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 26.610169491525426\n",
      "loss是： tensor(81.4777, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 27.424242424242422\n",
      "loss是： tensor(89.2121, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 27.611940298507463\n",
      "loss是： tensor(87.1442, device='cuda:0', grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert计算的loss是： 22.033898305084744\n",
      "loss是： tensor(82.0666, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 40.30769230769231\n",
      "loss是： tensor(96.3115, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 24.603174603174605\n",
      "loss是： tensor(71.9934, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 33.559322033898304\n",
      "loss是： tensor(75.9956, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 49.242424242424235\n",
      "loss是： tensor(106.9796, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 34.074074074074076\n",
      "loss是： tensor(100.5441, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 29.84375\n",
      "loss是： tensor(91.2933, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 41.58730158730159\n",
      "loss是： tensor(109.1841, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 36.885245901639344\n",
      "loss是： tensor(93.1835, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 29.846153846153847\n",
      "loss是： tensor(97.8514, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 23.77049180327869\n",
      "loss是： tensor(88.6421, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 37.41379310344827\n",
      "loss是： tensor(115.8262, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 29.61038961038961\n",
      "loss是： tensor(95.1373, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 37.11538461538461\n",
      "loss是： tensor(101.3186, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 39.83050847457627\n",
      "loss是： tensor(105.9380, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 31.73913043478261\n",
      "loss是： tensor(105.4909, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 37.532467532467535\n",
      "loss是： tensor(128.2641, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 33.39622641509434\n",
      "loss是： tensor(99.2925, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 23.225806451612904\n",
      "loss是： tensor(70.7678, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 28.88888888888889\n",
      "loss是： tensor(62.0572, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 31.967213114754102\n",
      "loss是： tensor(90.5676, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 34.42622950819672\n",
      "loss是： tensor(100.4750, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 35.88235294117647\n",
      "loss是： tensor(104.8970, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 6.515151515151515\n",
      "loss是： tensor(60.6144, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 33.150684931506845\n",
      "loss是： tensor(97.4365, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 32.57575757575758\n",
      "loss是： tensor(84.4861, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 4.776119402985074\n",
      "loss是： tensor(63.9759, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 23.0\n",
      "loss是： tensor(81.1303, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 37.96296296296296\n",
      "loss是： tensor(89.2956, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 33.728813559322035\n",
      "loss是： tensor(92.7167, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 47.69230769230769\n",
      "loss是： tensor(115.7773, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 7.3584905660377355\n",
      "loss是： tensor(63.1378, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 24.285714285714285\n",
      "loss是： tensor(81.0177, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 35.24590163934426\n",
      "loss是： tensor(120.2563, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 30.15625\n",
      "loss是： tensor(85.1543, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 33.75\n",
      "loss是： tensor(90.3439, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 55.23809523809524\n",
      "loss是： tensor(155.0843, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 35.59322033898305\n",
      "loss是： tensor(98.5063, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 35.78947368421053\n",
      "loss是： tensor(83.1856, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 28.571428571428573\n",
      "loss是： tensor(85.4416, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 28.656716417910445\n",
      "loss是： tensor(83.4140, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 26.984126984126988\n",
      "loss是： tensor(103.4922, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 38.205128205128204\n",
      "loss是： tensor(83.3685, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 2.2950819672131146\n",
      "loss是： tensor(42.8438, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 25.932203389830512\n",
      "loss是： tensor(78.1252, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 37.61904761904762\n",
      "loss是： tensor(98.2900, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 28.70967741935484\n",
      "loss是： tensor(72.3086, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 29.682539682539684\n",
      "loss是： tensor(106.7949, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 33.59375\n",
      "loss是： tensor(72.3017, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 20.35087719298246\n",
      "loss是： tensor(66.2041, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 26.333333333333332\n",
      "loss是： tensor(76.0063, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 27.142857142857146\n",
      "loss是： tensor(66.2509, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 33.78787878787879\n",
      "loss是： tensor(99.2866, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 71.27272727272727\n",
      "loss是： tensor(125.1930, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 33.4375\n",
      "loss是： tensor(98.2922, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 30.317460317460316\n",
      "loss是： tensor(72.3344, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 50.18181818181819\n",
      "loss是： tensor(92.4561, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 22.5\n",
      "loss是： tensor(79.5416, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 33.0\n",
      "loss是： tensor(73.7169, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 40.46153846153846\n",
      "loss是： tensor(106.5505, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 39.29577464788733\n",
      "loss是： tensor(128.8058, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 36.231884057971016\n",
      "loss是： tensor(106.2520, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 38.630136986301366\n",
      "loss是： tensor(107.6882, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 33.0\n",
      "loss是： tensor(83.1714, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 22.586206896551726\n",
      "loss是： tensor(76.7928, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 25.853658536585368\n",
      "loss是： tensor(78.4474, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 28.674698795180724\n",
      "loss是： tensor(93.7629, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 37.23076923076923\n",
      "loss是： tensor(112.1067, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 2.5\n",
      "loss是： tensor(53.6329, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 31.076923076923077\n",
      "loss是： tensor(101.7642, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 29.848484848484848\n",
      "loss是： tensor(75.5709, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 31.343283582089555\n",
      "loss是： tensor(87.2780, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 17.586206896551722\n",
      "loss是： tensor(64.9370, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 26.610169491525426\n",
      "loss是： tensor(73.2588, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 9.571428571428571\n",
      "loss是： tensor(71.9869, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 29.642857142857146\n",
      "loss是： tensor(81.6757, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 44.7457627118644\n",
      "loss是： tensor(109.4332, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 57.377049180327866\n",
      "loss是： tensor(138.7495, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 19.29824561403509\n",
      "loss是： tensor(62.3299, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 29.642857142857146\n",
      "loss是： tensor(108.7874, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 64.42622950819671\n",
      "loss是： tensor(126.0370, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 32.666666666666664\n",
      "loss是： tensor(79.2698, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 30.943396226415096\n",
      "loss是： tensor(72.1084, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 34.03225806451613\n",
      "loss是： tensor(84.8602, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 26.666666666666664\n",
      "loss是： tensor(76.3079, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 28.448275862068964\n",
      "loss是： tensor(63.1676, device='cuda:0', grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert计算的loss是： 30.46875\n",
      "loss是： tensor(106.2792, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 29.43661971830986\n",
      "loss是： tensor(104.2549, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 27.058823529411768\n",
      "loss是： tensor(50.2849, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 40.42857142857143\n",
      "loss是： tensor(112.4799, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 33.22033898305085\n",
      "loss是： tensor(99.8623, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 35.52238805970149\n",
      "loss是： tensor(73.8711, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 35.33333333333333\n",
      "loss是： tensor(95.8306, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 27.321428571428573\n",
      "loss是： tensor(66.3244, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 27.424242424242422\n",
      "loss是： tensor(83.2504, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 32.57142857142857\n",
      "loss是： tensor(82.4564, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 33.42465753424658\n",
      "loss是： tensor(101.8340, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 37.37704918032787\n",
      "loss是： tensor(93.9315, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 1.4516129032258065\n",
      "loss是： tensor(62.1907, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 26.29032258064516\n",
      "loss是： tensor(61.0711, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 39.50819672131148\n",
      "loss是： tensor(100.5616, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 37.719298245614034\n",
      "loss是： tensor(94.3004, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 38.94736842105263\n",
      "loss是： tensor(96.5666, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 24.142857142857142\n",
      "loss是： tensor(91.9399, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 24.40677966101695\n",
      "loss是： tensor(60.0345, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 34.73684210526316\n",
      "loss是： tensor(108.7443, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 53.49206349206349\n",
      "loss是： tensor(117.9598, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 31.186440677966104\n",
      "loss是： tensor(80.4643, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 10.87719298245614\n",
      "loss是： tensor(62.4294, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 26.956521739130434\n",
      "loss是： tensor(77.7155, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 26.96969696969697\n",
      "loss是： tensor(79.9060, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 48.78787878787879\n",
      "loss是： tensor(73.5796, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 29.423076923076927\n",
      "loss是： tensor(67.6221, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 31.21212121212121\n",
      "loss是： tensor(90.6467, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 35.08474576271186\n",
      "loss是： tensor(83.3608, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 37.0\n",
      "loss是： tensor(100.2991, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 33.768115942028984\n",
      "loss是： tensor(104.4799, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 36.12903225806451\n",
      "loss是： tensor(95.5611, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 33.492063492063494\n",
      "loss是： tensor(94.3298, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 33.23076923076923\n",
      "loss是： tensor(90.0864, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 36.61290322580645\n",
      "loss是： tensor(121.7957, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 44.47761194029851\n",
      "loss是： tensor(104.8267, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 41.47058823529412\n",
      "loss是： tensor(109.4662, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 55.3448275862069\n",
      "loss是： tensor(112.0424, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 9.393939393939394\n",
      "loss是： tensor(58.9484, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 25.454545454545453\n",
      "loss是： tensor(91.0570, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 24.915254237288135\n",
      "loss是： tensor(79.9904, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 45.32258064516129\n",
      "loss是： tensor(135.0431, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 27.413793103448274\n",
      "loss是： tensor(76.6901, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 29.1044776119403\n",
      "loss是： tensor(94.7480, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 41.940298507462686\n",
      "loss是： tensor(89.0639, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 30.317460317460316\n",
      "loss是： tensor(92.1314, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 36.62162162162162\n",
      "loss是： tensor(89.6745, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 26.333333333333332\n",
      "loss是： tensor(56.6695, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 22.424242424242422\n",
      "loss是： tensor(45.0630, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 27.6271186440678\n",
      "loss是： tensor(72.9183, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 27.419354838709676\n",
      "loss是： tensor(80.0676, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 48.10344827586207\n",
      "loss是： tensor(133.5609, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 34.285714285714285\n",
      "loss是： tensor(94.1828, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 63.77049180327869\n",
      "loss是： tensor(135.3075, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 28.615384615384617\n",
      "loss是： tensor(89.8558, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 37.41379310344827\n",
      "loss是： tensor(92.6330, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 39.84615384615385\n",
      "loss是： tensor(134.4017, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 40.42253521126761\n",
      "loss是： tensor(97.4745, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 36.37931034482759\n",
      "loss是： tensor(81.8421, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 22.884615384615383\n",
      "loss是： tensor(76.4517, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 31.818181818181817\n",
      "loss是： tensor(113.4096, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 32.1875\n",
      "loss是： tensor(102.0259, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 25.614035087719298\n",
      "loss是： tensor(54.5289, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 31.132075471698112\n",
      "loss是： tensor(83.1378, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 32.20779220779221\n",
      "loss是： tensor(105.1090, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 29.42857142857143\n",
      "loss是： tensor(91.7933, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 33.134328358208954\n",
      "loss是： tensor(108.9002, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 36.82539682539682\n",
      "loss是： tensor(114.3137, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 25.9375\n",
      "loss是： tensor(75.9285, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 44.716981132075475\n",
      "loss是： tensor(105.4914, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 29.73333333333333\n",
      "loss是： tensor(74.5087, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 24.375\n",
      "loss是： tensor(43.4545, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 29.464285714285715\n",
      "loss是： tensor(83.2324, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 60.27777777777778\n",
      "loss是： tensor(153.6696, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 31.076923076923077\n",
      "loss是： tensor(77.0719, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 33.142857142857146\n",
      "loss是： tensor(88.0635, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 31.428571428571427\n",
      "loss是： tensor(80.2660, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 6.470588235294118\n",
      "loss是： tensor(73.1486, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 36.76923076923077\n",
      "loss是： tensor(100.8253, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 7.962962962962963\n",
      "loss是： tensor(68.8256, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 36.5625\n",
      "loss是： tensor(97.0429, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 28.24561403508772\n",
      "loss是： tensor(68.2036, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 34.642857142857146\n",
      "loss是： tensor(80.8836, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 20.76923076923077\n",
      "loss是： tensor(54.6692, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 53.38461538461538\n",
      "loss是： tensor(141.7916, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 40.15384615384615\n",
      "loss是： tensor(123.6249, device='cuda:0', grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert计算的loss是： 30.806451612903224\n",
      "loss是： tensor(84.5803, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 29.82456140350877\n",
      "loss是： tensor(93.3149, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 33.026315789473685\n",
      "loss是： tensor(109.9291, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 25.223880597014926\n",
      "loss是： tensor(91.9370, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 31.267605633802816\n",
      "loss是： tensor(93.5051, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 35.0\n",
      "loss是： tensor(87.9599, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 30.357142857142854\n",
      "loss是： tensor(72.3567, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 4.507042253521127\n",
      "loss是： tensor(64.5919, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 40.76923076923077\n",
      "loss是： tensor(68.9709, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 52.42424242424242\n",
      "loss是： tensor(118.4821, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 51.864406779661024\n",
      "loss是： tensor(112.4314, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 29.23076923076923\n",
      "loss是： tensor(78.1586, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 21.470588235294116\n",
      "loss是： tensor(64.1837, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 36.666666666666664\n",
      "loss是： tensor(88.6838, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 27.0\n",
      "loss是： tensor(87.6447, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 35.73529411764706\n",
      "loss是： tensor(109.0784, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 44.920634920634924\n",
      "loss是： tensor(120.1004, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 23.174603174603178\n",
      "loss是： tensor(61.3078, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 33.10344827586207\n",
      "loss是： tensor(96.0904, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 36.0655737704918\n",
      "loss是： tensor(77.5447, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 25.901639344262296\n",
      "loss是： tensor(76.7966, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 33.18181818181819\n",
      "loss是： tensor(110.1542, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 27.540983606557376\n",
      "loss是： tensor(100.6706, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 25.901639344262296\n",
      "loss是： tensor(87.9817, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 35.49295774647887\n",
      "loss是： tensor(83.7478, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 31.016949152542374\n",
      "loss是： tensor(99.3157, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 29.333333333333332\n",
      "loss是： tensor(83.4032, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 30.615384615384613\n",
      "loss是： tensor(91.1539, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 32.608695652173914\n",
      "loss是： tensor(93.1355, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 30.588235294117645\n",
      "loss是： tensor(87.1145, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 41.84615384615385\n",
      "loss是： tensor(103.3597, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 45.0\n",
      "loss是： tensor(76.3319, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "270/300\n",
      "bert计算的loss是： 28.03030303030303\n",
      "loss是： tensor(90.4659, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 30.3125\n",
      "loss是： tensor(85.8842, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 38.909090909090914\n",
      "loss是： tensor(86.6741, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 24.76923076923077\n",
      "loss是： tensor(68.0842, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 60.78125\n",
      "loss是： tensor(116.8086, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 25.5\n",
      "loss是： tensor(85.9118, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 33.582089552238806\n",
      "loss是： tensor(92.1689, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 37.42424242424242\n",
      "loss是： tensor(96.9233, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 34.56140350877193\n",
      "loss是： tensor(96.2911, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 30.0\n",
      "loss是： tensor(74.2973, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 25.78125\n",
      "loss是： tensor(106.9275, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 36.29032258064516\n",
      "loss是： tensor(89.3506, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 35.625\n",
      "loss是： tensor(112.0138, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 37.09090909090909\n",
      "loss是： tensor(88.2643, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 34.03225806451613\n",
      "loss是： tensor(95.1488, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 50.32786885245901\n",
      "loss是： tensor(121.6291, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 34.21875\n",
      "loss是： tensor(94.2759, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 23.78787878787879\n",
      "loss是： tensor(95.5051, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 33.484848484848484\n",
      "loss是： tensor(86.5347, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 29.5\n",
      "loss是： tensor(125.4666, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 40.943396226415096\n",
      "loss是： tensor(102.4630, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 30.422535211267604\n",
      "loss是： tensor(85.7296, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 30.930232558139537\n",
      "loss是： tensor(66.7126, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 33.59375\n",
      "loss是： tensor(97.8583, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 38.59375\n",
      "loss是： tensor(92.2807, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 31.791044776119403\n",
      "loss是： tensor(77.7491, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 37.87878787878788\n",
      "loss是： tensor(97.0907, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 30.454545454545453\n",
      "loss是： tensor(71.8948, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 49.48275862068965\n",
      "loss是： tensor(96.5051, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 28.88888888888889\n",
      "loss是： tensor(77.0750, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 24.26470588235294\n",
      "loss是： tensor(71.8126, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 16.086956521739133\n",
      "loss是： tensor(71.3959, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 25.5\n",
      "loss是： tensor(92.6962, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 56.11940298507463\n",
      "loss是： tensor(107.0923, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 34.516129032258064\n",
      "loss是： tensor(79.0693, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 25.074626865671643\n",
      "loss是： tensor(96.7207, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 31.052631578947366\n",
      "loss是： tensor(77.9208, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 9.491525423728813\n",
      "loss是： tensor(54.6942, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 30.3125\n",
      "loss是： tensor(78.8418, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 25.0\n",
      "loss是： tensor(98.2471, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 41.166666666666664\n",
      "loss是： tensor(117.3477, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 33.63636363636364\n",
      "loss是： tensor(80.9319, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 32.46376811594203\n",
      "loss是： tensor(71.7260, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 36.36363636363636\n",
      "loss是： tensor(87.3481, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 32.599999999999994\n",
      "loss是： tensor(74.2762, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 39.743589743589745\n",
      "loss是： tensor(78.5680, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 2.833333333333333\n",
      "loss是： tensor(60.2584, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 31.475409836065573\n",
      "loss是： tensor(78.6223, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 45.13888888888889\n",
      "loss是： tensor(98.5979, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 35.45454545454545\n",
      "loss是： tensor(86.9973, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 42.0\n",
      "loss是： tensor(116.7823, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 35.9375\n",
      "loss是： tensor(94.1053, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 37.868852459016395\n",
      "loss是： tensor(100.7647, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 23.666666666666668\n",
      "loss是： tensor(71.4104, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 24.5\n",
      "loss是： tensor(81.8294, device='cuda:0', grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert计算的loss是： 21.403508771929825\n",
      "loss是： tensor(61.7226, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 27.384615384615387\n",
      "loss是： tensor(84.4057, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 38.5\n",
      "loss是： tensor(116.1334, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 29.682539682539684\n",
      "loss是： tensor(70.6992, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 25.538461538461537\n",
      "loss是： tensor(88.6279, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 36.557377049180324\n",
      "loss是： tensor(108.4889, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 35.78947368421053\n",
      "loss是： tensor(100.0645, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 33.4375\n",
      "loss是： tensor(77.7630, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 33.73134328358209\n",
      "loss是： tensor(90.3828, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 38.208955223880594\n",
      "loss是： tensor(109.3070, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 42.74193548387097\n",
      "loss是： tensor(112.8017, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 50.303030303030305\n",
      "loss是： tensor(95.3157, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 31.38888888888889\n",
      "loss是： tensor(83.5999, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 31.538461538461537\n",
      "loss是： tensor(62.0450, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 33.38461538461539\n",
      "loss是： tensor(91.1207, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 34.0983606557377\n",
      "loss是： tensor(86.4726, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 25.535714285714285\n",
      "loss是： tensor(55.7686, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 22.0\n",
      "loss是： tensor(59.0693, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 38.54838709677419\n",
      "loss是： tensor(94.4475, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 27.91044776119403\n",
      "loss是： tensor(76.9965, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 30.158730158730158\n",
      "loss是： tensor(96.2374, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 34.225352112676056\n",
      "loss是： tensor(107.7358, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 29.076923076923077\n",
      "loss是： tensor(85.5904, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 29.71014492753623\n",
      "loss是： tensor(98.4174, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 40.61538461538461\n",
      "loss是： tensor(109.6909, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 35.42372881355932\n",
      "loss是： tensor(102.1520, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 32.95774647887324\n",
      "loss是： tensor(116.6949, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 37.01492537313433\n",
      "loss是： tensor(98.8352, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 38.833333333333336\n",
      "loss是： tensor(101.6470, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 31.666666666666664\n",
      "loss是： tensor(86.7445, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 11.029411764705884\n",
      "loss是： tensor(68.6140, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 31.35593220338983\n",
      "loss是： tensor(97.2814, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 50.78125\n",
      "loss是： tensor(86.4297, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 42.64150943396227\n",
      "loss是： tensor(107.0686, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 35.51724137931035\n",
      "loss是： tensor(71.7714, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 36.89655172413793\n",
      "loss是： tensor(84.7041, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 29.18918918918919\n",
      "loss是： tensor(74.6003, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 32.74193548387097\n",
      "loss是： tensor(90.6498, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 34.02985074626866\n",
      "loss是： tensor(86.0435, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 36.77966101694915\n",
      "loss是： tensor(86.8953, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 36.76056338028169\n",
      "loss是： tensor(80.9297, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 32.6865671641791\n",
      "loss是： tensor(89.4156, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 33.16666666666667\n",
      "loss是： tensor(79.0180, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 29.53846153846154\n",
      "loss是： tensor(77.1595, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 53.52941176470588\n",
      "loss是： tensor(120.7038, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 47.19298245614035\n",
      "loss是： tensor(118.3733, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 38.25396825396825\n",
      "loss是： tensor(94.8034, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 35.21739130434783\n",
      "loss是： tensor(107.1171, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 36.93877551020408\n",
      "loss是： tensor(83.7287, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 29.090909090909093\n",
      "loss是： tensor(74.1518, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 29.137931034482758\n",
      "loss是： tensor(71.3717, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 29.682539682539684\n",
      "loss是： tensor(73.1412, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 40.0\n",
      "loss是： tensor(111.7354, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 37.96875\n",
      "loss是： tensor(88.8321, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 36.0\n",
      "loss是： tensor(75.7157, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 26.93877551020408\n",
      "loss是： tensor(72.3800, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 25.57377049180328\n",
      "loss是： tensor(68.7877, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 24.137931034482758\n",
      "loss是： tensor(78.9443, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 30.333333333333332\n",
      "loss是： tensor(85.4752, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 20.0\n",
      "loss是： tensor(48.0130, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 24.262295081967213\n",
      "loss是： tensor(80.9350, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 36.486486486486484\n",
      "loss是： tensor(87.3951, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 34.36363636363637\n",
      "loss是： tensor(99.0035, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 26.779661016949152\n",
      "loss是： tensor(71.0556, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 30.441176470588232\n",
      "loss是： tensor(90.1824, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 39.84375\n",
      "loss是： tensor(114.5659, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 21.26984126984127\n",
      "loss是： tensor(111.0168, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 37.27272727272727\n",
      "loss是： tensor(96.9369, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 1.4545454545454546\n",
      "loss是： tensor(80.4749, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 38.46153846153846\n",
      "loss是： tensor(89.5627, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 35.53846153846153\n",
      "loss是： tensor(87.0518, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 35.0\n",
      "loss是： tensor(93.7597, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 22.1875\n",
      "loss是： tensor(81.8701, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 62.11538461538461\n",
      "loss是： tensor(109.2301, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 42.58620689655173\n",
      "loss是： tensor(104.1359, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 25.789473684210527\n",
      "loss是： tensor(79.4573, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 29.84375\n",
      "loss是： tensor(82.2017, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 25.78125\n",
      "loss是： tensor(89.2670, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 33.80952380952381\n",
      "loss是： tensor(98.2902, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 47.93650793650794\n",
      "loss是： tensor(105.9439, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 32.08955223880597\n",
      "loss是： tensor(73.8703, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 32.96875\n",
      "loss是： tensor(85.6310, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 26.904761904761905\n",
      "loss是： tensor(59.2579, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 26.9811320754717\n",
      "loss是： tensor(69.4500, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 24.727272727272727\n",
      "loss是： tensor(69.6147, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 30.5\n",
      "loss是： tensor(78.3014, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 20.39473684210526\n",
      "loss是： tensor(113.5654, device='cuda:0', grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert计算的loss是： 26.71428571428571\n",
      "loss是： tensor(90.3034, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 29.866666666666667\n",
      "loss是： tensor(90.8315, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 33.6764705882353\n",
      "loss是： tensor(89.7859, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 27.142857142857146\n",
      "loss是： tensor(71.5612, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 33.75\n",
      "loss是： tensor(83.8983, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 29.6875\n",
      "loss是： tensor(77.0919, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 29.22077922077922\n",
      "loss是： tensor(98.6742, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 32.90909090909091\n",
      "loss是： tensor(82.8145, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 40.44776119402985\n",
      "loss是： tensor(103.3342, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 55.0\n",
      "loss是： tensor(118.8102, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 41.81818181818181\n",
      "loss是： tensor(103.8786, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 48.392857142857146\n",
      "loss是： tensor(112.2482, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 39.83870967741935\n",
      "loss是： tensor(106.7034, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 23.650793650793652\n",
      "loss是： tensor(69.5821, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 32.083333333333336\n",
      "loss是： tensor(92.1093, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 32.857142857142854\n",
      "loss是： tensor(110.6853, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 22.10526315789474\n",
      "loss是： tensor(66.5130, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 28.2\n",
      "loss是： tensor(81.5046, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 43.40909090909091\n",
      "loss是： tensor(96.4054, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 31.884057971014492\n",
      "loss是： tensor(78.0034, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 32.35294117647059\n",
      "loss是： tensor(76.5163, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 28.4375\n",
      "loss是： tensor(100.2464, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 31.044776119402986\n",
      "loss是： tensor(104.1899, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 37.03125\n",
      "loss是： tensor(94.5200, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 40.0\n",
      "loss是： tensor(104.6446, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 29.571428571428573\n",
      "loss是： tensor(93.1880, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 32.5\n",
      "loss是： tensor(108.9292, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 27.6271186440678\n",
      "loss是： tensor(85.1306, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 28.653846153846153\n",
      "loss是： tensor(59.9906, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 32.64705882352941\n",
      "loss是： tensor(90.8729, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 70.1470588235294\n",
      "loss是： tensor(132.2705, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 34.833333333333336\n",
      "loss是： tensor(101.2535, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 34.44444444444444\n",
      "loss是： tensor(94.6604, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 22.98507462686567\n",
      "loss是： tensor(76.3750, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 9.782608695652174\n",
      "loss是： tensor(54.7928, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 43.63636363636363\n",
      "loss是： tensor(111.3533, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 23.846153846153847\n",
      "loss是： tensor(68.0363, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 42.45614035087719\n",
      "loss是： tensor(95.9919, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 31.896551724137932\n",
      "loss是： tensor(79.9118, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 25.64516129032258\n",
      "loss是： tensor(63.2157, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 35.08474576271186\n",
      "loss是： tensor(89.0566, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 5.675675675675675\n",
      "loss是： tensor(28.0546, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 30.714285714285715\n",
      "loss是： tensor(118.3021, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 35.625\n",
      "loss是： tensor(88.5970, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 29.0\n",
      "loss是： tensor(67.6466, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 34.83870967741935\n",
      "loss是： tensor(76.3781, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 7.321428571428571\n",
      "loss是： tensor(69.5165, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 34.696969696969695\n",
      "loss是： tensor(100.1055, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 50.50847457627118\n",
      "loss是： tensor(107.1742, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 28.571428571428573\n",
      "loss是： tensor(72.8031, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 23.81818181818182\n",
      "loss是： tensor(84.9912, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 40.15873015873016\n",
      "loss是： tensor(118.2068, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 30.338983050847457\n",
      "loss是： tensor(81.6460, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 34.0\n",
      "loss是： tensor(86.9034, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 31.594202898550723\n",
      "loss是： tensor(79.9413, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 28.055555555555554\n",
      "loss是： tensor(109.4710, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 31.594202898550723\n",
      "loss是： tensor(81.2732, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 27.460317460317462\n",
      "loss是： tensor(83.5420, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 39.84848484848485\n",
      "loss是： tensor(96.9052, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 24.126984126984127\n",
      "loss是： tensor(82.6405, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 30.78125\n",
      "loss是： tensor(96.7979, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 36.0377358490566\n",
      "loss是： tensor(104.3016, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 29.516129032258064\n",
      "loss是： tensor(79.8333, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 54.50704225352113\n",
      "loss是： tensor(125.8345, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 54.73684210526316\n",
      "loss是： tensor(78.8633, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 30.151515151515152\n",
      "loss是： tensor(72.1779, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 33.05084745762712\n",
      "loss是： tensor(68.9686, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 29.206349206349206\n",
      "loss是： tensor(83.8475, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 42.22222222222222\n",
      "loss是： tensor(108.7487, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 36.98412698412699\n",
      "loss是： tensor(97.7363, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 26.615384615384613\n",
      "loss是： tensor(66.1111, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 33.07692307692307\n",
      "loss是： tensor(99.2462, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 31.904761904761905\n",
      "loss是： tensor(108.4910, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 61.754385964912274\n",
      "loss是： tensor(109.9595, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 35.0\n",
      "loss是： tensor(102.2857, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 20.35087719298246\n",
      "loss是： tensor(74.7953, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 38.5\n",
      "loss是： tensor(83.1609, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 26.769230769230766\n",
      "loss是： tensor(87.7862, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 26.29032258064516\n",
      "loss是： tensor(55.6611, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 29.55223880597015\n",
      "loss是： tensor(102.1394, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 25.357142857142854\n",
      "loss是： tensor(83.9917, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 33.65079365079365\n",
      "loss是： tensor(110.5896, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 37.758620689655174\n",
      "loss是： tensor(102.3225, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 37.0\n",
      "loss是： tensor(90.5166, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 46.52173913043478\n",
      "loss是： tensor(109.2861, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 28.524590163934427\n",
      "loss是： tensor(87.7283, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 30.0\n",
      "loss是： tensor(73.4364, device='cuda:0', grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert计算的loss是： 46.52173913043478\n",
      "loss是： tensor(87.1068, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "280/300\n",
      "bert计算的loss是： 21.607142857142854\n",
      "loss是： tensor(71.0651, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 28.88888888888889\n",
      "loss是： tensor(66.3854, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 35.97222222222222\n",
      "loss是： tensor(100.7965, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 25.454545454545453\n",
      "loss是： tensor(86.8451, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 30.952380952380953\n",
      "loss是： tensor(96.6965, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 29.322033898305087\n",
      "loss是： tensor(85.5300, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 36.03174603174603\n",
      "loss是： tensor(96.2624, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 33.15789473684211\n",
      "loss是： tensor(96.0793, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 30.163934426229506\n",
      "loss是： tensor(88.7710, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 22.878787878787882\n",
      "loss是： tensor(78.2002, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 41.28571428571429\n",
      "loss是： tensor(109.4812, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 42.57575757575758\n",
      "loss是： tensor(103.3596, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 27.575757575757578\n",
      "loss是： tensor(71.2067, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 28.5\n",
      "loss是： tensor(88.1850, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 37.25806451612903\n",
      "loss是： tensor(92.7606, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 31.052631578947366\n",
      "loss是： tensor(70.5890, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 59.12280701754386\n",
      "loss是： tensor(95.5851, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 22.432432432432435\n",
      "loss是： tensor(72.8110, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 21.639344262295083\n",
      "loss是： tensor(83.3946, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 39.84615384615385\n",
      "loss是： tensor(86.7781, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 32.58064516129033\n",
      "loss是： tensor(91.3235, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 54.26470588235294\n",
      "loss是： tensor(121.8530, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 17.5609756097561\n",
      "loss是： tensor(43.9528, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 33.14814814814815\n",
      "loss是： tensor(75.9380, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 13.484848484848484\n",
      "loss是： tensor(61.6340, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 15.072463768115941\n",
      "loss是： tensor(59.4750, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 37.3134328358209\n",
      "loss是： tensor(90.5346, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 24.576271186440678\n",
      "loss是： tensor(75.4911, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 45.625\n",
      "loss是： tensor(99.4624, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 38.275862068965516\n",
      "loss是： tensor(102.1432, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 24.057971014492754\n",
      "loss是： tensor(88.9604, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 32.38805970149254\n",
      "loss是： tensor(96.1822, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 25.789473684210527\n",
      "loss是： tensor(77.7426, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 35.43859649122807\n",
      "loss是： tensor(111.8153, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 35.91549295774648\n",
      "loss是： tensor(83.7105, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 27.10144927536232\n",
      "loss是： tensor(99.6506, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 33.10344827586207\n",
      "loss是： tensor(77.2470, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 27.460317460317462\n",
      "loss是： tensor(86.8780, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 18.545454545454547\n",
      "loss是： tensor(73.2434, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 39.710144927536234\n",
      "loss是： tensor(107.0385, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 65.08474576271186\n",
      "loss是： tensor(128.5991, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 35.87301587301587\n",
      "loss是： tensor(92.5850, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 27.258064516129032\n",
      "loss是： tensor(112.8876, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 38.5\n",
      "loss是： tensor(96.8806, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 31.0\n",
      "loss是： tensor(92.6014, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 37.857142857142854\n",
      "loss是： tensor(77.3390, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 27.580645161290324\n",
      "loss是： tensor(94.8553, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 28.47457627118644\n",
      "loss是： tensor(67.9063, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 26.515151515151516\n",
      "loss是： tensor(74.5483, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 37.45762711864407\n",
      "loss是： tensor(82.6205, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 25.833333333333336\n",
      "loss是： tensor(74.2807, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 35.67567567567568\n",
      "loss是： tensor(97.7973, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 35.87301587301587\n",
      "loss是： tensor(95.8139, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 26.451612903225804\n",
      "loss是： tensor(75.9644, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 33.970588235294116\n",
      "loss是： tensor(98.0418, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 40.32786885245901\n",
      "loss是： tensor(96.8428, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 41.21212121212121\n",
      "loss是： tensor(111.2063, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 35.16129032258065\n",
      "loss是： tensor(84.6350, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 28.793103448275865\n",
      "loss是： tensor(88.0694, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 31.93548387096774\n",
      "loss是： tensor(115.7166, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 25.0\n",
      "loss是： tensor(108.1875, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 25.692307692307693\n",
      "loss是： tensor(79.0278, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 34.074074074074076\n",
      "loss是： tensor(66.2550, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 41.11111111111111\n",
      "loss是： tensor(102.5872, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 31.551724137931036\n",
      "loss是： tensor(90.8393, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 1.3333333333333333\n",
      "loss是： tensor(65.6540, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 29.333333333333332\n",
      "loss是： tensor(68.2988, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 34.375\n",
      "loss是： tensor(87.8462, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 66.66666666666667\n",
      "loss是： tensor(103.9136, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 21.538461538461537\n",
      "loss是： tensor(81.1300, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 28.448275862068964\n",
      "loss是： tensor(65.7669, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 24.038461538461537\n",
      "loss是： tensor(51.6765, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 9.733333333333334\n",
      "loss是： tensor(61.8943, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 31.875\n",
      "loss是： tensor(94.4870, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 37.74193548387097\n",
      "loss是： tensor(71.9094, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 32.8125\n",
      "loss是： tensor(85.4501, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 35.45454545454545\n",
      "loss是： tensor(82.0575, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 23.80952380952381\n",
      "loss是： tensor(74.3883, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 61.63636363636364\n",
      "loss是： tensor(123.5241, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 42.98507462686567\n",
      "loss是： tensor(94.9937, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 28.727272727272727\n",
      "loss是： tensor(64.1592, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 32.03125\n",
      "loss是： tensor(62.9745, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 37.76119402985075\n",
      "loss是： tensor(88.6452, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 35.2\n",
      "loss是： tensor(79.0483, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 21.93548387096774\n",
      "loss是： tensor(82.4131, device='cuda:0', grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert计算的loss是： 0.9090909090909092\n",
      "loss是： tensor(63.7997, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 50.0\n",
      "loss是： tensor(112.3524, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 25.0\n",
      "loss是： tensor(79.9995, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 39.35483870967742\n",
      "loss是： tensor(98.4175, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 31.8\n",
      "loss是： tensor(78.7518, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 45.0\n",
      "loss是： tensor(117.9526, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 31.470588235294116\n",
      "loss是： tensor(61.9244, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 33.28358208955224\n",
      "loss是： tensor(102.0748, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 35.714285714285715\n",
      "loss是： tensor(78.0292, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 14.133333333333333\n",
      "loss是： tensor(78.2054, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 27.719298245614038\n",
      "loss是： tensor(80.9333, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 24.153846153846153\n",
      "loss是： tensor(82.1871, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 32.698412698412696\n",
      "loss是： tensor(76.6593, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 36.96969696969697\n",
      "loss是： tensor(94.5535, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 58.305084745762706\n",
      "loss是： tensor(107.1181, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 39.29577464788733\n",
      "loss是： tensor(106.4680, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 31.09375\n",
      "loss是： tensor(84.7175, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 31.75438596491228\n",
      "loss是： tensor(76.7571, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 30.869565217391305\n",
      "loss是： tensor(85.1577, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 37.96296296296296\n",
      "loss是： tensor(86.3513, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 32.25806451612903\n",
      "loss是： tensor(93.9795, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 30.606060606060606\n",
      "loss是： tensor(102.3488, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 51.63934426229508\n",
      "loss是： tensor(117.9558, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 32.30769230769231\n",
      "loss是： tensor(81.4723, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 33.16666666666667\n",
      "loss是： tensor(82.3013, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 29.714285714285715\n",
      "loss是： tensor(77.9958, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 32.88461538461539\n",
      "loss是： tensor(68.3725, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 26.129032258064516\n",
      "loss是： tensor(62.1110, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 39.85294117647059\n",
      "loss是： tensor(100.5413, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 25.384615384615383\n",
      "loss是： tensor(63.3014, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 19.41176470588235\n",
      "loss是： tensor(49.4174, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 31.833333333333332\n",
      "loss是： tensor(96.9115, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 30.161290322580644\n",
      "loss是： tensor(76.6477, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 31.818181818181817\n",
      "loss是： tensor(66.7676, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 38.507462686567166\n",
      "loss是： tensor(107.0821, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 37.794117647058826\n",
      "loss是： tensor(85.9953, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 25.967741935483872\n",
      "loss是： tensor(93.1007, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 1.746031746031746\n",
      "loss是： tensor(53.8377, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 2.0\n",
      "loss是： tensor(62.9559, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 19.090909090909093\n",
      "loss是： tensor(65.2830, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 46.0\n",
      "loss是： tensor(135.0544, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 2.9230769230769234\n",
      "loss是： tensor(59.3595, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 27.758620689655174\n",
      "loss是： tensor(84.4020, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 36.52173913043478\n",
      "loss是： tensor(85.2158, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 29.090909090909093\n",
      "loss是： tensor(69.4223, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 32.57575757575758\n",
      "loss是： tensor(87.9944, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 39.85714285714286\n",
      "loss是： tensor(129.6300, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 38.8\n",
      "loss是： tensor(92.8768, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 23.77049180327869\n",
      "loss是： tensor(78.3479, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 29.242424242424242\n",
      "loss是： tensor(84.8061, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 35.483870967741936\n",
      "loss是： tensor(81.1459, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 52.83333333333333\n",
      "loss是： tensor(104.3939, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 8.947368421052632\n",
      "loss是： tensor(41.0445, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 66.10169491525424\n",
      "loss是： tensor(143.8196, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 35.79710144927536\n",
      "loss是： tensor(94.7083, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 34.54545454545455\n",
      "loss是： tensor(90.8007, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 26.825396825396822\n",
      "loss是： tensor(98.1561, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 27.758620689655174\n",
      "loss是： tensor(88.4614, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 33.013698630136986\n",
      "loss是： tensor(101.2213, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 40.57692307692307\n",
      "loss是： tensor(117.5909, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 37.25806451612903\n",
      "loss是： tensor(88.2752, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 26.166666666666668\n",
      "loss是： tensor(87.6040, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 39.48275862068965\n",
      "loss是： tensor(93.0902, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 33.60655737704918\n",
      "loss是： tensor(70.9522, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 31.076923076923077\n",
      "loss是： tensor(99.6534, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 33.42857142857143\n",
      "loss是： tensor(94.1355, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 43.833333333333336\n",
      "loss是： tensor(90.3141, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 29.365079365079364\n",
      "loss是： tensor(71.5343, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 38.199999999999996\n",
      "loss是： tensor(75.3914, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 43.4375\n",
      "loss是： tensor(125.7152, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 27.384615384615387\n",
      "loss是： tensor(72.1019, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 21.176470588235293\n",
      "loss是： tensor(68.8967, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 30.677966101694913\n",
      "loss是： tensor(84.9732, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 25.849056603773587\n",
      "loss是： tensor(98.2305, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 27.733333333333334\n",
      "loss是： tensor(111.8663, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 36.81818181818181\n",
      "loss是： tensor(62.8587, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 24.305555555555554\n",
      "loss是： tensor(58.1641, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 39.074074074074076\n",
      "loss是： tensor(73.6268, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 35.38461538461539\n",
      "loss是： tensor(84.6994, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 60.0\n",
      "loss是： tensor(116.6017, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 30.0\n",
      "loss是： tensor(76.2063, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 32.41935483870967\n",
      "loss是： tensor(77.0797, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 26.71232876712329\n",
      "loss是： tensor(108.9729, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 24.21875\n",
      "loss是： tensor(84.8981, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 31.846153846153843\n",
      "loss是： tensor(87.8264, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 39.66101694915254\n",
      "loss是： tensor(101.1343, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 32.80701754385965\n",
      "loss是： tensor(82.4449, device='cuda:0', grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert计算的loss是： 23.70967741935484\n",
      "loss是： tensor(60.4137, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 33.28358208955224\n",
      "loss是： tensor(82.9505, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 42.131147540983605\n",
      "loss是： tensor(113.1297, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 38.38709677419355\n",
      "loss是： tensor(106.0750, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 30.151515151515152\n",
      "loss是： tensor(95.2094, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 19.18032786885246\n",
      "loss是： tensor(69.4744, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 21.80327868852459\n",
      "loss是： tensor(82.2556, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 35.44117647058823\n",
      "loss是： tensor(105.8078, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 31.73076923076923\n",
      "loss是： tensor(105.1951, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 28.253968253968257\n",
      "loss是： tensor(86.0110, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 43.63636363636363\n",
      "loss是： tensor(132.4936, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 32.5\n",
      "loss是： tensor(56.7152, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 32.63157894736842\n",
      "loss是： tensor(76.5522, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 40.83333333333333\n",
      "loss是： tensor(82.7913, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 34.833333333333336\n",
      "loss是： tensor(111.4463, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 30.294117647058822\n",
      "loss是： tensor(100.9318, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 63.6231884057971\n",
      "loss是： tensor(124.9823, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 37.11864406779661\n",
      "loss是： tensor(99.1332, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 36.3013698630137\n",
      "loss是： tensor(99.8038, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 26.615384615384613\n",
      "loss是： tensor(98.9776, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 31.25\n",
      "loss是： tensor(85.7057, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 23.43283582089552\n",
      "loss是： tensor(77.4980, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 33.07692307692307\n",
      "loss是： tensor(98.1118, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 26.94915254237288\n",
      "loss是： tensor(79.9578, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 35.46875\n",
      "loss是： tensor(97.6785, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 22.096774193548384\n",
      "loss是： tensor(55.3453, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 33.59375\n",
      "loss是： tensor(91.7551, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 35.94202898550725\n",
      "loss是： tensor(81.3616, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 31.228070175438596\n",
      "loss是： tensor(86.8087, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 33.53846153846154\n",
      "loss是： tensor(75.1815, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 29.310344827586206\n",
      "loss是： tensor(90.5906, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 36.12903225806451\n",
      "loss是： tensor(123.5982, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 31.071428571428573\n",
      "loss是： tensor(87.7096, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 29.516129032258064\n",
      "loss是： tensor(79.9592, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 48.94736842105263\n",
      "loss是： tensor(73.8937, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 30.303030303030305\n",
      "loss是： tensor(99.7836, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 12.238805970149254\n",
      "loss是： tensor(67.8794, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 39.701492537313435\n",
      "loss是： tensor(96.9950, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 27.878787878787882\n",
      "loss是： tensor(96.4308, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 53.484848484848484\n",
      "loss是： tensor(125.2899, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 35.714285714285715\n",
      "loss是： tensor(102.1872, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 26.46153846153846\n",
      "loss是： tensor(97.3158, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 6.415094339622641\n",
      "loss是： tensor(49.7644, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 14.307692307692308\n",
      "loss是： tensor(53.1782, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 30.727272727272727\n",
      "loss是： tensor(94.5740, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 31.896551724137932\n",
      "loss是： tensor(74.2797, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 29.411764705882355\n",
      "loss是： tensor(85.0776, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 8.730158730158731\n",
      "loss是： tensor(67.5843, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 29.473684210526315\n",
      "loss是： tensor(77.9085, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 34.75409836065574\n",
      "loss是： tensor(76.7335, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 33.45454545454545\n",
      "loss是： tensor(100.3027, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 57.74193548387097\n",
      "loss是： tensor(122.8198, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 23.38235294117647\n",
      "loss是： tensor(80.8697, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 28.666666666666668\n",
      "loss是： tensor(78.2493, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 30.727272727272727\n",
      "loss是： tensor(75.3089, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 31.846153846153843\n",
      "loss是： tensor(102.2718, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 37.79661016949153\n",
      "loss是： tensor(81.0438, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 38.139534883720934\n",
      "loss是： tensor(74.5354, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "290/300\n",
      "bert计算的loss是： 25.538461538461537\n",
      "loss是： tensor(72.2201, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 28.06451612903226\n",
      "loss是： tensor(81.2245, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 12.34375\n",
      "loss是： tensor(76.1931, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 29.807692307692307\n",
      "loss是： tensor(71.1389, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 39.12280701754386\n",
      "loss是： tensor(86.7400, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 33.47826086956522\n",
      "loss是： tensor(78.6492, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 40.77922077922078\n",
      "loss是： tensor(105.9428, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 37.3134328358209\n",
      "loss是： tensor(109.6202, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 28.275862068965516\n",
      "loss是： tensor(77.7123, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 48.28125\n",
      "loss是： tensor(104.6358, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 41.45161290322581\n",
      "loss是： tensor(92.8617, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 40.588235294117645\n",
      "loss是： tensor(128.1999, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 21.85185185185185\n",
      "loss是： tensor(61.7077, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 23.88888888888889\n",
      "loss是： tensor(73.4388, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 60.50847457627118\n",
      "loss是： tensor(117.1657, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 36.61764705882353\n",
      "loss是： tensor(116.6203, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 8.405797101449275\n",
      "loss是： tensor(69.6696, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 24.126984126984127\n",
      "loss是： tensor(85.0574, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 25.73529411764706\n",
      "loss是： tensor(88.2872, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 32.83018867924528\n",
      "loss是： tensor(87.6435, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 33.96551724137931\n",
      "loss是： tensor(77.8016, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 35.39682539682539\n",
      "loss是： tensor(77.1588, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 40.5\n",
      "loss是： tensor(71.3886, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 0.0\n",
      "loss是： tensor(55.3273, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 33.859649122807014\n",
      "loss是： tensor(102.2683, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 25.606060606060606\n",
      "loss是： tensor(73.7419, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 33.43283582089552\n",
      "loss是： tensor(96.5191, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 29.076923076923077\n",
      "loss是： tensor(81.7526, device='cuda:0', grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert计算的loss是： 24.137931034482758\n",
      "loss是： tensor(61.8338, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 58.833333333333336\n",
      "loss是： tensor(86.1593, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 24.35483870967742\n",
      "loss是： tensor(67.2259, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 31.0\n",
      "loss是： tensor(89.2326, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 34.76190476190476\n",
      "loss是： tensor(81.3988, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 31.363636363636363\n",
      "loss是： tensor(81.5193, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 33.57142857142857\n",
      "loss是： tensor(89.6726, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 39.285714285714285\n",
      "loss是： tensor(84.0469, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 22.295081967213115\n",
      "loss是： tensor(57.3252, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 35.39682539682539\n",
      "loss是： tensor(83.6960, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 29.81818181818182\n",
      "loss是： tensor(80.4038, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 29.032258064516128\n",
      "loss是： tensor(78.3859, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 34.74576271186441\n",
      "loss是： tensor(89.1070, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 41.911764705882355\n",
      "loss是： tensor(124.0070, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 35.9375\n",
      "loss是： tensor(96.2680, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 35.79710144927536\n",
      "loss是： tensor(84.1934, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 30.5\n",
      "loss是： tensor(78.3286, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 32.325581395348834\n",
      "loss是： tensor(80.2078, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 38.76923076923077\n",
      "loss是： tensor(92.9815, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 25.555555555555554\n",
      "loss是： tensor(58.8418, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 35.94202898550725\n",
      "loss是： tensor(90.7241, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 44.91803278688525\n",
      "loss是： tensor(86.3888, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 41.607142857142854\n",
      "loss是： tensor(88.2801, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 30.483870967741936\n",
      "loss是： tensor(66.5707, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 29.491525423728817\n",
      "loss是： tensor(67.4670, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 32.46376811594203\n",
      "loss是： tensor(73.6472, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 32.20779220779221\n",
      "loss是： tensor(103.2839, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 28.656716417910445\n",
      "loss是： tensor(81.5161, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 39.67741935483871\n",
      "loss是： tensor(114.9730, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 33.38235294117647\n",
      "loss是： tensor(92.4255, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 35.64516129032258\n",
      "loss是： tensor(90.5258, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 57.5\n",
      "loss是： tensor(115.5341, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 36.12903225806451\n",
      "loss是： tensor(82.1317, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 26.666666666666664\n",
      "loss是： tensor(67.1896, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 7.1875\n",
      "loss是： tensor(50.0499, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 21.228070175438596\n",
      "loss是： tensor(61.8661, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 37.36842105263158\n",
      "loss是： tensor(71.7943, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 33.27272727272727\n",
      "loss是： tensor(111.7833, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 24.603174603174605\n",
      "loss是： tensor(89.3808, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 34.72222222222222\n",
      "loss是： tensor(89.9307, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 26.451612903225804\n",
      "loss是： tensor(48.9316, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 33.392857142857146\n",
      "loss是： tensor(90.5606, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 28.5\n",
      "loss是： tensor(71.3540, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 48.07017543859649\n",
      "loss是： tensor(96.0756, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 5.59322033898305\n",
      "loss是： tensor(41.7053, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 39.07692307692307\n",
      "loss是： tensor(110.9030, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 25.901639344262296\n",
      "loss是： tensor(77.1674, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 24.40677966101695\n",
      "loss是： tensor(81.9243, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 37.285714285714285\n",
      "loss是： tensor(92.4314, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 40.0\n",
      "loss是： tensor(92.8852, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 35.0\n",
      "loss是： tensor(72.8503, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 31.846153846153843\n",
      "loss是： tensor(81.0740, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 35.08196721311475\n",
      "loss是： tensor(78.4195, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 32.05882352941177\n",
      "loss是： tensor(109.5425, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 30.508474576271183\n",
      "loss是： tensor(88.5536, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 28.70967741935484\n",
      "loss是： tensor(87.7341, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 74.7457627118644\n",
      "loss是： tensor(134.8378, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 34.60317460317461\n",
      "loss是： tensor(94.9734, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 25.166666666666664\n",
      "loss是： tensor(60.7279, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 35.84615384615385\n",
      "loss是： tensor(97.4728, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 14.918032786885247\n",
      "loss是： tensor(66.4235, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 30.0\n",
      "loss是： tensor(77.4663, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 31.470588235294116\n",
      "loss是： tensor(91.6880, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 8.78048780487805\n",
      "loss是： tensor(33.1353, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 30.892857142857146\n",
      "loss是： tensor(79.9329, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 25.344827586206897\n",
      "loss是： tensor(62.9277, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 8.775510204081632\n",
      "loss是： tensor(52.7941, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 35.44117647058823\n",
      "loss是： tensor(84.8002, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 28.63013698630137\n",
      "loss是： tensor(88.3965, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 13.157894736842106\n",
      "loss是： tensor(50.9639, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 29.384615384615387\n",
      "loss是： tensor(90.8706, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 41.38461538461539\n",
      "loss是： tensor(93.9316, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 38.0\n",
      "loss是： tensor(98.0894, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 13.548387096774192\n",
      "loss是： tensor(115.6322, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 35.625\n",
      "loss是： tensor(96.9311, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 27.346938775510203\n",
      "loss是： tensor(63.9386, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 26.379310344827584\n",
      "loss是： tensor(65.9683, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 44.09836065573771\n",
      "loss是： tensor(100.1394, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 33.43283582089552\n",
      "loss是： tensor(114.2681, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 26.440677966101696\n",
      "loss是： tensor(66.5059, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 41.16883116883117\n",
      "loss是： tensor(107.0915, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 41.166666666666664\n",
      "loss是： tensor(99.5562, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 27.936507936507937\n",
      "loss是： tensor(84.3215, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 32.95774647887324\n",
      "loss是： tensor(102.3974, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 39.53125\n",
      "loss是： tensor(96.7996, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 37.46031746031746\n",
      "loss是： tensor(80.5388, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 24.146341463414632\n",
      "loss是： tensor(57.6383, device='cuda:0', grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert计算的loss是： 32.41935483870967\n",
      "loss是： tensor(77.0862, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 44.375\n",
      "loss是： tensor(107.8715, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 34.92307692307692\n",
      "loss是： tensor(71.9724, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 29.846153846153847\n",
      "loss是： tensor(72.8680, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 37.096774193548384\n",
      "loss是： tensor(94.1147, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 30.425531914893615\n",
      "loss是： tensor(65.8626, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 25.73529411764706\n",
      "loss是： tensor(81.6777, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 57.213114754098356\n",
      "loss是： tensor(113.3928, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 45.81818181818182\n",
      "loss是： tensor(79.8793, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 39.074074074074076\n",
      "loss是： tensor(78.3334, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 29.295774647887324\n",
      "loss是： tensor(80.0367, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 13.38235294117647\n",
      "loss是： tensor(90.4326, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 36.06060606060606\n",
      "loss是： tensor(138.3046, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 27.017543859649123\n",
      "loss是： tensor(76.1866, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 33.114754098360656\n",
      "loss是： tensor(92.3136, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 29.491525423728817\n",
      "loss是： tensor(58.6879, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 39.85294117647059\n",
      "loss是： tensor(85.8989, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 33.16666666666667\n",
      "loss是： tensor(99.6660, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 32.78481012658228\n",
      "loss是： tensor(95.4297, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 29.558823529411768\n",
      "loss是： tensor(93.4548, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 24.727272727272727\n",
      "loss是： tensor(88.1830, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 36.140350877192986\n",
      "loss是： tensor(103.2265, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 12.325581395348838\n",
      "loss是： tensor(48.7602, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 33.73134328358209\n",
      "loss是： tensor(64.5546, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 51.754385964912274\n",
      "loss是： tensor(107.5112, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 59.516129032258064\n",
      "loss是： tensor(137.1518, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 28.666666666666668\n",
      "loss是： tensor(75.2245, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 27.580645161290324\n",
      "loss是： tensor(65.5829, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 34.66666666666667\n",
      "loss是： tensor(77.1187, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 3.968253968253968\n",
      "loss是： tensor(62.4327, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 26.896551724137932\n",
      "loss是： tensor(84.3976, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 30.307692307692307\n",
      "loss是： tensor(77.9339, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 29.384615384615387\n",
      "loss是： tensor(85.2282, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 36.507936507936506\n",
      "loss是： tensor(104.7365, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 33.54838709677419\n",
      "loss是： tensor(79.6159, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 29.11764705882353\n",
      "loss是： tensor(111.2615, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 23.571428571428573\n",
      "loss是： tensor(69.4423, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 29.242424242424242\n",
      "loss是： tensor(70.5504, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 30.29850746268657\n",
      "loss是： tensor(67.2536, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 30.344827586206897\n",
      "loss是： tensor(74.2708, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 25.64516129032258\n",
      "loss是： tensor(77.0565, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 31.96078431372549\n",
      "loss是： tensor(71.9360, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 37.16417910447761\n",
      "loss是： tensor(78.0562, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 29.6875\n",
      "loss是： tensor(74.9426, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 30.847457627118647\n",
      "loss是： tensor(80.9994, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 35.13513513513514\n",
      "loss是： tensor(43.1726, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 33.728813559322035\n",
      "loss是： tensor(101.2235, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 30.793650793650794\n",
      "loss是： tensor(97.3493, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 61.45161290322581\n",
      "loss是： tensor(130.0954, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 27.586206896551726\n",
      "loss是： tensor(99.5807, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 39.82142857142857\n",
      "loss是： tensor(82.2537, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 26.769230769230766\n",
      "loss是： tensor(71.4714, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 32.794117647058826\n",
      "loss是： tensor(88.8096, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 21.904761904761905\n",
      "loss是： tensor(86.0542, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 36.935483870967744\n",
      "loss是： tensor(72.8592, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 34.285714285714285\n",
      "loss是： tensor(100.2954, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 46.0344827586207\n",
      "loss是： tensor(93.5044, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 33.66197183098591\n",
      "loss是： tensor(94.4180, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 37.63636363636364\n",
      "loss是： tensor(68.8485, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 33.73134328358209\n",
      "loss是： tensor(77.8541, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 31.52777777777778\n",
      "loss是： tensor(83.2369, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 34.05797101449275\n",
      "loss是： tensor(91.7456, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 20.714285714285715\n",
      "loss是： tensor(63.3037, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 37.72727272727273\n",
      "loss是： tensor(110.7457, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 33.225806451612904\n",
      "loss是： tensor(91.7915, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 32.96875\n",
      "loss是： tensor(77.4302, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 34.3859649122807\n",
      "loss是： tensor(72.0587, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 29.838709677419356\n",
      "loss是： tensor(94.5127, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 26.944444444444446\n",
      "loss是： tensor(65.4485, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 9.82456140350877\n",
      "loss是： tensor(61.3001, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 29.242424242424242\n",
      "loss是： tensor(99.2586, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 21.166666666666668\n",
      "loss是： tensor(72.5921, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 8.615384615384617\n",
      "loss是： tensor(64.7828, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 32.903225806451616\n",
      "loss是： tensor(71.9737, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 41.92982456140351\n",
      "loss是： tensor(105.5640, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 23.59375\n",
      "loss是： tensor(72.4441, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 38.0327868852459\n",
      "loss是： tensor(96.3051, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 27.77777777777778\n",
      "loss是： tensor(69.3338, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 28.732394366197184\n",
      "loss是： tensor(97.8197, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 24.67741935483871\n",
      "loss是： tensor(71.3526, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 66.09375\n",
      "loss是： tensor(145.7600, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 44.6875\n",
      "loss是： tensor(90.7289, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 44.83870967741935\n",
      "loss是： tensor(85.2722, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 26.551724137931036\n",
      "loss是： tensor(68.5667, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 25.967741935483872\n",
      "loss是： tensor(74.5537, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 33.333333333333336\n",
      "loss是： tensor(77.9354, device='cuda:0', grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert计算的loss是： 26.25\n",
      "loss是： tensor(98.1120, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 30.476190476190474\n",
      "loss是： tensor(89.3540, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 31.694915254237287\n",
      "loss是： tensor(80.2506, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 35.32258064516129\n",
      "loss是： tensor(123.3907, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 30.588235294117645\n",
      "loss是： tensor(98.7277, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 33.11111111111111\n",
      "loss是： tensor(65.3824, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "正在训练第 4轮\n",
      "Model(\n",
      "  (gen_embedding): Embedding(8518, 300)\n",
      "  (conv1): Conv1d(300, 128, kernel_size=(5,), stride=(1,), padding=(2,))\n",
      "  (conv2): Conv1d(300, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "  (dropout): Dropout(p=0.55, inplace=False)\n",
      "  (conv3): Conv1d(256, 256, kernel_size=(5,), stride=(1,), padding=(2,))\n",
      "  (conv4): Conv1d(256, 256, kernel_size=(5,), stride=(1,), padding=(2,))\n",
      "  (conv5): Conv1d(256, 256, kernel_size=(5,), stride=(1,), padding=(2,))\n",
      "  (linear_ae): Linear(in_features=256, out_features=3, bias=True)\n",
      "  (crf): ConditionalRandomField()\n",
      ")\n",
      "0/300\n",
      "10/300\n",
      "20/300\n",
      "30/300\n",
      "40/300\n",
      "50/300\n",
      "60/300\n",
      "70/300\n",
      "80/300\n",
      "90/300\n",
      "100/300\n",
      "110/300\n",
      "120/300\n",
      "130/300\n",
      "140/300\n",
      "150/300\n",
      "160/300\n",
      "170/300\n",
      "180/300\n",
      "190/300\n",
      "200/300\n",
      "210/300\n",
      "220/300\n",
      "230/300\n",
      "240/300\n",
      "bert计算的loss是： 27.246376811594203\n",
      "loss是： tensor(90.3920, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 34.44444444444444\n",
      "loss是： tensor(94.1053, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 33.125\n",
      "loss是： tensor(108.4833, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 31.85185185185185\n",
      "loss是： tensor(96.1993, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 18.095238095238095\n",
      "loss是： tensor(69.7160, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 31.166666666666668\n",
      "loss是： tensor(77.5211, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 33.666666666666664\n",
      "loss是： tensor(75.0952, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 32.46376811594203\n",
      "loss是： tensor(76.9031, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 29.53125\n",
      "loss是： tensor(74.6753, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 1.3846153846153846\n",
      "loss是： tensor(53.5875, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 46.11111111111111\n",
      "loss是： tensor(90.2096, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 47.61904761904762\n",
      "loss是： tensor(104.4538, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 31.147540983606557\n",
      "loss是： tensor(70.2262, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 37.333333333333336\n",
      "loss是： tensor(88.1282, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 29.15492957746479\n",
      "loss是： tensor(108.7778, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 37.79661016949153\n",
      "loss是： tensor(90.7599, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 26.911764705882355\n",
      "loss是： tensor(90.0346, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 37.5\n",
      "loss是： tensor(78.0585, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 28.80597014925373\n",
      "loss是： tensor(74.0280, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 37.63636363636364\n",
      "loss是： tensor(97.3945, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 49.15492957746479\n",
      "loss是： tensor(117.6957, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 33.768115942028984\n",
      "loss是： tensor(90.3332, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 42.56410256410256\n",
      "loss是： tensor(79.4952, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "250/300\n",
      "bert计算的loss是： 29.545454545454547\n",
      "loss是： tensor(83.5300, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 31.515151515151516\n",
      "loss是： tensor(87.6012, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 25.57142857142857\n",
      "loss是： tensor(116.3331, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 34.76190476190476\n",
      "loss是： tensor(95.6969, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 22.950819672131146\n",
      "loss是： tensor(81.9124, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 14.03225806451613\n",
      "loss是： tensor(68.4030, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 28.676470588235293\n",
      "loss是： tensor(76.9538, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 30.46875\n",
      "loss是： tensor(107.3594, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 68.66666666666666\n",
      "loss是： tensor(111.6261, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 50.86206896551724\n",
      "loss是： tensor(86.9887, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 22.388059701492537\n",
      "loss是： tensor(79.7319, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 32.42857142857143\n",
      "loss是： tensor(116.7843, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 44.09836065573771\n",
      "loss是： tensor(108.1376, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 32.83333333333333\n",
      "loss是： tensor(108.4577, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 24.35483870967742\n",
      "loss是： tensor(78.3557, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 33.90625\n",
      "loss是： tensor(99.8266, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 30.526315789473685\n",
      "loss是： tensor(74.9669, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 38.45070422535211\n",
      "loss是： tensor(121.1675, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 46.25\n",
      "loss是： tensor(125.6441, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 36.029411764705884\n",
      "loss是： tensor(103.3774, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 24.754098360655735\n",
      "loss是： tensor(86.6986, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 27.735849056603776\n",
      "loss是： tensor(77.3837, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 30.909090909090907\n",
      "loss是： tensor(69.5431, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 28.571428571428573\n",
      "loss是： tensor(85.9498, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 33.78787878787879\n",
      "loss是： tensor(103.0178, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 21.041666666666664\n",
      "loss是： tensor(76.5326, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 32.71186440677966\n",
      "loss是： tensor(104.2232, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 51.80555555555556\n",
      "loss是： tensor(122.5433, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 40.563380281690144\n",
      "loss是： tensor(118.6378, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 24.629629629629626\n",
      "loss是： tensor(53.5157, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 40.15384615384615\n",
      "loss是： tensor(128.2563, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 41.904761904761905\n",
      "loss是： tensor(101.1389, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 28.5\n",
      "loss是： tensor(88.5828, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 30.677966101694913\n",
      "loss是： tensor(85.8096, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 29.42857142857143\n",
      "loss是： tensor(82.7316, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 35.60606060606061\n",
      "loss是： tensor(103.4350, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 29.836065573770494\n",
      "loss是： tensor(110.1449, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 28.225806451612904\n",
      "loss是： tensor(87.7758, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 26.19047619047619\n",
      "loss是： tensor(96.1858, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 31.551724137931036\n",
      "loss是： tensor(90.6526, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 40.666666666666664\n",
      "loss是： tensor(116.9952, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 36.774193548387096\n",
      "loss是： tensor(95.7617, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 31.11111111111111\n",
      "loss是： tensor(102.6179, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 40.16949152542373\n",
      "loss是： tensor(100.7342, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 23.833333333333332\n",
      "loss是： tensor(101.4620, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 33.84615384615385\n",
      "loss是： tensor(78.1827, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 37.142857142857146\n",
      "loss是： tensor(84.4434, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 29.82456140350877\n",
      "loss是： tensor(80.4973, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 21.785714285714285\n",
      "loss是： tensor(69.0147, device='cuda:0', grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert计算的loss是： 28.64406779661017\n",
      "loss是： tensor(60.1818, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 34.34782608695652\n",
      "loss是： tensor(119.8105, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 29.523809523809526\n",
      "loss是： tensor(88.7456, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 62.0\n",
      "loss是： tensor(135.7702, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 35.5\n",
      "loss是： tensor(71.1132, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 35.38461538461539\n",
      "loss是： tensor(89.5786, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 28.125\n",
      "loss是： tensor(77.8381, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 30.166666666666664\n",
      "loss是： tensor(78.9287, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 35.40983606557377\n",
      "loss是： tensor(88.2325, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 34.30769230769231\n",
      "loss是： tensor(92.1369, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 43.6231884057971\n",
      "loss是： tensor(110.0093, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 31.044776119402986\n",
      "loss是： tensor(91.6511, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 14.0\n",
      "loss是： tensor(61.6675, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 9.836065573770492\n",
      "loss是： tensor(51.5276, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 30.151515151515152\n",
      "loss是： tensor(87.3308, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 36.98412698412699\n",
      "loss是： tensor(89.7856, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 22.205882352941178\n",
      "loss是： tensor(78.3361, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 28.545454545454547\n",
      "loss是： tensor(70.6999, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 42.71186440677966\n",
      "loss是： tensor(122.0484, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 30.46511627906977\n",
      "loss是： tensor(67.3819, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 32.25806451612903\n",
      "loss是： tensor(81.9452, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 24.696969696969695\n",
      "loss是： tensor(97.6995, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 23.650793650793652\n",
      "loss是： tensor(87.7690, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 33.51851851851852\n",
      "loss是： tensor(92.2352, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 25.217391304347828\n",
      "loss是： tensor(94.3006, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 36.111111111111114\n",
      "loss是： tensor(98.7443, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 33.28125\n",
      "loss是： tensor(94.5669, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 58.47457627118644\n",
      "loss是： tensor(107.5873, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 27.10144927536232\n",
      "loss是： tensor(87.0063, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 35.230769230769226\n",
      "loss是： tensor(99.6071, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 31.343283582089555\n",
      "loss是： tensor(92.7691, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 29.387755102040817\n",
      "loss是： tensor(77.2252, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 35.55555555555556\n",
      "loss是： tensor(122.2283, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 44.626865671641795\n",
      "loss是： tensor(113.7765, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 25.483870967741936\n",
      "loss是： tensor(92.0638, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 25.901639344262296\n",
      "loss是： tensor(66.2045, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 31.323529411764707\n",
      "loss是： tensor(93.2598, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 39.21875\n",
      "loss是： tensor(103.7744, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 35.61643835616438\n",
      "loss是： tensor(98.2321, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 48.245614035087726\n",
      "loss是： tensor(103.6411, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 23.70967741935484\n",
      "loss是： tensor(80.4410, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 41.89655172413793\n",
      "loss是： tensor(106.1331, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 11.891891891891893\n",
      "loss是： tensor(42.5195, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 34.72222222222222\n",
      "loss是： tensor(114.4022, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 33.442622950819676\n",
      "loss是： tensor(86.0594, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 37.8125\n",
      "loss是： tensor(105.0423, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 39.25373134328358\n",
      "loss是： tensor(100.5580, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 31.76470588235294\n",
      "loss是： tensor(77.4128, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 47.5\n",
      "loss是： tensor(118.2813, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 19.46666666666667\n",
      "loss是： tensor(102.1324, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 30.943396226415096\n",
      "loss是： tensor(70.4462, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 34.516129032258064\n",
      "loss是： tensor(100.2744, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 20.943396226415096\n",
      "loss是： tensor(65.7209, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 44.909090909090914\n",
      "loss是： tensor(114.5115, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 25.238095238095237\n",
      "loss是： tensor(104.0394, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 20.0\n",
      "loss是： tensor(65.5649, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 27.796610169491526\n",
      "loss是： tensor(82.4184, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 17.241379310344826\n",
      "loss是： tensor(83.1764, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 32.698412698412696\n",
      "loss是： tensor(85.8548, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 32.53968253968254\n",
      "loss是： tensor(103.7172, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 27.01492537313433\n",
      "loss是： tensor(85.4383, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 65.0\n",
      "loss是： tensor(119.3830, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 29.82758620689655\n",
      "loss是： tensor(79.6464, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 42.87878787878788\n",
      "loss是： tensor(112.2236, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 4.242424242424242\n",
      "loss是： tensor(70.3906, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 33.513513513513516\n",
      "loss是： tensor(60.2473, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 59.714285714285715\n",
      "loss是： tensor(127.0687, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 27.586206896551726\n",
      "loss是： tensor(71.1024, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 31.538461538461537\n",
      "loss是： tensor(101.1815, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 34.25925925925926\n",
      "loss是： tensor(93.7316, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 34.50704225352113\n",
      "loss是： tensor(80.2322, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 40.16129032258065\n",
      "loss是： tensor(94.1486, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 30.140845070422536\n",
      "loss是： tensor(103.1315, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 42.72727272727273\n",
      "loss是： tensor(88.7688, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 33.859649122807014\n",
      "loss是： tensor(100.3794, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 29.402985074626862\n",
      "loss是： tensor(102.5820, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 35.483870967741936\n",
      "loss是： tensor(99.7863, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 24.655172413793103\n",
      "loss是： tensor(65.3689, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 33.69230769230769\n",
      "loss是： tensor(106.7477, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 27.460317460317462\n",
      "loss是： tensor(87.6621, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 31.166666666666668\n",
      "loss是： tensor(99.4814, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 8.225806451612904\n",
      "loss是： tensor(72.2798, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 35.3448275862069\n",
      "loss是： tensor(88.1386, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 33.05084745762712\n",
      "loss是： tensor(92.1633, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 25.614035087719298\n",
      "loss是： tensor(72.1363, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 29.67741935483871\n",
      "loss是： tensor(74.5232, device='cuda:0', grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert计算的loss是： 30.151515151515152\n",
      "loss是： tensor(114.2469, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 40.72463768115942\n",
      "loss是： tensor(100.5324, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 14.324324324324325\n",
      "loss是： tensor(35.2297, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 27.77777777777778\n",
      "loss是： tensor(104.6749, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 27.61904761904762\n",
      "loss是： tensor(74.6237, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 16.714285714285715\n",
      "loss是： tensor(81.8823, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 30.161290322580644\n",
      "loss是： tensor(90.1739, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 42.03125\n",
      "loss是： tensor(94.1180, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 69.84126984126985\n",
      "loss是： tensor(140.5409, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 30.35087719298246\n",
      "loss是： tensor(94.9609, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 33.07692307692307\n",
      "loss是： tensor(73.1287, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 37.96296296296296\n",
      "loss是： tensor(88.2758, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 30.4\n",
      "loss是： tensor(79.5956, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 33.492063492063494\n",
      "loss是： tensor(84.3413, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 29.70149253731343\n",
      "loss是： tensor(86.1422, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 34.62686567164179\n",
      "loss是： tensor(91.1510, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 31.864406779661017\n",
      "loss是： tensor(103.0181, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 29.682539682539684\n",
      "loss是： tensor(100.0855, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 9.230769230769232\n",
      "loss是： tensor(53.7335, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 32.18181818181818\n",
      "loss是： tensor(110.6717, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 28.142857142857146\n",
      "loss是： tensor(100.2961, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 47.285714285714285\n",
      "loss是： tensor(119.6494, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 34.91803278688525\n",
      "loss是： tensor(94.5003, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 26.10169491525424\n",
      "loss是： tensor(82.0812, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 35.072463768115945\n",
      "loss是： tensor(117.9917, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 37.1875\n",
      "loss是： tensor(62.6202, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 32.32142857142857\n",
      "loss是： tensor(81.2226, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 45.19230769230769\n",
      "loss是： tensor(100.6535, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 25.396825396825395\n",
      "loss是： tensor(70.4524, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 7.571428571428571\n",
      "loss是： tensor(87.9128, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 57.83333333333333\n",
      "loss是： tensor(103.8572, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 40.625\n",
      "loss是： tensor(103.8666, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 26.06060606060606\n",
      "loss是： tensor(83.6716, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 22.388059701492537\n",
      "loss是： tensor(71.4115, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 34.33333333333333\n",
      "loss是： tensor(76.8884, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 40.3448275862069\n",
      "loss是： tensor(105.5708, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 31.904761904761905\n",
      "loss是： tensor(79.5474, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 27.647058823529413\n",
      "loss是： tensor(91.4808, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 36.57142857142857\n",
      "loss是： tensor(112.1168, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 40.33333333333333\n",
      "loss是： tensor(98.7588, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 42.57575757575758\n",
      "loss是： tensor(123.4725, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 36.89655172413793\n",
      "loss是： tensor(91.6249, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 33.38461538461539\n",
      "loss是： tensor(89.6292, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 34.193548387096776\n",
      "loss是： tensor(99.0474, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 33.44827586206896\n",
      "loss是： tensor(84.2202, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 25.538461538461537\n",
      "loss是： tensor(95.1275, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 28.923076923076923\n",
      "loss是： tensor(111.3213, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 34.60317460317461\n",
      "loss是： tensor(109.0432, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 31.666666666666664\n",
      "loss是： tensor(55.3798, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 25.142857142857142\n",
      "loss是： tensor(77.6576, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 44.909090909090914\n",
      "loss是： tensor(114.2844, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 49.23076923076923\n",
      "loss是： tensor(126.7186, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 32.76923076923077\n",
      "loss是： tensor(98.8803, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 50.8955223880597\n",
      "loss是： tensor(102.4087, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 25.88235294117647\n",
      "loss是： tensor(77.4109, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 17.301587301587304\n",
      "loss是： tensor(64.5569, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 36.18181818181818\n",
      "loss是： tensor(102.6743, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 20.555555555555554\n",
      "loss是： tensor(84.2922, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 35.53846153846153\n",
      "loss是： tensor(108.1041, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 33.69230769230769\n",
      "loss是： tensor(109.8454, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 55.71428571428571\n",
      "loss是： tensor(126.6665, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 26.164383561643834\n",
      "loss是： tensor(98.4980, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 34.5\n",
      "loss是： tensor(99.0144, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 27.540983606557376\n",
      "loss是： tensor(89.8390, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 26.071428571428573\n",
      "loss是： tensor(82.0898, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 30.294117647058822\n",
      "loss是： tensor(79.0134, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 42.98245614035088\n",
      "loss是： tensor(135.8022, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 31.940298507462686\n",
      "loss是： tensor(95.3669, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 33.125\n",
      "loss是： tensor(81.9697, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 12.903225806451612\n",
      "loss是： tensor(79.1752, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 34.225352112676056\n",
      "loss是： tensor(102.0845, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 23.055555555555554\n",
      "loss是： tensor(56.6609, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 37.704918032786885\n",
      "loss是： tensor(113.5642, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 31.320754716981135\n",
      "loss是： tensor(73.7450, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 30.0\n",
      "loss是： tensor(88.3266, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 34.0\n",
      "loss是： tensor(82.5913, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 30.666666666666668\n",
      "loss是： tensor(78.7297, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 29.130434782608695\n",
      "loss是： tensor(88.0689, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 15.223880597014926\n",
      "loss是： tensor(72.7161, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 29.206349206349206\n",
      "loss是： tensor(103.2525, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 30.0\n",
      "loss是： tensor(85.9378, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 26.727272727272727\n",
      "loss是： tensor(70.2213, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 44.72727272727273\n",
      "loss是： tensor(97.6844, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 27.8125\n",
      "loss是： tensor(72.3685, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 43.125\n",
      "loss是： tensor(112.9339, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 10.735294117647058\n",
      "loss是： tensor(58.8409, device='cuda:0', grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert计算的loss是： 29.70149253731343\n",
      "loss是： tensor(97.8065, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 30.405405405405403\n",
      "loss是： tensor(93.7974, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 39.11764705882353\n",
      "loss是： tensor(129.9253, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 32.5\n",
      "loss是： tensor(68.3923, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 25.714285714285715\n",
      "loss是： tensor(70.0576, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 42.5\n",
      "loss是： tensor(92.4444, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 67.94117647058823\n",
      "loss是： tensor(147.4639, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 28.70967741935484\n",
      "loss是： tensor(76.2010, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 33.61702127659575\n",
      "loss是： tensor(85.0198, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "260/300\n",
      "bert计算的loss是： 36.76056338028169\n",
      "loss是： tensor(105.9016, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 27.27272727272727\n",
      "loss是： tensor(105.1148, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 31.0\n",
      "loss是： tensor(95.7314, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 38.42857142857143\n",
      "loss是： tensor(98.1865, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 51.06060606060606\n",
      "loss是： tensor(122.5995, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 28.461538461538463\n",
      "loss是： tensor(89.9568, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 36.111111111111114\n",
      "loss是： tensor(94.1370, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 31.833333333333332\n",
      "loss是： tensor(69.9013, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 33.20754716981132\n",
      "loss是： tensor(91.2782, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 21.774193548387096\n",
      "loss是： tensor(49.7868, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 26.666666666666664\n",
      "loss是： tensor(59.6326, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 30.149253731343283\n",
      "loss是： tensor(77.8876, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 42.87878787878788\n",
      "loss是： tensor(101.8029, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 32.096774193548384\n",
      "loss是： tensor(108.8241, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 36.666666666666664\n",
      "loss是： tensor(76.0953, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 31.875\n",
      "loss是： tensor(75.9553, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 28.87323943661972\n",
      "loss是： tensor(87.4128, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 36.714285714285715\n",
      "loss是： tensor(85.8495, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 33.57142857142857\n",
      "loss是： tensor(95.6198, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 30.17241379310345\n",
      "loss是： tensor(110.0162, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 49.0625\n",
      "loss是： tensor(99.8483, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 30.689655172413794\n",
      "loss是： tensor(70.7325, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 22.564102564102562\n",
      "loss是： tensor(54.0317, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 27.457627118644066\n",
      "loss是： tensor(70.2268, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 35.39682539682539\n",
      "loss是： tensor(77.7015, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 31.833333333333332\n",
      "loss是： tensor(84.4841, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 24.0\n",
      "loss是： tensor(82.3880, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 37.16666666666667\n",
      "loss是： tensor(94.7708, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 39.523809523809526\n",
      "loss是： tensor(110.4640, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 30.454545454545453\n",
      "loss是： tensor(133.0515, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 23.442622950819676\n",
      "loss是： tensor(94.6761, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 28.833333333333332\n",
      "loss是： tensor(93.3547, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 33.9344262295082\n",
      "loss是： tensor(100.4187, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 38.656716417910445\n",
      "loss是： tensor(99.7910, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 34.21875\n",
      "loss是： tensor(100.2894, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 32.30769230769231\n",
      "loss是： tensor(83.2335, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 39.57142857142857\n",
      "loss是： tensor(104.7016, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 39.14285714285714\n",
      "loss是： tensor(92.1139, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 30.144927536231883\n",
      "loss是： tensor(111.6594, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 59.850746268656714\n",
      "loss是： tensor(122.1521, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 21.636363636363637\n",
      "loss是： tensor(72.3399, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 29.0625\n",
      "loss是： tensor(100.9793, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 32.27272727272727\n",
      "loss是： tensor(91.3454, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 46.78571428571429\n",
      "loss是： tensor(97.4899, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 32.36363636363636\n",
      "loss是： tensor(73.5325, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 38.46153846153846\n",
      "loss是： tensor(72.9050, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 43.4375\n",
      "loss是： tensor(117.4395, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 17.586206896551722\n",
      "loss是： tensor(80.5517, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 29.857142857142858\n",
      "loss是： tensor(98.0644, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 35.714285714285715\n",
      "loss是： tensor(95.6363, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 39.81132075471698\n",
      "loss是： tensor(80.7795, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 31.333333333333332\n",
      "loss是： tensor(79.0763, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 43.88059701492537\n",
      "loss是： tensor(95.0214, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 25.846153846153847\n",
      "loss是： tensor(77.3621, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 27.017543859649123\n",
      "loss是： tensor(77.2259, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 6.25\n",
      "loss是： tensor(68.1474, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 69.10714285714286\n",
      "loss是： tensor(141.4303, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 26.71875\n",
      "loss是： tensor(86.2996, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 34.91525423728814\n",
      "loss是： tensor(86.6798, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 27.03125\n",
      "loss是： tensor(68.6071, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 32.65625\n",
      "loss是： tensor(79.4068, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 21.449275362318843\n",
      "loss是： tensor(61.7874, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 32.74193548387097\n",
      "loss是： tensor(83.6806, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 36.57534246575342\n",
      "loss是： tensor(117.1238, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 30.0\n",
      "loss是： tensor(72.7751, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 26.129032258064516\n",
      "loss是： tensor(69.7418, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 27.333333333333336\n",
      "loss是： tensor(87.2694, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 22.884615384615383\n",
      "loss是： tensor(62.5512, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 31.25\n",
      "loss是： tensor(102.3274, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 34.82142857142857\n",
      "loss是： tensor(78.7212, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 33.225806451612904\n",
      "loss是： tensor(101.0584, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 43.33333333333333\n",
      "loss是： tensor(139.5734, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 27.241379310344826\n",
      "loss是： tensor(87.0250, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 24.776119402985074\n",
      "loss是： tensor(75.1058, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 35.51724137931035\n",
      "loss是： tensor(68.0546, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 35.97222222222222\n",
      "loss是： tensor(104.2184, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 39.27272727272727\n",
      "loss是： tensor(89.0339, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 31.147540983606557\n",
      "loss是： tensor(87.6846, device='cuda:0', grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert计算的loss是： 24.84375\n",
      "loss是： tensor(84.5751, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 33.77358490566038\n",
      "loss是： tensor(80.9170, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 29.863013698630137\n",
      "loss是： tensor(88.4292, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 41.01694915254237\n",
      "loss是： tensor(80.8635, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 44.333333333333336\n",
      "loss是： tensor(87.0586, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 58.75\n",
      "loss是： tensor(110.1001, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 28.76923076923077\n",
      "loss是： tensor(72.3922, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 40.81081081081081\n",
      "loss是： tensor(113.5117, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 22.741935483870968\n",
      "loss是： tensor(95.5071, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 32.131147540983605\n",
      "loss是： tensor(95.9498, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 29.833333333333336\n",
      "loss是： tensor(80.0820, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 33.714285714285715\n",
      "loss是： tensor(98.8953, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 23.448275862068964\n",
      "loss是： tensor(60.5708, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 23.61111111111111\n",
      "loss是： tensor(64.4866, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 44.67741935483871\n",
      "loss是： tensor(110.1445, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 32.62295081967213\n",
      "loss是： tensor(93.0037, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 38.69565217391304\n",
      "loss是： tensor(107.2867, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 4.0625\n",
      "loss是： tensor(71.3121, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 28.484848484848484\n",
      "loss是： tensor(96.4532, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 40.87719298245614\n",
      "loss是： tensor(127.7943, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 53.392857142857146\n",
      "loss是： tensor(86.3860, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 25.07936507936508\n",
      "loss是： tensor(90.2510, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 31.694915254237287\n",
      "loss是： tensor(85.8536, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 37.79661016949153\n",
      "loss是： tensor(82.8249, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 37.57142857142857\n",
      "loss是： tensor(91.4731, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 30.8955223880597\n",
      "loss是： tensor(121.8906, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 32.53731343283582\n",
      "loss是： tensor(102.5797, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 22.295081967213115\n",
      "loss是： tensor(84.3165, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 35.53846153846153\n",
      "loss是： tensor(78.6574, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 30.9375\n",
      "loss是： tensor(93.9975, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 29.58904109589041\n",
      "loss是： tensor(75.3367, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 30.8\n",
      "loss是： tensor(76.6955, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 30.17241379310345\n",
      "loss是： tensor(99.6237, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 28.615384615384617\n",
      "loss是： tensor(87.1970, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 34.75409836065574\n",
      "loss是： tensor(74.2786, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 20.35087719298246\n",
      "loss是： tensor(54.9246, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 32.04545454545455\n",
      "loss是： tensor(74.9440, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 57.301587301587304\n",
      "loss是： tensor(100.8813, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 39.464285714285715\n",
      "loss是： tensor(90.0937, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 27.868852459016395\n",
      "loss是： tensor(76.8413, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 32.096774193548384\n",
      "loss是： tensor(104.4952, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 29.83050847457627\n",
      "loss是： tensor(91.7845, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 38.793103448275865\n",
      "loss是： tensor(101.8874, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 26.885245901639344\n",
      "loss是： tensor(88.6033, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 30.0\n",
      "loss是： tensor(79.3260, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 28.484848484848484\n",
      "loss是： tensor(96.2320, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 18.5\n",
      "loss是： tensor(74.0387, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 40.303030303030305\n",
      "loss是： tensor(102.9684, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 28.636363636363637\n",
      "loss是： tensor(70.9720, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 7.704918032786885\n",
      "loss是： tensor(56.7078, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 8.88888888888889\n",
      "loss是： tensor(68.6195, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 24.63768115942029\n",
      "loss是： tensor(90.2337, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 36.48148148148148\n",
      "loss是： tensor(89.8919, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 37.41935483870967\n",
      "loss是： tensor(87.5484, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 38.656716417910445\n",
      "loss是： tensor(110.8470, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 30.8955223880597\n",
      "loss是： tensor(85.7157, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 41.940298507462686\n",
      "loss是： tensor(102.1424, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 44.06779661016949\n",
      "loss是： tensor(109.5604, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 30.909090909090907\n",
      "loss是： tensor(87.7749, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 23.42857142857143\n",
      "loss是： tensor(54.6208, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 23.384615384615387\n",
      "loss是： tensor(82.3819, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 37.361111111111114\n",
      "loss是： tensor(101.9863, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 38.9041095890411\n",
      "loss是： tensor(96.7879, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 28.07017543859649\n",
      "loss是： tensor(69.3519, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 32.20338983050847\n",
      "loss是： tensor(105.1315, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 26.785714285714285\n",
      "loss是： tensor(75.4689, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 26.93548387096774\n",
      "loss是： tensor(85.2209, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 40.72727272727272\n",
      "loss是： tensor(107.9742, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 58.52459016393443\n",
      "loss是： tensor(148.1425, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 35.88235294117647\n",
      "loss是： tensor(96.3721, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 34.861111111111114\n",
      "loss是： tensor(116.5795, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 39.67213114754098\n",
      "loss是： tensor(104.2176, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 32.94117647058823\n",
      "loss是： tensor(81.1622, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 37.53623188405797\n",
      "loss是： tensor(94.7481, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 28.461538461538463\n",
      "loss是： tensor(87.4246, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 27.096774193548384\n",
      "loss是： tensor(93.0564, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 34.375\n",
      "loss是： tensor(71.7733, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 27.28813559322034\n",
      "loss是： tensor(110.1258, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 38.59154929577465\n",
      "loss是： tensor(101.4465, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 36.36363636363636\n",
      "loss是： tensor(86.4442, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 26.875\n",
      "loss是： tensor(86.9762, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 30.338983050847457\n",
      "loss是： tensor(79.0209, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 32.7027027027027\n",
      "loss是： tensor(68.0828, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 58.95522388059702\n",
      "loss是： tensor(140.0856, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 27.8125\n",
      "loss是： tensor(107.0009, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 35.072463768115945\n",
      "loss是： tensor(100.7189, device='cuda:0', grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert计算的loss是： 41.29032258064516\n",
      "loss是： tensor(107.0585, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 29.607843137254903\n",
      "loss是： tensor(68.8741, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 27.903225806451616\n",
      "loss是： tensor(100.4273, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 47.69230769230769\n",
      "loss是： tensor(106.0842, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 26.96969696969697\n",
      "loss是： tensor(79.0135, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 23.384615384615387\n",
      "loss是： tensor(95.2275, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 22.3943661971831\n",
      "loss是： tensor(72.6778, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 26.166666666666668\n",
      "loss是： tensor(86.6876, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 41.78571428571429\n",
      "loss是： tensor(81.1689, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 33.114754098360656\n",
      "loss是： tensor(87.1598, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 31.384615384615383\n",
      "loss是： tensor(71.0371, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 25.57377049180328\n",
      "loss是： tensor(85.5284, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 39.84615384615385\n",
      "loss是： tensor(101.1189, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 34.75609756097561\n",
      "loss是： tensor(104.0215, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 29.82456140350877\n",
      "loss是： tensor(71.2837, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 32.45283018867924\n",
      "loss是： tensor(78.1922, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 37.01492537313433\n",
      "loss是： tensor(93.0166, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 41.61290322580645\n",
      "loss是： tensor(87.5714, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 25.636363636363633\n",
      "loss是： tensor(79.6429, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 22.058823529411768\n",
      "loss是： tensor(47.2578, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 38.507462686567166\n",
      "loss是： tensor(101.6384, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 30.151515151515152\n",
      "loss是： tensor(85.3693, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 42.22222222222222\n",
      "loss是： tensor(98.1555, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 7.727272727272727\n",
      "loss是： tensor(69.9388, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 16.296296296296294\n",
      "loss是： tensor(50.9371, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 26.557377049180324\n",
      "loss是： tensor(73.3553, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 56.45161290322581\n",
      "loss是： tensor(116.8164, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 6.851851851851852\n",
      "loss是： tensor(42.7584, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 40.3125\n",
      "loss是： tensor(112.2542, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 30.428571428571427\n",
      "loss是： tensor(88.6263, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 31.904761904761905\n",
      "loss是： tensor(70.9526, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 9.230769230769232\n",
      "loss是： tensor(69.3016, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 26.557377049180324\n",
      "loss是： tensor(81.0858, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 22.830188679245285\n",
      "loss是： tensor(57.6869, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 30.535714285714285\n",
      "loss是： tensor(95.0136, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 31.186440677966104\n",
      "loss是： tensor(79.6960, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 31.03448275862069\n",
      "loss是： tensor(87.1273, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 26.46153846153846\n",
      "loss是： tensor(73.8604, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 31.666666666666664\n",
      "loss是： tensor(85.1161, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 30.317460317460316\n",
      "loss是： tensor(76.2642, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 42.08955223880597\n",
      "loss是： tensor(97.4435, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 22.933333333333334\n",
      "loss是： tensor(69.3655, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 38.47826086956522\n",
      "loss是： tensor(67.0237, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 1.8055555555555556\n",
      "loss是： tensor(47.0806, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 22.295081967213115\n",
      "loss是： tensor(90.1168, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 38.24561403508772\n",
      "loss是： tensor(95.2330, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 41.9672131147541\n",
      "loss是： tensor(91.4404, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 28.095238095238095\n",
      "loss是： tensor(90.6372, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 28.75\n",
      "loss是： tensor(86.5713, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 74.03508771929825\n",
      "loss是： tensor(132.0659, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 28.30769230769231\n",
      "loss是： tensor(94.1557, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 29.384615384615387\n",
      "loss是： tensor(68.5014, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 29.48275862068965\n",
      "loss是： tensor(85.2803, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 24.078947368421055\n",
      "loss是： tensor(71.0912, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 44.61538461538461\n",
      "loss是： tensor(91.9022, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 31.23076923076923\n",
      "loss是： tensor(74.9011, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 34.10958904109589\n",
      "loss是： tensor(95.8802, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 31.967213114754102\n",
      "loss是： tensor(75.1374, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 29.841269841269842\n",
      "loss是： tensor(82.1960, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 10.491803278688526\n",
      "loss是： tensor(36.2211, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 26.923076923076927\n",
      "loss是： tensor(90.9700, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 46.166666666666664\n",
      "loss是： tensor(113.5644, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 37.22222222222222\n",
      "loss是： tensor(87.6804, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 32.465753424657535\n",
      "loss是： tensor(97.3342, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 27.142857142857146\n",
      "loss是： tensor(77.3780, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 54.358974358974365\n",
      "loss是： tensor(87.1335, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "270/300\n",
      "bert计算的loss是： 35.33333333333333\n",
      "loss是： tensor(77.8717, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 34.44444444444444\n",
      "loss是： tensor(98.4667, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 24.153846153846153\n",
      "loss是： tensor(81.6260, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 36.06060606060606\n",
      "loss是： tensor(120.9732, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 30.0\n",
      "loss是： tensor(80.7156, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 47.733333333333334\n",
      "loss是： tensor(113.4261, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 35.63636363636363\n",
      "loss是： tensor(76.4709, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 25.272727272727273\n",
      "loss是： tensor(71.5872, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 15.789473684210527\n",
      "loss是： tensor(67.5587, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 33.442622950819676\n",
      "loss是： tensor(78.1359, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 34.40677966101695\n",
      "loss是： tensor(67.0576, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 35.51724137931035\n",
      "loss是： tensor(91.6434, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 34.15384615384615\n",
      "loss是： tensor(81.7504, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 39.25373134328358\n",
      "loss是： tensor(95.0434, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 36.83333333333333\n",
      "loss是： tensor(102.0900, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 24.92753623188406\n",
      "loss是： tensor(75.0673, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 32.06349206349206\n",
      "loss是： tensor(85.5717, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 29.18032786885246\n",
      "loss是： tensor(95.1273, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 38.028169014084504\n",
      "loss是： tensor(89.4624, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 25.0\n",
      "loss是： tensor(70.5863, device='cuda:0', grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert计算的loss是： 40.0\n",
      "loss是： tensor(131.3324, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 30.714285714285715\n",
      "loss是： tensor(81.4315, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 20.444444444444443\n",
      "loss是： tensor(55.6927, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 25.671641791044774\n",
      "loss是： tensor(61.3347, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 27.0\n",
      "loss是： tensor(65.5840, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 52.03125\n",
      "loss是： tensor(94.3024, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 16.91176470588235\n",
      "loss是： tensor(74.5481, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 29.285714285714285\n",
      "loss是： tensor(70.8482, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 42.32876712328767\n",
      "loss是： tensor(109.7911, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 27.241379310344826\n",
      "loss是： tensor(78.7156, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 40.370370370370374\n",
      "loss是： tensor(123.7680, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 31.5625\n",
      "loss是： tensor(79.4444, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 28.405797101449277\n",
      "loss是： tensor(114.8344, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 32.83018867924528\n",
      "loss是： tensor(92.9021, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 33.22033898305085\n",
      "loss是： tensor(75.3655, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 26.779661016949152\n",
      "loss是： tensor(71.3795, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 39.310344827586206\n",
      "loss是： tensor(81.8535, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 29.571428571428573\n",
      "loss是： tensor(96.6125, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 28.47457627118644\n",
      "loss是： tensor(85.5732, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 34.09090909090909\n",
      "loss是： tensor(97.8337, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 30.3125\n",
      "loss是： tensor(89.6478, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 29.402985074626862\n",
      "loss是： tensor(97.8997, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 36.857142857142854\n",
      "loss是： tensor(118.0120, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 17.586206896551722\n",
      "loss是： tensor(69.6508, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 43.833333333333336\n",
      "loss是： tensor(113.0238, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 46.97674418604652\n",
      "loss是： tensor(86.3367, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 41.29032258064516\n",
      "loss是： tensor(112.5174, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 11.846153846153847\n",
      "loss是： tensor(73.3622, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 33.54838709677419\n",
      "loss是： tensor(101.8952, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 38.36065573770492\n",
      "loss是： tensor(118.3068, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 27.91044776119403\n",
      "loss是： tensor(109.8483, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 29.63636363636364\n",
      "loss是： tensor(110.4552, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 29.705882352941178\n",
      "loss是： tensor(79.5302, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 36.5\n",
      "loss是： tensor(83.7623, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 27.246376811594203\n",
      "loss是： tensor(92.9264, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 30.625\n",
      "loss是： tensor(86.9571, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 31.555555555555554\n",
      "loss是： tensor(88.1624, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 30.545454545454547\n",
      "loss是： tensor(66.7877, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 40.298507462686565\n",
      "loss是： tensor(91.6242, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 37.868852459016395\n",
      "loss是： tensor(83.8714, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 40.625\n",
      "loss是： tensor(82.3483, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 40.476190476190474\n",
      "loss是： tensor(100.0289, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 31.791044776119403\n",
      "loss是： tensor(87.4244, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 29.696969696969695\n",
      "loss是： tensor(64.8427, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 36.19047619047619\n",
      "loss是： tensor(98.1273, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 30.793650793650794\n",
      "loss是： tensor(85.4356, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 4.0625\n",
      "loss是： tensor(72.1621, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 34.30769230769231\n",
      "loss是： tensor(93.8332, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 34.0\n",
      "loss是： tensor(81.1223, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 29.090909090909093\n",
      "loss是： tensor(79.7834, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 29.285714285714285\n",
      "loss是： tensor(79.7335, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 27.936507936507937\n",
      "loss是： tensor(65.2668, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 33.65079365079365\n",
      "loss是： tensor(85.6575, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 22.258064516129032\n",
      "loss是： tensor(77.5150, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 24.92063492063492\n",
      "loss是： tensor(71.5664, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 51.492537313432834\n",
      "loss是： tensor(123.0212, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 36.34920634920635\n",
      "loss是： tensor(111.4267, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 32.0\n",
      "loss是： tensor(96.3999, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 50.8955223880597\n",
      "loss是： tensor(110.4812, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 38.833333333333336\n",
      "loss是： tensor(96.6659, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 41.42857142857143\n",
      "loss是： tensor(88.6505, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 27.419354838709676\n",
      "loss是： tensor(59.9749, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 25.9375\n",
      "loss是： tensor(96.0504, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 28.529411764705884\n",
      "loss是： tensor(79.8232, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 36.666666666666664\n",
      "loss是： tensor(96.6875, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 25.9375\n",
      "loss是： tensor(104.8163, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 39.047619047619044\n",
      "loss是： tensor(95.9995, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 16.507936507936506\n",
      "loss是： tensor(67.4285, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 36.2962962962963\n",
      "loss是： tensor(105.9005, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 32.57575757575758\n",
      "loss是： tensor(71.2626, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 31.636363636363637\n",
      "loss是： tensor(101.0633, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 23.75\n",
      "loss是： tensor(62.1786, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 40.42253521126761\n",
      "loss是： tensor(101.1113, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 32.76923076923077\n",
      "loss是： tensor(77.0947, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 39.25925925925926\n",
      "loss是： tensor(92.0899, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 11.384615384615383\n",
      "loss是： tensor(72.6716, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 29.545454545454547\n",
      "loss是： tensor(77.9303, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 34.0983606557377\n",
      "loss是： tensor(76.7007, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 35.892857142857146\n",
      "loss是： tensor(85.0138, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 31.52542372881356\n",
      "loss是： tensor(59.4377, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 72.1311475409836\n",
      "loss是： tensor(121.7147, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 26.440677966101696\n",
      "loss是： tensor(75.4452, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 33.484848484848484\n",
      "loss是： tensor(79.5500, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 40.625\n",
      "loss是： tensor(107.3196, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 8.307692307692308\n",
      "loss是： tensor(59.0403, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 39.07692307692307\n",
      "loss是： tensor(103.1431, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 31.52542372881356\n",
      "loss是： tensor(66.3897, device='cuda:0', grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert计算的loss是： 22.8125\n",
      "loss是： tensor(98.5099, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 29.18032786885246\n",
      "loss是： tensor(71.1797, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 23.53846153846154\n",
      "loss是： tensor(97.3791, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 28.88888888888889\n",
      "loss是： tensor(83.0420, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 29.242424242424242\n",
      "loss是： tensor(78.2069, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 24.23728813559322\n",
      "loss是： tensor(94.1452, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 37.80821917808219\n",
      "loss是： tensor(108.7145, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 26.363636363636363\n",
      "loss是： tensor(54.0752, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 39.14285714285714\n",
      "loss是： tensor(112.4799, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 30.454545454545453\n",
      "loss是： tensor(88.8989, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 30.821917808219176\n",
      "loss是： tensor(103.7783, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 27.1875\n",
      "loss是： tensor(90.4553, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 29.130434782608695\n",
      "loss是： tensor(82.3478, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 3.0303030303030303\n",
      "loss是： tensor(50.4882, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 33.278688524590166\n",
      "loss是： tensor(85.1353, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 24.814814814814813\n",
      "loss是： tensor(59.3155, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 38.84615384615385\n",
      "loss是： tensor(88.2847, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 7.34375\n",
      "loss是： tensor(56.1964, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 23.0\n",
      "loss是： tensor(51.5097, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 55.151515151515156\n",
      "loss是： tensor(118.9617, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 37.704918032786885\n",
      "loss是： tensor(97.2467, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 31.29032258064516\n",
      "loss是： tensor(93.7685, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 36.515151515151516\n",
      "loss是： tensor(85.2973, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 31.458333333333336\n",
      "loss是： tensor(74.8585, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 13.880597014925373\n",
      "loss是： tensor(50.0680, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 35.0\n",
      "loss是： tensor(74.4311, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 31.578947368421055\n",
      "loss是： tensor(91.9864, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 35.90909090909091\n",
      "loss是： tensor(79.3924, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 33.15789473684211\n",
      "loss是： tensor(104.2995, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 35.38461538461539\n",
      "loss是： tensor(109.9931, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 33.94736842105263\n",
      "loss是： tensor(73.1785, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 31.833333333333332\n",
      "loss是： tensor(101.1318, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 33.18181818181819\n",
      "loss是： tensor(96.5750, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 33.088235294117645\n",
      "loss是： tensor(104.0846, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 31.451612903225804\n",
      "loss是： tensor(80.6225, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 39.09090909090909\n",
      "loss是： tensor(84.7665, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 28.64864864864865\n",
      "loss是： tensor(88.5837, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 48.52459016393443\n",
      "loss是： tensor(108.3233, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 20.425531914893615\n",
      "loss是： tensor(55.2645, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 8.813559322033898\n",
      "loss是： tensor(54.9019, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 32.5\n",
      "loss是： tensor(78.8986, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 44.23728813559322\n",
      "loss是： tensor(104.2503, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 35.97014925373134\n",
      "loss是： tensor(69.8396, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 29.333333333333332\n",
      "loss是： tensor(74.2723, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 25.245901639344265\n",
      "loss是： tensor(71.4974, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 3.9622641509433967\n",
      "loss是： tensor(70.3036, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 42.08955223880597\n",
      "loss是： tensor(125.3477, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 32.41379310344827\n",
      "loss是： tensor(75.8832, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 22.253521126760564\n",
      "loss是： tensor(93.7855, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 32.758620689655174\n",
      "loss是： tensor(62.4674, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 30.163934426229506\n",
      "loss是： tensor(89.8628, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 34.927536231884055\n",
      "loss是： tensor(100.8945, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 33.38028169014085\n",
      "loss是： tensor(98.9653, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 34.8936170212766\n",
      "loss是： tensor(86.3458, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 34.657534246575345\n",
      "loss是： tensor(114.9616, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 38.611111111111114\n",
      "loss是： tensor(138.9160, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 42.16666666666667\n",
      "loss是： tensor(99.9949, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 34.12698412698413\n",
      "loss是： tensor(88.1142, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 30.833333333333336\n",
      "loss是： tensor(84.7015, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 32.63157894736842\n",
      "loss是： tensor(84.5786, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 36.271186440677965\n",
      "loss是： tensor(106.5788, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 27.352941176470587\n",
      "loss是： tensor(70.5715, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 26.984126984126988\n",
      "loss是： tensor(84.9249, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 38.775510204081634\n",
      "loss是： tensor(70.9487, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 32.950819672131146\n",
      "loss是： tensor(80.8569, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 33.06666666666666\n",
      "loss是： tensor(114.5809, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 34.166666666666664\n",
      "loss是： tensor(68.2416, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 25.07936507936508\n",
      "loss是： tensor(96.0241, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 27.230769230769234\n",
      "loss是： tensor(76.2406, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 25.76271186440678\n",
      "loss是： tensor(81.1673, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 26.721311475409838\n",
      "loss是： tensor(76.9928, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 32.5\n",
      "loss是： tensor(80.5356, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 33.225806451612904\n",
      "loss是： tensor(83.7229, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 37.03125\n",
      "loss是： tensor(88.8215, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 41.35593220338983\n",
      "loss是： tensor(94.2113, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 35.55555555555556\n",
      "loss是： tensor(81.7320, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 82.0\n",
      "loss是： tensor(133.1042, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 35.172413793103445\n",
      "loss是： tensor(116.9143, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 20.689655172413794\n",
      "loss是： tensor(102.4995, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 16.09375\n",
      "loss是： tensor(48.2664, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 44.117647058823536\n",
      "loss是： tensor(111.9431, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 37.07692307692308\n",
      "loss是： tensor(101.8351, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 36.885245901639344\n",
      "loss是： tensor(98.1025, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 26.307692307692307\n",
      "loss是： tensor(86.2530, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 35.18518518518518\n",
      "loss是： tensor(99.9635, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 26.323529411764707\n",
      "loss是： tensor(65.2371, device='cuda:0', grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert计算的loss是： 26.43835616438356\n",
      "loss是： tensor(66.7896, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 30.28985507246377\n",
      "loss是： tensor(79.9704, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 34.90909090909091\n",
      "loss是： tensor(107.5672, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 33.38709677419355\n",
      "loss是： tensor(118.6894, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 24.262295081967213\n",
      "loss是： tensor(69.3778, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 38.857142857142854\n",
      "loss是： tensor(92.3169, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 49.28571428571429\n",
      "loss是： tensor(91.4108, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 42.03389830508475\n",
      "loss是： tensor(102.1091, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 35.60606060606061\n",
      "loss是： tensor(74.7638, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 36.0\n",
      "loss是： tensor(110.4153, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 48.833333333333336\n",
      "loss是： tensor(97.7148, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 24.6875\n",
      "loss是： tensor(58.9969, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 31.538461538461537\n",
      "loss是： tensor(75.9926, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 25.945945945945947\n",
      "loss是： tensor(70.4472, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 24.629629629629626\n",
      "loss是： tensor(92.8438, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 37.03703703703704\n",
      "loss是： tensor(71.4772, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 29.70149253731343\n",
      "loss是： tensor(99.4964, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 24.285714285714285\n",
      "loss是： tensor(76.2050, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 38.656716417910445\n",
      "loss是： tensor(94.3767, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 36.03174603174603\n",
      "loss是： tensor(95.0407, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 35.0\n",
      "loss是： tensor(83.8725, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 33.01587301587301\n",
      "loss是： tensor(89.8505, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 29.420289855072465\n",
      "loss是： tensor(102.2554, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 29.444444444444446\n",
      "loss是： tensor(56.5215, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 8.28125\n",
      "loss是： tensor(84.3450, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 36.56716417910448\n",
      "loss是： tensor(91.8147, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 31.25\n",
      "loss是： tensor(92.5918, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 35.52238805970149\n",
      "loss是： tensor(92.2970, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 30.634920634920636\n",
      "loss是： tensor(74.5621, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 23.58490566037736\n",
      "loss是： tensor(67.2849, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 29.365079365079364\n",
      "loss是： tensor(84.0885, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 7.65625\n",
      "loss是： tensor(52.8162, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 23.974358974358974\n",
      "loss是： tensor(69.2087, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 41.09090909090909\n",
      "loss是： tensor(87.4275, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 41.07692307692307\n",
      "loss是： tensor(103.9507, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 32.916666666666664\n",
      "loss是： tensor(84.5887, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 47.5\n",
      "loss是： tensor(93.5725, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "280/300\n",
      "bert计算的loss是： 37.80821917808219\n",
      "loss是： tensor(108.0775, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 26.349206349206348\n",
      "loss是： tensor(78.8791, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 27.72727272727273\n",
      "loss是： tensor(95.8095, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 40.151515151515156\n",
      "loss是： tensor(90.2069, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 33.125\n",
      "loss是： tensor(87.3367, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 24.262295081967213\n",
      "loss是： tensor(79.9584, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 33.0188679245283\n",
      "loss是： tensor(95.9267, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 27.03125\n",
      "loss是： tensor(68.5942, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 23.220338983050848\n",
      "loss是： tensor(76.3500, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 35.24590163934426\n",
      "loss是： tensor(79.3367, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 31.044776119402986\n",
      "loss是： tensor(68.9441, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 40.81967213114754\n",
      "loss是： tensor(103.3854, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 56.610169491525426\n",
      "loss是： tensor(119.0032, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 33.389830508474574\n",
      "loss是： tensor(86.0790, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 30.757575757575758\n",
      "loss是： tensor(85.7270, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 21.428571428571427\n",
      "loss是： tensor(74.5485, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 26.25\n",
      "loss是： tensor(82.4332, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 34.02985074626866\n",
      "loss是： tensor(87.4713, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 37.27272727272727\n",
      "loss是： tensor(86.6419, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 23.833333333333332\n",
      "loss是： tensor(75.7926, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 46.56716417910448\n",
      "loss是： tensor(101.2322, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 4.333333333333334\n",
      "loss是： tensor(51.5018, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 37.42857142857143\n",
      "loss是： tensor(66.4836, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 46.18181818181818\n",
      "loss是： tensor(80.2908, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 32.950819672131146\n",
      "loss是： tensor(73.7796, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 21.343283582089555\n",
      "loss是： tensor(66.7200, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 26.065573770491802\n",
      "loss是： tensor(74.5186, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 21.967213114754102\n",
      "loss是： tensor(74.6913, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 56.271186440677965\n",
      "loss是： tensor(115.0339, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 16.89189189189189\n",
      "loss是： tensor(84.6506, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 34.074074074074076\n",
      "loss是： tensor(83.1324, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 42.71186440677966\n",
      "loss是： tensor(75.3262, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 19.67741935483871\n",
      "loss是： tensor(66.6927, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 31.0\n",
      "loss是： tensor(76.9894, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 34.48275862068965\n",
      "loss是： tensor(100.1848, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 36.0655737704918\n",
      "loss是： tensor(83.6204, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 7.352941176470589\n",
      "loss是： tensor(46.7013, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 41.38461538461539\n",
      "loss是： tensor(87.4942, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 17.460317460317462\n",
      "loss是： tensor(75.0452, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 38.0\n",
      "loss是： tensor(92.8354, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 41.04477611940298\n",
      "loss是： tensor(105.3157, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 25.671641791044774\n",
      "loss是： tensor(91.6617, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 31.91780821917808\n",
      "loss是： tensor(92.0682, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 29.090909090909093\n",
      "loss是： tensor(91.7585, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 33.22033898305085\n",
      "loss是： tensor(84.3211, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 44.04761904761905\n",
      "loss是： tensor(84.7703, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 33.93939393939394\n",
      "loss是： tensor(84.8507, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 25.689655172413794\n",
      "loss是： tensor(67.5791, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 28.852459016393443\n",
      "loss是： tensor(84.1986, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 24.21875\n",
      "loss是： tensor(71.2948, device='cuda:0', grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert计算的loss是： 33.19444444444444\n",
      "loss是： tensor(70.1536, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 33.65384615384615\n",
      "loss是： tensor(81.8863, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 33.529411764705884\n",
      "loss是： tensor(88.3805, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 30.0\n",
      "loss是： tensor(68.0162, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 37.07692307692308\n",
      "loss是： tensor(103.9061, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 35.27272727272727\n",
      "loss是： tensor(101.3381, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 36.25\n",
      "loss是： tensor(90.6295, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 29.444444444444446\n",
      "loss是： tensor(76.6362, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 30.16949152542373\n",
      "loss是： tensor(60.2715, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 40.0\n",
      "loss是： tensor(137.0369, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 24.93150684931507\n",
      "loss是： tensor(81.4256, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 31.52542372881356\n",
      "loss是： tensor(83.7551, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 53.064516129032256\n",
      "loss是： tensor(98.5719, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 33.714285714285715\n",
      "loss是： tensor(88.3028, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 41.25\n",
      "loss是： tensor(88.4897, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 34.75409836065574\n",
      "loss是： tensor(92.3386, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 31.25\n",
      "loss是： tensor(83.9488, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 39.35483870967742\n",
      "loss是： tensor(79.6598, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 0.5128205128205128\n",
      "loss是： tensor(35.1352, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 22.90909090909091\n",
      "loss是： tensor(81.7119, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 28.78787878787879\n",
      "loss是： tensor(69.7482, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 40.0\n",
      "loss是： tensor(79.8645, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 66.32352941176471\n",
      "loss是： tensor(133.0882, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 9.464285714285714\n",
      "loss是： tensor(46.3555, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 39.57746478873239\n",
      "loss是： tensor(90.8535, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 26.666666666666664\n",
      "loss是： tensor(83.6732, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 30.142857142857142\n",
      "loss是： tensor(79.3577, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 23.78787878787879\n",
      "loss是： tensor(53.9260, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 28.5\n",
      "loss是： tensor(64.8991, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 32.0\n",
      "loss是： tensor(77.7590, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 33.833333333333336\n",
      "loss是： tensor(83.6383, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 25.737704918032787\n",
      "loss是： tensor(70.5888, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 31.818181818181817\n",
      "loss是： tensor(76.3641, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 29.67741935483871\n",
      "loss是： tensor(76.2496, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 11.486486486486488\n",
      "loss是： tensor(76.7318, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 34.285714285714285\n",
      "loss是： tensor(72.5601, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 36.029411764705884\n",
      "loss是： tensor(80.2806, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 41.206896551724135\n",
      "loss是： tensor(94.8907, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 34.34782608695652\n",
      "loss是： tensor(88.3103, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 40.175438596491226\n",
      "loss是： tensor(86.2194, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 47.142857142857146\n",
      "loss是： tensor(86.6376, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 27.0\n",
      "loss是： tensor(51.9349, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 35.15625\n",
      "loss是： tensor(80.7473, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 30.270270270270274\n",
      "loss是： tensor(103.2208, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 36.15384615384615\n",
      "loss是： tensor(72.3655, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 7.966101694915254\n",
      "loss是： tensor(56.4738, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 32.758620689655174\n",
      "loss是： tensor(76.6403, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 32.06896551724138\n",
      "loss是： tensor(90.2085, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 29.81818181818182\n",
      "loss是： tensor(76.7152, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 30.57142857142857\n",
      "loss是： tensor(103.2492, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 27.037037037037038\n",
      "loss是： tensor(77.9141, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 39.375\n",
      "loss是： tensor(109.7210, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 41.85714285714286\n",
      "loss是： tensor(102.3386, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 32.38095238095238\n",
      "loss是： tensor(71.0169, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 33.8235294117647\n",
      "loss是： tensor(115.5138, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 24.923076923076923\n",
      "loss是： tensor(73.2484, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 23.859649122807017\n",
      "loss是： tensor(53.9349, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 36.3013698630137\n",
      "loss是： tensor(93.9925, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 32.0\n",
      "loss是： tensor(98.2219, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 25.0\n",
      "loss是： tensor(89.4756, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 25.217391304347828\n",
      "loss是： tensor(96.0747, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 59.016393442622956\n",
      "loss是： tensor(123.1125, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 33.10344827586207\n",
      "loss是： tensor(89.7748, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 65.17241379310346\n",
      "loss是： tensor(118.3663, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 34.57142857142857\n",
      "loss是： tensor(57.3305, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 14.411764705882353\n",
      "loss是： tensor(70.7738, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 40.46875\n",
      "loss是： tensor(99.5413, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 35.26315789473684\n",
      "loss是： tensor(88.9861, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 32.5\n",
      "loss是： tensor(61.6716, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 7.647058823529411\n",
      "loss是： tensor(83.1128, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 51.14754098360656\n",
      "loss是： tensor(115.6471, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 32.59259259259259\n",
      "loss是： tensor(86.7640, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 30.78125\n",
      "loss是： tensor(71.4372, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 9.682539682539682\n",
      "loss是： tensor(52.7601, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 22.3728813559322\n",
      "loss是： tensor(48.5893, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 26.176470588235293\n",
      "loss是： tensor(94.2094, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 30.56338028169014\n",
      "loss是： tensor(78.7341, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 25.303030303030305\n",
      "loss是： tensor(56.8158, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 33.529411764705884\n",
      "loss是： tensor(98.9180, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 37.391304347826086\n",
      "loss是： tensor(94.8476, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 46.78571428571429\n",
      "loss是： tensor(114.9939, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 27.714285714285715\n",
      "loss是： tensor(78.9953, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 15.11111111111111\n",
      "loss是： tensor(63.8431, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 34.59016393442623\n",
      "loss是： tensor(98.3072, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 26.400000000000002\n",
      "loss是： tensor(98.9700, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 44.67741935483871\n",
      "loss是： tensor(111.9277, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 51.71875\n",
      "loss是： tensor(107.9711, device='cuda:0', grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert计算的loss是： 44.57142857142857\n",
      "loss是： tensor(85.7774, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 36.12903225806451\n",
      "loss是： tensor(96.1601, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 61.875\n",
      "loss是： tensor(118.0400, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 34.516129032258064\n",
      "loss是： tensor(72.1596, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 41.694915254237294\n",
      "loss是： tensor(82.6816, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 35.0\n",
      "loss是： tensor(104.2917, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 31.93548387096774\n",
      "loss是： tensor(76.8629, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 28.524590163934427\n",
      "loss是： tensor(71.7685, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 29.726027397260275\n",
      "loss是： tensor(79.4200, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 32.0\n",
      "loss是： tensor(85.7670, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 27.457627118644066\n",
      "loss是： tensor(62.1645, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 11.129032258064516\n",
      "loss是： tensor(74.8076, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 45.2542372881356\n",
      "loss是： tensor(97.7222, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 37.16417910447761\n",
      "loss是： tensor(86.8224, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 23.333333333333336\n",
      "loss是： tensor(93.3195, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 27.241379310344826\n",
      "loss是： tensor(66.3229, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 44.7887323943662\n",
      "loss是： tensor(94.9903, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 25.263157894736842\n",
      "loss是： tensor(73.6303, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 30.17543859649123\n",
      "loss是： tensor(69.0953, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 31.408450704225352\n",
      "loss是： tensor(97.3772, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 30.441176470588232\n",
      "loss是： tensor(97.8405, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 18.627450980392158\n",
      "loss是： tensor(79.1035, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 24.776119402985074\n",
      "loss是： tensor(85.1401, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 34.285714285714285\n",
      "loss是： tensor(64.7410, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 25.25423728813559\n",
      "loss是： tensor(67.4205, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 35.75757575757576\n",
      "loss是： tensor(78.5394, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 37.5\n",
      "loss是： tensor(99.0314, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 26.956521739130434\n",
      "loss是： tensor(70.2982, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 38.14814814814815\n",
      "loss是： tensor(94.3007, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 42.15384615384615\n",
      "loss是： tensor(102.0783, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 31.206896551724135\n",
      "loss是： tensor(105.3923, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 33.225806451612904\n",
      "loss是： tensor(81.6758, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 31.21212121212121\n",
      "loss是： tensor(74.7726, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 32.857142857142854\n",
      "loss是： tensor(61.8046, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 32.94117647058823\n",
      "loss是： tensor(85.7440, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 28.78787878787879\n",
      "loss是： tensor(82.1788, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 31.323529411764707\n",
      "loss是： tensor(84.7231, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 57.924528301886795\n",
      "loss是： tensor(126.9008, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 26.923076923076927\n",
      "loss是： tensor(71.3850, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 37.666666666666664\n",
      "loss是： tensor(81.8465, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 27.741935483870968\n",
      "loss是： tensor(80.1864, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 32.131147540983605\n",
      "loss是： tensor(108.5423, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 37.049180327868854\n",
      "loss是： tensor(104.1869, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 26.071428571428573\n",
      "loss是： tensor(80.6817, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 43.484848484848484\n",
      "loss是： tensor(106.7298, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 28.507462686567166\n",
      "loss是： tensor(95.7611, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 28.571428571428573\n",
      "loss是： tensor(51.5653, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 28.166666666666668\n",
      "loss是： tensor(86.8243, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 32.15384615384615\n",
      "loss是： tensor(88.1267, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 5.774647887323944\n",
      "loss是： tensor(51.5498, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 40.5\n",
      "loss是： tensor(79.4768, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 21.228070175438596\n",
      "loss是： tensor(60.0856, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 31.176470588235293\n",
      "loss是： tensor(85.8792, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 32.53731343283582\n",
      "loss是： tensor(95.6151, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 42.333333333333336\n",
      "loss是： tensor(82.0629, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 41.25\n",
      "loss是： tensor(87.6512, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 39.66666666666667\n",
      "loss是： tensor(95.5119, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 17.818181818181817\n",
      "loss是： tensor(57.3641, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 34.11764705882353\n",
      "loss是： tensor(80.0685, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 46.333333333333336\n",
      "loss是： tensor(92.5122, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 60.70175438596492\n",
      "loss是： tensor(120.3676, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 39.55882352941177\n",
      "loss是： tensor(101.0733, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 13.278688524590162\n",
      "loss是： tensor(63.0694, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 27.16417910447761\n",
      "loss是： tensor(77.3491, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 27.321428571428573\n",
      "loss是： tensor(58.4785, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 42.5\n",
      "loss是： tensor(101.7221, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 28.524590163934427\n",
      "loss是： tensor(73.5187, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 31.818181818181817\n",
      "loss是： tensor(101.6220, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 31.73913043478261\n",
      "loss是： tensor(80.2496, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 27.89473684210526\n",
      "loss是： tensor(50.6301, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 39.152542372881356\n",
      "loss是： tensor(80.4437, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 24.35483870967742\n",
      "loss是： tensor(72.1772, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 29.5\n",
      "loss是： tensor(63.9345, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 30.634920634920636\n",
      "loss是： tensor(96.7868, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 36.865671641791046\n",
      "loss是： tensor(87.2019, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 34.76190476190476\n",
      "loss是： tensor(75.0494, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 37.567567567567565\n",
      "loss是： tensor(82.9202, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 35.38461538461539\n",
      "loss是： tensor(84.5933, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 33.333333333333336\n",
      "loss是： tensor(67.7791, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 24.814814814814813\n",
      "loss是： tensor(60.6383, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 41.74603174603175\n",
      "loss是： tensor(104.6179, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 11.09375\n",
      "loss是： tensor(68.3750, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 29.45945945945946\n",
      "loss是： tensor(85.2698, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 54.17910447761194\n",
      "loss是： tensor(99.5842, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 34.920634920634924\n",
      "loss是： tensor(102.9120, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 28.11320754716981\n",
      "loss是： tensor(71.3455, device='cuda:0', grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert计算的loss是： 53.38461538461538\n",
      "loss是： tensor(101.5175, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 35.3448275862069\n",
      "loss是： tensor(107.2907, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 22.741935483870968\n",
      "loss是： tensor(67.8871, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 31.228070175438596\n",
      "loss是： tensor(102.7816, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 28.03030303030303\n",
      "loss是： tensor(90.1128, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 40.51724137931035\n",
      "loss是： tensor(90.9072, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 31.62162162162162\n",
      "loss是： tensor(81.1673, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "290/300\n",
      "bert计算的loss是： 35.072463768115945\n",
      "loss是： tensor(125.6829, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 28.73015873015873\n",
      "loss是： tensor(74.4654, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 31.142857142857142\n",
      "loss是： tensor(95.4004, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 42.10526315789473\n",
      "loss是： tensor(80.0241, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 25.942028985507246\n",
      "loss是： tensor(107.6901, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 30.73529411764706\n",
      "loss是： tensor(79.5961, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 31.076923076923077\n",
      "loss是： tensor(86.9735, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 25.0\n",
      "loss是： tensor(69.0895, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 40.30769230769231\n",
      "loss是： tensor(98.8230, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 26.90909090909091\n",
      "loss是： tensor(79.7293, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 33.275862068965516\n",
      "loss是： tensor(98.2737, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 28.867924528301888\n",
      "loss是： tensor(68.7958, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 27.230769230769234\n",
      "loss是： tensor(86.0159, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 36.029411764705884\n",
      "loss是： tensor(96.6801, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 32.98245614035088\n",
      "loss是： tensor(77.9257, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 34.05797101449275\n",
      "loss是： tensor(89.6027, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 22.65625\n",
      "loss是： tensor(99.8876, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 36.721311475409834\n",
      "loss是： tensor(74.5343, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 43.49206349206349\n",
      "loss是： tensor(114.9639, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 40.50847457627118\n",
      "loss是： tensor(77.2865, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 29.285714285714285\n",
      "loss是： tensor(71.5648, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 50.0\n",
      "loss是： tensor(110.9141, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 32.0\n",
      "loss是： tensor(69.5612, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 65.59322033898306\n",
      "loss是： tensor(124.4738, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 21.475409836065573\n",
      "loss是： tensor(74.5441, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 38.46153846153846\n",
      "loss是： tensor(98.6146, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 27.833333333333332\n",
      "loss是： tensor(75.4355, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 47.5\n",
      "loss是： tensor(110.2151, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 28.80597014925373\n",
      "loss是： tensor(80.0254, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 37.05882352941177\n",
      "loss是： tensor(97.5072, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 34.230769230769226\n",
      "loss是： tensor(88.7740, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 41.83098591549296\n",
      "loss是： tensor(106.5031, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 8.208955223880597\n",
      "loss是： tensor(67.7817, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 30.17543859649123\n",
      "loss是： tensor(75.1129, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 28.08219178082192\n",
      "loss是： tensor(102.0471, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 33.164556962025316\n",
      "loss是： tensor(112.7420, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 6.923076923076923\n",
      "loss是： tensor(61.1777, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 24.137931034482758\n",
      "loss是： tensor(72.6114, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 23.846153846153847\n",
      "loss是： tensor(72.9920, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 40.14492753623188\n",
      "loss是： tensor(92.3052, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 40.75471698113208\n",
      "loss是： tensor(81.2743, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 29.253731343283583\n",
      "loss是： tensor(78.6250, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 32.45614035087719\n",
      "loss是： tensor(81.8132, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 37.06896551724138\n",
      "loss是： tensor(94.2164, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 28.076923076923073\n",
      "loss是： tensor(87.1105, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 35.14285714285714\n",
      "loss是： tensor(78.5345, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 14.375\n",
      "loss是： tensor(45.6774, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 24.155844155844157\n",
      "loss是： tensor(98.9133, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 33.134328358208954\n",
      "loss是： tensor(107.1419, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 32.0\n",
      "loss是： tensor(78.6037, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 49.03225806451613\n",
      "loss是： tensor(125.8498, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 22.78688524590164\n",
      "loss是： tensor(82.1177, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 24.18181818181818\n",
      "loss是： tensor(74.6086, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 40.73529411764706\n",
      "loss是： tensor(88.9588, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 37.84615384615385\n",
      "loss是： tensor(90.4302, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 35.08474576271186\n",
      "loss是： tensor(83.2644, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 41.78571428571429\n",
      "loss是： tensor(93.7455, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 39.69230769230769\n",
      "loss是： tensor(96.0111, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 37.599999999999994\n",
      "loss是： tensor(83.1422, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 26.285714285714285\n",
      "loss是： tensor(69.6806, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 28.923076923076923\n",
      "loss是： tensor(79.1115, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 33.888888888888886\n",
      "loss是： tensor(89.4026, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 30.98360655737705\n",
      "loss是： tensor(73.5109, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 27.301587301587304\n",
      "loss是： tensor(84.8807, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 26.923076923076927\n",
      "loss是： tensor(78.5196, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 38.62068965517241\n",
      "loss是： tensor(97.4032, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 33.87096774193549\n",
      "loss是： tensor(73.4948, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 64.16666666666667\n",
      "loss是： tensor(130.9294, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 15.555555555555555\n",
      "loss是： tensor(44.8482, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 25.76271186440678\n",
      "loss是： tensor(72.7990, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 29.642857142857146\n",
      "loss是： tensor(84.9681, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 30.307692307692307\n",
      "loss是： tensor(80.8928, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 40.625\n",
      "loss是： tensor(91.9465, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 1.9402985074626866\n",
      "loss是： tensor(78.3628, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 27.169811320754715\n",
      "loss是： tensor(75.5220, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 42.857142857142854\n",
      "loss是： tensor(98.1466, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 7.833333333333333\n",
      "loss是： tensor(66.4471, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 30.0\n",
      "loss是： tensor(74.3802, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 39.375\n",
      "loss是： tensor(103.7242, device='cuda:0', grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert计算的loss是： 28.73015873015873\n",
      "loss是： tensor(97.5685, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 26.0\n",
      "loss是： tensor(78.5110, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 39.31506849315069\n",
      "loss是： tensor(89.9778, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 40.18867924528302\n",
      "loss是： tensor(97.9669, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 22.72727272727273\n",
      "loss是： tensor(71.1551, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 28.28571428571429\n",
      "loss是： tensor(99.9525, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 28.11320754716981\n",
      "loss是： tensor(64.4610, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 34.375\n",
      "loss是： tensor(83.4759, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 39.0625\n",
      "loss是： tensor(90.2370, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 33.125\n",
      "loss是： tensor(74.9392, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 35.285714285714285\n",
      "loss是： tensor(90.3458, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 33.15789473684211\n",
      "loss是： tensor(74.6943, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 80.54054054054055\n",
      "loss是： tensor(111.2096, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 25.789473684210527\n",
      "loss是： tensor(84.3327, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 42.45283018867924\n",
      "loss是： tensor(89.8370, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 24.126984126984127\n",
      "loss是： tensor(81.4724, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 40.70422535211268\n",
      "loss是： tensor(102.8112, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 36.5\n",
      "loss是： tensor(92.5749, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 23.076923076923073\n",
      "loss是： tensor(70.7678, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 28.450704225352116\n",
      "loss是： tensor(67.3798, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 9.200000000000001\n",
      "loss是： tensor(67.5916, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 34.61538461538461\n",
      "loss是： tensor(105.2343, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 26.666666666666664\n",
      "loss是： tensor(59.1791, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 39.34426229508197\n",
      "loss是： tensor(93.0083, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 54.46153846153847\n",
      "loss是： tensor(102.2077, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 27.741935483870968\n",
      "loss是： tensor(87.3920, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 40.20833333333333\n",
      "loss是： tensor(83.2722, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 47.53846153846154\n",
      "loss是： tensor(120.5168, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 8.620689655172413\n",
      "loss是： tensor(68.9779, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 29.84375\n",
      "loss是： tensor(71.6928, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 34.85294117647059\n",
      "loss是： tensor(75.0157, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 25.43859649122807\n",
      "loss是： tensor(60.3308, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 28.06451612903226\n",
      "loss是： tensor(77.2047, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 38.225806451612904\n",
      "loss是： tensor(88.4647, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 24.59016393442623\n",
      "loss是： tensor(75.1656, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 34.78260869565217\n",
      "loss是： tensor(86.0135, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 24.615384615384617\n",
      "loss是： tensor(70.8719, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 35.17857142857143\n",
      "loss是： tensor(93.2195, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 32.333333333333336\n",
      "loss是： tensor(89.8537, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 28.636363636363637\n",
      "loss是： tensor(94.9496, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 34.12698412698413\n",
      "loss是： tensor(80.4916, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 37.40740740740741\n",
      "loss是： tensor(72.8324, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 37.32142857142857\n",
      "loss是： tensor(71.6582, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 38.813559322033896\n",
      "loss是： tensor(71.7749, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 29.58904109589041\n",
      "loss是： tensor(69.0450, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 22.77777777777778\n",
      "loss是： tensor(63.1061, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 38.47457627118644\n",
      "loss是： tensor(70.9257, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 26.71641791044776\n",
      "loss是： tensor(64.2547, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 34.66666666666667\n",
      "loss是： tensor(92.4975, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 28.088235294117645\n",
      "loss是： tensor(84.0954, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 43.53846153846153\n",
      "loss是： tensor(117.8265, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 31.333333333333332\n",
      "loss是： tensor(78.2240, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 30.491803278688522\n",
      "loss是： tensor(70.1741, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 29.661016949152543\n",
      "loss是： tensor(91.2702, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 30.746268656716417\n",
      "loss是： tensor(91.3217, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 61.78571428571429\n",
      "loss是： tensor(118.0967, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 42.38805970149254\n",
      "loss是： tensor(108.9909, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 36.666666666666664\n",
      "loss是： tensor(121.8327, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 28.571428571428573\n",
      "loss是： tensor(65.2850, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 39.54545454545455\n",
      "loss是： tensor(78.8154, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 26.065573770491802\n",
      "loss是： tensor(68.4490, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 20.51724137931035\n",
      "loss是： tensor(68.5365, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 34.107142857142854\n",
      "loss是： tensor(84.7859, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 45.0\n",
      "loss是： tensor(74.6013, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 35.42372881355932\n",
      "loss是： tensor(99.8372, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 33.59375\n",
      "loss是： tensor(122.3578, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 34.411764705882355\n",
      "loss是： tensor(99.7658, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 55.71428571428571\n",
      "loss是： tensor(111.7915, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 32.19178082191781\n",
      "loss是： tensor(90.4375, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 27.424242424242422\n",
      "loss是： tensor(69.9755, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 30.3125\n",
      "loss是： tensor(93.6277, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 32.857142857142854\n",
      "loss是： tensor(78.6242, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 49.824561403508774\n",
      "loss是： tensor(103.4949, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 22.903225806451616\n",
      "loss是： tensor(60.7578, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 30.967741935483872\n",
      "loss是： tensor(82.6208, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 28.5\n",
      "loss是： tensor(78.8886, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 30.294117647058822\n",
      "loss是： tensor(81.4038, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 26.911764705882355\n",
      "loss是： tensor(64.1758, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 33.96825396825397\n",
      "loss是： tensor(83.1611, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 26.53061224489796\n",
      "loss是： tensor(62.8753, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 34.6875\n",
      "loss是： tensor(113.9228, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 24.782608695652172\n",
      "loss是： tensor(51.4509, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 41.515151515151516\n",
      "loss是： tensor(91.9209, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 31.206896551724135\n",
      "loss是： tensor(81.5756, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 4.23728813559322\n",
      "loss是： tensor(51.2639, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 31.12676056338028\n",
      "loss是： tensor(66.8015, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 59.516129032258064\n",
      "loss是： tensor(119.8425, device='cuda:0', grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert计算的loss是： 23.571428571428573\n",
      "loss是： tensor(62.3730, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 28.852459016393443\n",
      "loss是： tensor(88.6499, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 28.484848484848484\n",
      "loss是： tensor(89.5650, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 45.19230769230769\n",
      "loss是： tensor(79.2830, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 32.30769230769231\n",
      "loss是： tensor(93.4094, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 36.785714285714285\n",
      "loss是： tensor(115.1703, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 30.46153846153846\n",
      "loss是： tensor(87.2502, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 27.424242424242422\n",
      "loss是： tensor(90.8827, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 31.071428571428573\n",
      "loss是： tensor(83.2434, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 42.25806451612903\n",
      "loss是： tensor(87.0009, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 40.76923076923077\n",
      "loss是： tensor(80.5040, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 51.42857142857143\n",
      "loss是： tensor(110.2811, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 31.80327868852459\n",
      "loss是： tensor(82.9540, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 25.245901639344265\n",
      "loss是： tensor(59.0702, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 27.580645161290324\n",
      "loss是： tensor(66.2869, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 28.80597014925373\n",
      "loss是： tensor(89.4495, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 23.606557377049178\n",
      "loss是： tensor(82.2678, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 35.64102564102564\n",
      "loss是： tensor(75.5186, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 28.095238095238095\n",
      "loss是： tensor(65.7213, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 17.936507936507937\n",
      "loss是： tensor(82.3101, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 28.333333333333336\n",
      "loss是： tensor(90.5487, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 69.54545454545455\n",
      "loss是： tensor(131.7891, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 45.4054054054054\n",
      "loss是： tensor(102.3991, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 27.59259259259259\n",
      "loss是： tensor(70.6404, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 45.73529411764706\n",
      "loss是： tensor(104.5518, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 30.32258064516129\n",
      "loss是： tensor(70.7742, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 34.91525423728814\n",
      "loss是： tensor(84.1635, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 25.862068965517242\n",
      "loss是： tensor(58.4671, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 25.166666666666664\n",
      "loss是： tensor(79.4125, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 32.46376811594203\n",
      "loss是： tensor(110.5965, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 32.22222222222222\n",
      "loss是： tensor(67.2876, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 26.507936507936506\n",
      "loss是： tensor(60.7640, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 40.1639344262295\n",
      "loss是： tensor(89.9963, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 43.92857142857143\n",
      "loss是： tensor(102.5138, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 28.793103448275865\n",
      "loss是： tensor(75.9731, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 31.379310344827584\n",
      "loss是： tensor(75.4297, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 35.072463768115945\n",
      "loss是： tensor(77.3914, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 28.732394366197184\n",
      "loss是： tensor(97.3101, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 24.328358208955226\n",
      "loss是： tensor(71.7467, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 21.35593220338983\n",
      "loss是： tensor(70.9708, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 39.72222222222222\n",
      "loss是： tensor(73.2880, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "正在训练第 5轮\n",
      "Model(\n",
      "  (gen_embedding): Embedding(8518, 300)\n",
      "  (conv1): Conv1d(300, 128, kernel_size=(5,), stride=(1,), padding=(2,))\n",
      "  (conv2): Conv1d(300, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "  (dropout): Dropout(p=0.55, inplace=False)\n",
      "  (conv3): Conv1d(256, 256, kernel_size=(5,), stride=(1,), padding=(2,))\n",
      "  (conv4): Conv1d(256, 256, kernel_size=(5,), stride=(1,), padding=(2,))\n",
      "  (conv5): Conv1d(256, 256, kernel_size=(5,), stride=(1,), padding=(2,))\n",
      "  (linear_ae): Linear(in_features=256, out_features=3, bias=True)\n",
      "  (crf): ConditionalRandomField()\n",
      ")\n",
      "0/300\n",
      "10/300\n",
      "20/300\n",
      "30/300\n",
      "40/300\n",
      "50/300\n",
      "60/300\n",
      "70/300\n",
      "80/300\n",
      "90/300\n",
      "100/300\n",
      "110/300\n",
      "120/300\n",
      "130/300\n",
      "140/300\n",
      "150/300\n",
      "160/300\n",
      "170/300\n",
      "180/300\n",
      "190/300\n",
      "200/300\n",
      "210/300\n",
      "220/300\n",
      "230/300\n",
      "240/300\n",
      "bert计算的loss是： 30.454545454545453\n",
      "loss是： tensor(110.1843, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 38.03030303030303\n",
      "loss是： tensor(114.3073, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 32.87878787878788\n",
      "loss是： tensor(98.5083, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 33.66197183098591\n",
      "loss是： tensor(119.6377, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 25.161290322580644\n",
      "loss是： tensor(78.4882, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 33.75\n",
      "loss是： tensor(77.1666, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 27.91044776119403\n",
      "loss是： tensor(91.0345, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 41.57142857142858\n",
      "loss是： tensor(133.4377, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 18.983050847457626\n",
      "loss是： tensor(97.5096, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 27.884615384615383\n",
      "loss是： tensor(98.2721, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 27.333333333333336\n",
      "loss是： tensor(91.1039, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 19.315068493150683\n",
      "loss是： tensor(81.7264, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 33.90625\n",
      "loss是： tensor(97.8105, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 37.07692307692308\n",
      "loss是： tensor(95.6045, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 27.28813559322034\n",
      "loss是： tensor(87.3100, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 35.07462686567165\n",
      "loss是： tensor(132.3517, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 27.01492537313433\n",
      "loss是： tensor(107.6764, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 42.15384615384615\n",
      "loss是： tensor(127.5852, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 38.666666666666664\n",
      "loss是： tensor(104.1113, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 1.0526315789473684\n",
      "loss是： tensor(84.2823, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 60.14705882352941\n",
      "loss是： tensor(130.0722, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 37.857142857142854\n",
      "loss是： tensor(89.7673, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 47.5\n",
      "loss是： tensor(101.4911, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "250/300\n",
      "bert计算的loss是： 37.41379310344827\n",
      "loss是： tensor(112.9108, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 29.473684210526315\n",
      "loss是： tensor(80.1850, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 41.73076923076923\n",
      "loss是： tensor(115.1187, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 2.1052631578947367\n",
      "loss是： tensor(68.6821, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 32.11267605633803\n",
      "loss是： tensor(105.3682, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 39.473684210526315\n",
      "loss是： tensor(135.8695, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 32.38805970149254\n",
      "loss是： tensor(104.3115, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 33.333333333333336\n",
      "loss是： tensor(112.0657, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 58.68852459016394\n",
      "loss是： tensor(158.5465, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 4.615384615384616\n",
      "loss是： tensor(82.5760, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 34.285714285714285\n",
      "loss是： tensor(92.0573, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 38.49315068493151\n",
      "loss是： tensor(92.7650, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 30.0\n",
      "loss是： tensor(108.1559, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 31.492537313432834\n",
      "loss是： tensor(110.2980, device='cuda:0', grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert计算的loss是： 30.9375\n",
      "loss是： tensor(107.9564, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 30.806451612903224\n",
      "loss是： tensor(100.8529, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 28.035714285714285\n",
      "loss是： tensor(84.9421, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 23.103448275862068\n",
      "loss是： tensor(81.4450, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 35.57142857142857\n",
      "loss是： tensor(123.4716, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 23.333333333333336\n",
      "loss是： tensor(67.1453, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 35.172413793103445\n",
      "loss是： tensor(82.4611, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 39.824561403508774\n",
      "loss是： tensor(139.4014, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 26.400000000000002\n",
      "loss是： tensor(97.3676, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 31.09090909090909\n",
      "loss是： tensor(101.7287, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 32.5\n",
      "loss是： tensor(111.9152, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 36.721311475409834\n",
      "loss是： tensor(106.9769, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 36.76470588235294\n",
      "loss是： tensor(97.6283, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 8.307692307692308\n",
      "loss是： tensor(67.1605, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 60.16129032258065\n",
      "loss是： tensor(125.6880, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 38.75\n",
      "loss是： tensor(108.6782, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 30.483870967741936\n",
      "loss是： tensor(115.0188, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 4.63768115942029\n",
      "loss是： tensor(91.8109, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 33.582089552238806\n",
      "loss是： tensor(107.1546, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 38.490566037735846\n",
      "loss是： tensor(107.7539, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 36.29032258064516\n",
      "loss是： tensor(102.4522, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 31.200000000000003\n",
      "loss是： tensor(107.7776, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 11.09375\n",
      "loss是： tensor(105.3406, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 29.242424242424242\n",
      "loss是： tensor(75.6402, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 10.0\n",
      "loss是： tensor(99.4357, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 28.541666666666664\n",
      "loss是： tensor(68.4934, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 34.107142857142854\n",
      "loss是： tensor(93.2827, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 47.12121212121212\n",
      "loss是： tensor(122.5704, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 21.25\n",
      "loss是： tensor(100.1725, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 35.230769230769226\n",
      "loss是： tensor(115.2611, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 36.901408450704224\n",
      "loss是： tensor(146.7328, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 32.44444444444444\n",
      "loss是： tensor(82.2517, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 29.696969696969695\n",
      "loss是： tensor(126.3258, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 28.412698412698415\n",
      "loss是： tensor(136.3850, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 29.82758620689655\n",
      "loss是： tensor(104.9545, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 27.16417910447761\n",
      "loss是： tensor(84.8473, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 33.285714285714285\n",
      "loss是： tensor(95.0446, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 39.24242424242424\n",
      "loss是： tensor(92.7767, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 56.94915254237288\n",
      "loss是： tensor(128.0040, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 35.96774193548387\n",
      "loss是： tensor(99.7498, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 41.7910447761194\n",
      "loss是： tensor(109.7821, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 39.523809523809526\n",
      "loss是： tensor(116.7188, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 26.06060606060606\n",
      "loss是： tensor(94.5100, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 34.21052631578947\n",
      "loss是： tensor(102.1075, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 41.372549019607845\n",
      "loss是： tensor(100.8018, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 25.492957746478876\n",
      "loss是： tensor(115.6085, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 26.338028169014084\n",
      "loss是： tensor(101.6161, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 30.625\n",
      "loss是： tensor(94.5173, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 51.66666666666667\n",
      "loss是： tensor(129.8017, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 40.75471698113208\n",
      "loss是： tensor(112.9116, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 26.666666666666664\n",
      "loss是： tensor(88.4056, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 45.71428571428571\n",
      "loss是： tensor(153.6437, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 24.35483870967742\n",
      "loss是： tensor(91.2126, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 39.833333333333336\n",
      "loss是： tensor(103.2789, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 28.409090909090907\n",
      "loss是： tensor(74.1334, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 51.11111111111111\n",
      "loss是： tensor(115.1263, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 25.82089552238806\n",
      "loss是： tensor(120.1615, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 37.46268656716418\n",
      "loss是： tensor(116.9917, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 26.774193548387096\n",
      "loss是： tensor(88.9234, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 36.666666666666664\n",
      "loss是： tensor(100.4317, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 34.6875\n",
      "loss是： tensor(137.9554, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 25.32258064516129\n",
      "loss是： tensor(87.3015, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 31.014492753623188\n",
      "loss是： tensor(92.1279, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 28.688524590163933\n",
      "loss是： tensor(107.6893, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 26.144578313253014\n",
      "loss是： tensor(106.1912, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 43.968253968253975\n",
      "loss是： tensor(97.8820, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 8.35820895522388\n",
      "loss是： tensor(79.3092, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 36.507936507936506\n",
      "loss是： tensor(99.3353, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 40.95238095238095\n",
      "loss是： tensor(135.5685, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 35.166666666666664\n",
      "loss是： tensor(92.6881, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 33.54838709677419\n",
      "loss是： tensor(98.7624, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 36.15384615384615\n",
      "loss是： tensor(118.6925, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 34.91803278688525\n",
      "loss是： tensor(119.7492, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 21.93548387096774\n",
      "loss是： tensor(81.2378, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 35.69230769230769\n",
      "loss是： tensor(78.1247, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 60.303030303030305\n",
      "loss是： tensor(144.3829, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 32.88461538461539\n",
      "loss是： tensor(86.1815, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 1.0810810810810811\n",
      "loss是： tensor(51.3377, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 0.8620689655172414\n",
      "loss是： tensor(50.3067, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 28.73015873015873\n",
      "loss是： tensor(76.6743, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 31.03448275862069\n",
      "loss是： tensor(69.4948, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 28.833333333333332\n",
      "loss是： tensor(103.9544, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 23.442622950819676\n",
      "loss是： tensor(85.4877, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 37.74193548387097\n",
      "loss是： tensor(87.9250, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 33.098591549295776\n",
      "loss是： tensor(125.4202, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 40.0\n",
      "loss是： tensor(95.6332, device='cuda:0', grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert计算的loss是： 37.22222222222222\n",
      "loss是： tensor(105.8970, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 56.25\n",
      "loss是： tensor(104.4261, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 39.696969696969695\n",
      "loss是： tensor(122.6830, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 35.9016393442623\n",
      "loss是： tensor(104.1140, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 28.955223880597014\n",
      "loss是： tensor(93.6514, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 22.5\n",
      "loss是： tensor(77.0168, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 32.30769230769231\n",
      "loss是： tensor(124.7982, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 28.732394366197184\n",
      "loss是： tensor(75.2683, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 28.507462686567166\n",
      "loss是： tensor(93.6273, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 42.74193548387097\n",
      "loss是： tensor(117.1407, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 26.451612903225804\n",
      "loss是： tensor(109.4473, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 56.833333333333336\n",
      "loss是： tensor(148.1456, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 40.96774193548387\n",
      "loss是： tensor(105.6331, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 36.56716417910448\n",
      "loss是： tensor(140.0913, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 4.705882352941177\n",
      "loss是： tensor(70.8099, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 67.25806451612902\n",
      "loss是： tensor(160.5119, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 25.846153846153847\n",
      "loss是： tensor(90.6361, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 28.10810810810811\n",
      "loss是： tensor(90.0665, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 35.64516129032258\n",
      "loss是： tensor(98.4207, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 31.129032258064516\n",
      "loss是： tensor(95.9046, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 33.392857142857146\n",
      "loss是： tensor(102.4059, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 38.61538461538461\n",
      "loss是： tensor(107.5317, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 12.222222222222223\n",
      "loss是： tensor(74.1643, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 27.413793103448274\n",
      "loss是： tensor(87.0491, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 28.88888888888889\n",
      "loss是： tensor(103.9110, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 31.475409836065573\n",
      "loss是： tensor(112.1322, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 32.6865671641791\n",
      "loss是： tensor(98.1800, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 41.34328358208955\n",
      "loss是： tensor(112.0527, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 50.61224489795919\n",
      "loss是： tensor(132.8908, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 27.424242424242422\n",
      "loss是： tensor(92.9543, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 40.59701492537314\n",
      "loss是： tensor(108.8603, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 35.820895522388064\n",
      "loss是： tensor(114.2498, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 20.147058823529413\n",
      "loss是： tensor(95.0360, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 40.298507462686565\n",
      "loss是： tensor(151.5691, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 42.22222222222222\n",
      "loss是： tensor(122.4625, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 22.698412698412696\n",
      "loss是： tensor(93.8486, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 21.0\n",
      "loss是： tensor(88.5089, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 37.42857142857143\n",
      "loss是： tensor(72.8800, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 41.911764705882355\n",
      "loss是： tensor(107.1638, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 29.696969696969695\n",
      "loss是： tensor(89.1985, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 35.87301587301587\n",
      "loss是： tensor(132.4705, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 30.98360655737705\n",
      "loss是： tensor(99.8265, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 55.24590163934426\n",
      "loss是： tensor(128.6341, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 38.813559322033896\n",
      "loss是： tensor(104.7676, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 31.23076923076923\n",
      "loss是： tensor(100.8001, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 34.464285714285715\n",
      "loss是： tensor(92.6529, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 25.0\n",
      "loss是： tensor(78.9713, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 31.428571428571427\n",
      "loss是： tensor(115.7124, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 24.444444444444446\n",
      "loss是： tensor(69.0009, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 28.76923076923077\n",
      "loss是： tensor(106.4465, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 36.45161290322581\n",
      "loss是： tensor(101.7090, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 33.389830508474574\n",
      "loss是： tensor(91.8435, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 17.894736842105264\n",
      "loss是： tensor(74.4658, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 29.58904109589041\n",
      "loss是： tensor(107.7852, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 25.967741935483872\n",
      "loss是： tensor(74.4464, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 42.32876712328767\n",
      "loss是： tensor(130.5773, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 37.11864406779661\n",
      "loss是： tensor(95.2411, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 39.44444444444444\n",
      "loss是： tensor(106.1939, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 40.0\n",
      "loss是： tensor(102.4675, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 39.166666666666664\n",
      "loss是： tensor(102.1723, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 33.095238095238095\n",
      "loss是： tensor(63.2784, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 38.225806451612904\n",
      "loss是： tensor(116.0484, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 35.14705882352941\n",
      "loss是： tensor(74.5815, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 24.6875\n",
      "loss是： tensor(82.8516, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 35.5\n",
      "loss是： tensor(91.3074, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 24.137931034482758\n",
      "loss是： tensor(86.1578, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 38.333333333333336\n",
      "loss是： tensor(115.7702, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 33.492063492063494\n",
      "loss是： tensor(97.1817, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 33.42465753424658\n",
      "loss是： tensor(95.1779, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 29.859154929577464\n",
      "loss是： tensor(104.0448, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 39.63636363636364\n",
      "loss是： tensor(121.2560, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 39.40298507462686\n",
      "loss是： tensor(121.2448, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 27.678571428571427\n",
      "loss是： tensor(96.6013, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 29.516129032258064\n",
      "loss是： tensor(89.9559, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 26.818181818181817\n",
      "loss是： tensor(82.5143, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 52.5\n",
      "loss是： tensor(112.1911, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 28.28125\n",
      "loss是： tensor(92.8798, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 4.516129032258064\n",
      "loss是： tensor(82.6853, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 39.5\n",
      "loss是： tensor(124.2549, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 32.22222222222222\n",
      "loss是： tensor(121.4326, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 31.71875\n",
      "loss是： tensor(105.6145, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 27.321428571428573\n",
      "loss是： tensor(98.8061, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 42.38095238095238\n",
      "loss是： tensor(108.3145, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 29.5\n",
      "loss是： tensor(57.3836, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 41.73333333333333\n",
      "loss是： tensor(129.3671, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 30.508474576271183\n",
      "loss是： tensor(106.2768, device='cuda:0', grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert计算的loss是： 38.208955223880594\n",
      "loss是： tensor(130.6218, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 10.476190476190476\n",
      "loss是： tensor(80.5411, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 40.8955223880597\n",
      "loss是： tensor(140.4198, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 29.523809523809526\n",
      "loss是： tensor(90.8720, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 39.11764705882353\n",
      "loss是： tensor(113.1714, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 26.46153846153846\n",
      "loss是： tensor(87.2660, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 41.515151515151516\n",
      "loss是： tensor(103.6577, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 42.16666666666667\n",
      "loss是： tensor(127.1905, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 39.55882352941177\n",
      "loss是： tensor(114.5329, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 30.526315789473685\n",
      "loss是： tensor(97.9687, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 59.03225806451613\n",
      "loss是： tensor(144.3194, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 30.491803278688522\n",
      "loss是： tensor(85.7730, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 26.417910447761194\n",
      "loss是： tensor(104.5524, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 10.338983050847457\n",
      "loss是： tensor(60.0434, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 40.76923076923077\n",
      "loss是： tensor(118.4592, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 25.4\n",
      "loss是： tensor(80.0253, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 24.242424242424242\n",
      "loss是： tensor(85.1983, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 26.885245901639344\n",
      "loss是： tensor(79.0607, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 32.786885245901644\n",
      "loss是： tensor(97.5939, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 34.47761194029851\n",
      "loss是： tensor(147.1948, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 25.333333333333332\n",
      "loss是： tensor(62.2586, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 29.193548387096776\n",
      "loss是： tensor(106.2052, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 26.666666666666664\n",
      "loss是： tensor(77.7347, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 23.59375\n",
      "loss是： tensor(88.7025, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 30.338983050847457\n",
      "loss是： tensor(84.9146, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 20.615384615384613\n",
      "loss是： tensor(63.2781, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 36.61971830985915\n",
      "loss是： tensor(145.8183, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 34.54545454545455\n",
      "loss是： tensor(91.8310, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 34.24242424242424\n",
      "loss是： tensor(89.5300, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 33.114754098360656\n",
      "loss是： tensor(104.0967, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 27.719298245614038\n",
      "loss是： tensor(89.7906, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 37.93103448275862\n",
      "loss是： tensor(105.8688, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 32.096774193548384\n",
      "loss是： tensor(103.0089, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 9.242424242424242\n",
      "loss是： tensor(88.3019, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 31.61290322580645\n",
      "loss是： tensor(102.5465, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 32.08955223880597\n",
      "loss是： tensor(119.2855, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 29.107142857142854\n",
      "loss是： tensor(99.4485, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 39.85714285714286\n",
      "loss是： tensor(106.7518, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 7.049180327868853\n",
      "loss是： tensor(58.8312, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 34.492753623188406\n",
      "loss是： tensor(88.1067, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 20.166666666666664\n",
      "loss是： tensor(115.6936, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 31.26984126984127\n",
      "loss是： tensor(82.0240, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 36.0\n",
      "loss是： tensor(108.0113, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 38.86363636363636\n",
      "loss是： tensor(94.3626, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "260/300\n",
      "bert计算的loss是： 31.666666666666664\n",
      "loss是： tensor(94.7711, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 51.40625\n",
      "loss是： tensor(128.3268, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 24.92753623188406\n",
      "loss是： tensor(101.4174, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 28.461538461538463\n",
      "loss是： tensor(89.4638, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 30.634920634920636\n",
      "loss是： tensor(94.0210, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 30.535714285714285\n",
      "loss是： tensor(80.3282, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 28.96551724137931\n",
      "loss是： tensor(71.8417, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 38.93939393939394\n",
      "loss是： tensor(121.5102, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 31.944444444444446\n",
      "loss是： tensor(114.5956, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 28.42857142857143\n",
      "loss是： tensor(105.1871, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 34.705882352941174\n",
      "loss是： tensor(122.1780, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 32.77777777777778\n",
      "loss是： tensor(103.7826, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 26.53061224489796\n",
      "loss是： tensor(77.9089, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 23.70967741935484\n",
      "loss是： tensor(64.8130, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 31.864406779661017\n",
      "loss是： tensor(80.8894, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 32.07547169811321\n",
      "loss是： tensor(84.8704, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 26.3265306122449\n",
      "loss是： tensor(78.2154, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 37.27272727272727\n",
      "loss是： tensor(111.3100, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 34.03225806451613\n",
      "loss是： tensor(107.6087, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 28.103448275862068\n",
      "loss是： tensor(70.7790, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 16.714285714285715\n",
      "loss是： tensor(102.7059, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 31.343283582089555\n",
      "loss是： tensor(109.1491, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 34.4\n",
      "loss是： tensor(125.2108, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 21.864406779661017\n",
      "loss是： tensor(123.0134, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 34.91803278688525\n",
      "loss是： tensor(86.9880, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 30.144927536231883\n",
      "loss是： tensor(102.0583, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 25.471698113207548\n",
      "loss是： tensor(72.1311, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 1.5873015873015872\n",
      "loss是： tensor(65.6614, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 41.74603174603175\n",
      "loss是： tensor(129.1306, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 26.056338028169016\n",
      "loss是： tensor(88.9310, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 31.454545454545453\n",
      "loss是： tensor(89.2140, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 34.62686567164179\n",
      "loss是： tensor(90.8487, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 30.3125\n",
      "loss是： tensor(103.9462, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 31.78082191780822\n",
      "loss是： tensor(97.9173, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 32.10526315789474\n",
      "loss是： tensor(87.5316, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 57.049180327868854\n",
      "loss是： tensor(123.4913, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 42.06349206349206\n",
      "loss是： tensor(102.7458, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 31.071428571428573\n",
      "loss是： tensor(90.3910, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 30.0\n",
      "loss是： tensor(106.9131, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 35.53846153846153\n",
      "loss是： tensor(94.5761, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 43.97058823529412\n",
      "loss是： tensor(122.4434, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 25.652173913043477\n",
      "loss是： tensor(89.1741, device='cuda:0', grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert计算的loss是： 39.710144927536234\n",
      "loss是： tensor(98.8420, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 32.25806451612903\n",
      "loss是： tensor(108.3755, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 32.205882352941174\n",
      "loss是： tensor(108.3937, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 30.526315789473685\n",
      "loss是： tensor(55.6019, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 41.166666666666664\n",
      "loss是： tensor(101.9640, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 34.24657534246575\n",
      "loss是： tensor(114.6479, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 30.357142857142854\n",
      "loss是： tensor(99.0494, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 7.230769230769231\n",
      "loss是： tensor(61.1191, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 16.166666666666668\n",
      "loss是： tensor(71.9490, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 30.0\n",
      "loss是： tensor(86.9361, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 12.537313432835822\n",
      "loss是： tensor(89.7410, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 60.175438596491226\n",
      "loss是： tensor(133.0205, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 23.833333333333332\n",
      "loss是： tensor(98.9293, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 32.142857142857146\n",
      "loss是： tensor(117.4772, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 33.333333333333336\n",
      "loss是： tensor(120.0565, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 45.07692307692308\n",
      "loss是： tensor(107.2970, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 28.333333333333336\n",
      "loss是： tensor(75.6833, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 26.29032258064516\n",
      "loss是： tensor(92.2418, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 37.72727272727273\n",
      "loss是： tensor(114.3261, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 27.5\n",
      "loss是： tensor(97.6376, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 34.50980392156863\n",
      "loss是： tensor(100.6379, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 33.492063492063494\n",
      "loss是： tensor(86.0336, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 24.736842105263158\n",
      "loss是： tensor(89.0967, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 31.11111111111111\n",
      "loss是： tensor(99.0816, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 52.68656716417911\n",
      "loss是： tensor(115.2823, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 41.15942028985507\n",
      "loss是： tensor(146.8013, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 25.675675675675677\n",
      "loss是： tensor(83.9759, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 26.94915254237288\n",
      "loss是： tensor(93.4371, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 53.67647058823529\n",
      "loss是： tensor(140.6428, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 34.15384615384615\n",
      "loss是： tensor(100.1907, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 14.153846153846155\n",
      "loss是： tensor(68.8981, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 33.225806451612904\n",
      "loss是： tensor(107.1984, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 28.4375\n",
      "loss是： tensor(77.5117, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 26.81159420289855\n",
      "loss是： tensor(91.9921, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 30.142857142857142\n",
      "loss是： tensor(77.0761, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 30.3125\n",
      "loss是： tensor(90.9041, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 33.96825396825397\n",
      "loss是： tensor(112.5900, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 34.72727272727273\n",
      "loss是： tensor(89.2914, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 31.40625\n",
      "loss是： tensor(93.5091, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 37.1875\n",
      "loss是： tensor(101.9778, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 32.64705882352941\n",
      "loss是： tensor(95.8042, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 28.840579710144926\n",
      "loss是： tensor(113.0661, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 28.833333333333332\n",
      "loss是： tensor(104.1071, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 41.58730158730159\n",
      "loss是： tensor(123.3519, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 35.51724137931035\n",
      "loss是： tensor(82.9781, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 38.18181818181819\n",
      "loss是： tensor(94.8060, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 10.517241379310345\n",
      "loss是： tensor(97.9802, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 30.757575757575758\n",
      "loss是： tensor(116.4641, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 41.864406779661024\n",
      "loss是： tensor(93.5726, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 39.729729729729726\n",
      "loss是： tensor(83.6398, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 30.0\n",
      "loss是： tensor(92.8230, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 38.87096774193549\n",
      "loss是： tensor(111.1200, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 32.53968253968254\n",
      "loss是： tensor(107.0524, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 35.57377049180328\n",
      "loss是： tensor(91.9207, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 24.47761194029851\n",
      "loss是： tensor(76.6583, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 60.86956521739131\n",
      "loss是： tensor(152.9617, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 27.636363636363637\n",
      "loss是： tensor(69.6970, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 34.225352112676056\n",
      "loss是： tensor(142.7261, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 26.96969696969697\n",
      "loss是： tensor(88.6573, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 37.23076923076923\n",
      "loss是： tensor(84.1692, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 4.363636363636363\n",
      "loss是： tensor(75.1320, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 32.7536231884058\n",
      "loss是： tensor(97.3543, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 43.064516129032256\n",
      "loss是： tensor(127.2499, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 23.59375\n",
      "loss是： tensor(86.5055, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 35.14285714285714\n",
      "loss是： tensor(107.8444, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 44.074074074074076\n",
      "loss是： tensor(102.6319, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 34.42622950819672\n",
      "loss是： tensor(104.4782, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 34.03508771929825\n",
      "loss是： tensor(109.1605, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 26.379310344827584\n",
      "loss是： tensor(80.9907, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 36.76056338028169\n",
      "loss是： tensor(83.3922, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 31.449275362318843\n",
      "loss是： tensor(101.9504, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 30.81967213114754\n",
      "loss是： tensor(93.8806, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 26.34146341463415\n",
      "loss是： tensor(57.5363, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 36.774193548387096\n",
      "loss是： tensor(121.6983, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 33.064516129032256\n",
      "loss是： tensor(108.0550, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 34.61538461538461\n",
      "loss是： tensor(112.3578, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 41.346153846153854\n",
      "loss是： tensor(103.8617, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 32.58064516129033\n",
      "loss是： tensor(107.0220, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 29.130434782608695\n",
      "loss是： tensor(76.7894, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 37.36842105263158\n",
      "loss是： tensor(94.1032, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 33.89830508474576\n",
      "loss是： tensor(94.0633, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 38.333333333333336\n",
      "loss是： tensor(102.5757, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 33.0\n",
      "loss是： tensor(88.1252, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 49.523809523809526\n",
      "loss是： tensor(95.4520, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 32.16666666666667\n",
      "loss是： tensor(90.7759, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 4.6268656716417915\n",
      "loss是： tensor(77.8314, device='cuda:0', grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert计算的loss是： 29.18032786885246\n",
      "loss是： tensor(80.0299, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 35.69230769230769\n",
      "loss是： tensor(105.8889, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 31.372549019607842\n",
      "loss是： tensor(87.2449, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 35.37313432835821\n",
      "loss是： tensor(105.8566, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 18.307692307692307\n",
      "loss是： tensor(100.2243, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 49.726027397260275\n",
      "loss是： tensor(137.5322, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 29.84375\n",
      "loss是： tensor(104.4832, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 28.591549295774648\n",
      "loss是： tensor(100.2224, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 31.884057971014492\n",
      "loss是： tensor(108.9823, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 24.0\n",
      "loss是： tensor(53.5929, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 37.46478873239437\n",
      "loss是： tensor(116.5162, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 46.065573770491795\n",
      "loss是： tensor(107.2287, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 59.6875\n",
      "loss是： tensor(160.7404, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 34.91228070175439\n",
      "loss是： tensor(107.0879, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 35.79710144927536\n",
      "loss是： tensor(114.8605, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 32.42857142857143\n",
      "loss是： tensor(93.6720, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 38.16666666666667\n",
      "loss是： tensor(92.1700, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 28.103448275862068\n",
      "loss是： tensor(86.3848, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 31.26984126984127\n",
      "loss是： tensor(82.9356, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 34.83870967741935\n",
      "loss是： tensor(89.5536, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 37.09090909090909\n",
      "loss是： tensor(98.0525, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 37.16666666666667\n",
      "loss是： tensor(92.9026, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 28.571428571428573\n",
      "loss是： tensor(102.9185, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 29.60526315789474\n",
      "loss是： tensor(112.1085, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 31.428571428571427\n",
      "loss是： tensor(105.2543, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 27.8125\n",
      "loss是： tensor(96.6628, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 32.30769230769231\n",
      "loss是： tensor(94.3655, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 5.0\n",
      "loss是： tensor(65.2065, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 29.850746268656717\n",
      "loss是： tensor(91.9734, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 38.18181818181819\n",
      "loss是： tensor(102.4814, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 14.411764705882353\n",
      "loss是： tensor(67.7298, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 20.46875\n",
      "loss是： tensor(71.1395, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 42.35294117647059\n",
      "loss是： tensor(92.2200, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 37.540983606557376\n",
      "loss是： tensor(144.0511, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 39.18032786885246\n",
      "loss是： tensor(101.2827, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 21.428571428571427\n",
      "loss是： tensor(94.0799, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 12.53968253968254\n",
      "loss是： tensor(84.9946, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 30.877192982456137\n",
      "loss是： tensor(91.4696, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 26.779661016949152\n",
      "loss是： tensor(96.5733, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 11.139240506329113\n",
      "loss是： tensor(80.6320, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 26.03174603174603\n",
      "loss是： tensor(81.0561, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 44.28571428571429\n",
      "loss是： tensor(112.3457, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 27.77777777777778\n",
      "loss是： tensor(81.9227, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 30.147058823529413\n",
      "loss是： tensor(94.7806, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 22.96875\n",
      "loss是： tensor(91.1898, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 35.55555555555556\n",
      "loss是： tensor(107.5433, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 31.06060606060606\n",
      "loss是： tensor(98.7119, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 41.14285714285714\n",
      "loss是： tensor(108.2532, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 33.22033898305085\n",
      "loss是： tensor(112.2772, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 43.89830508474576\n",
      "loss是： tensor(116.2185, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 55.362318840579704\n",
      "loss是： tensor(141.3238, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 28.596491228070175\n",
      "loss是： tensor(88.0309, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 43.78787878787879\n",
      "loss是： tensor(105.3824, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 30.634920634920636\n",
      "loss是： tensor(100.7454, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 40.86206896551724\n",
      "loss是： tensor(131.6557, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 61.53846153846154\n",
      "loss是： tensor(96.7262, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 28.412698412698415\n",
      "loss是： tensor(95.3837, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 27.61904761904762\n",
      "loss是： tensor(102.7943, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 35.67164179104478\n",
      "loss是： tensor(101.4124, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 35.5\n",
      "loss是： tensor(87.6252, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 30.434782608695656\n",
      "loss是： tensor(87.9245, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 25.862068965517242\n",
      "loss是： tensor(95.8330, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 39.84375\n",
      "loss是： tensor(114.5410, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 36.0\n",
      "loss是： tensor(102.5147, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 44.920634920634924\n",
      "loss是： tensor(119.8223, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 33.66197183098591\n",
      "loss是： tensor(113.0895, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 10.0\n",
      "loss是： tensor(86.3528, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 39.21875\n",
      "loss是： tensor(105.9097, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 36.31578947368421\n",
      "loss是： tensor(119.1466, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 49.03225806451613\n",
      "loss是： tensor(116.5321, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 24.21875\n",
      "loss是： tensor(89.3780, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 27.34375\n",
      "loss是： tensor(87.7897, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 42.75862068965517\n",
      "loss是： tensor(116.2559, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 38.36065573770492\n",
      "loss是： tensor(89.7076, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 31.23076923076923\n",
      "loss是： tensor(91.5692, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 35.6\n",
      "loss是： tensor(99.5119, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 24.833333333333336\n",
      "loss是： tensor(79.4610, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 32.76923076923077\n",
      "loss是： tensor(120.1205, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 19.45945945945946\n",
      "loss是： tensor(64.4109, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 28.666666666666668\n",
      "loss是： tensor(96.6633, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 5.441176470588235\n",
      "loss是： tensor(61.8300, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 30.65573770491803\n",
      "loss是： tensor(82.3302, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 35.84905660377358\n",
      "loss是： tensor(91.6416, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 27.027027027027025\n",
      "loss是： tensor(99.4970, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 42.16216216216216\n",
      "loss是： tensor(127.4486, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 28.26086956521739\n",
      "loss是： tensor(91.4770, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 46.12903225806452\n",
      "loss是： tensor(108.2539, device='cuda:0', grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert计算的loss是： 3.870967741935484\n",
      "loss是： tensor(68.8344, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 27.121212121212118\n",
      "loss是： tensor(103.1424, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 37.37704918032787\n",
      "loss是： tensor(119.1722, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 14.406779661016948\n",
      "loss是： tensor(109.5194, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 27.06896551724138\n",
      "loss是： tensor(72.6779, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 32.459016393442624\n",
      "loss是： tensor(97.0100, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 37.79661016949153\n",
      "loss是： tensor(109.6343, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 34.21052631578947\n",
      "loss是： tensor(81.4328, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 58.92857142857143\n",
      "loss是： tensor(140.8898, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 6.323529411764706\n",
      "loss是： tensor(68.5166, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 30.327868852459016\n",
      "loss是： tensor(110.1276, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 11.971830985915492\n",
      "loss是： tensor(78.7444, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 28.666666666666668\n",
      "loss是： tensor(80.5182, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 34.861111111111114\n",
      "loss是： tensor(109.7492, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 31.666666666666664\n",
      "loss是： tensor(80.4563, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "270/300\n",
      "bert计算的loss是： 38.80597014925373\n",
      "loss是： tensor(116.3499, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 35.892857142857146\n",
      "loss是： tensor(76.0565, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 25.614035087719298\n",
      "loss是： tensor(77.2535, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 31.12676056338028\n",
      "loss是： tensor(95.0624, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 35.166666666666664\n",
      "loss是： tensor(104.4609, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 26.779661016949152\n",
      "loss是： tensor(80.8755, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 36.71052631578947\n",
      "loss是： tensor(123.3941, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 33.28358208955224\n",
      "loss是： tensor(94.3919, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 53.23529411764706\n",
      "loss是： tensor(113.3338, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 24.363636363636367\n",
      "loss是： tensor(60.8109, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 43.275862068965516\n",
      "loss是： tensor(115.8144, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 34.92957746478873\n",
      "loss是： tensor(126.7475, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 7.288135593220338\n",
      "loss是： tensor(58.3835, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 38.0\n",
      "loss是： tensor(116.8081, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 34.59016393442623\n",
      "loss是： tensor(102.5435, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 27.333333333333336\n",
      "loss是： tensor(96.3503, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 26.666666666666664\n",
      "loss是： tensor(103.7119, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 37.32142857142857\n",
      "loss是： tensor(86.4595, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 28.14814814814815\n",
      "loss是： tensor(73.1202, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 45.21739130434782\n",
      "loss是： tensor(99.5595, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 34.75409836065574\n",
      "loss是： tensor(103.4342, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 40.63492063492063\n",
      "loss是： tensor(88.3682, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 19.210526315789473\n",
      "loss是： tensor(61.3662, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 34.61538461538461\n",
      "loss是： tensor(91.3464, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 36.46153846153846\n",
      "loss是： tensor(100.5643, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 30.185185185185187\n",
      "loss是： tensor(77.1139, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 53.63636363636363\n",
      "loss是： tensor(117.8958, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 26.363636363636363\n",
      "loss是： tensor(83.9322, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 20.9375\n",
      "loss是： tensor(100.4891, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 33.05084745762712\n",
      "loss是： tensor(76.5751, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 25.185185185185187\n",
      "loss是： tensor(72.2506, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 34.42307692307693\n",
      "loss是： tensor(81.2615, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 28.813559322033896\n",
      "loss是： tensor(92.9398, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 37.142857142857146\n",
      "loss是： tensor(88.2071, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 46.96428571428571\n",
      "loss是： tensor(105.9915, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 35.0\n",
      "loss是： tensor(95.0674, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 28.153846153846157\n",
      "loss是： tensor(73.6876, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 8.028169014084508\n",
      "loss是： tensor(84.7280, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 33.90625\n",
      "loss是： tensor(111.8735, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 33.559322033898304\n",
      "loss是： tensor(81.5934, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 35.55555555555556\n",
      "loss是： tensor(89.3151, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 25.774647887323944\n",
      "loss是： tensor(73.8571, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 36.0\n",
      "loss是： tensor(141.6887, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 57.142857142857146\n",
      "loss是： tensor(135.7348, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 27.352941176470587\n",
      "loss是： tensor(93.3533, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 34.883720930232556\n",
      "loss是： tensor(88.5638, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 6.724137931034483\n",
      "loss是： tensor(75.1359, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 38.24561403508772\n",
      "loss是： tensor(105.3456, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 46.206896551724135\n",
      "loss是： tensor(87.7607, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 24.666666666666668\n",
      "loss是： tensor(105.0722, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 30.483870967741936\n",
      "loss是： tensor(76.5571, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 28.75\n",
      "loss是： tensor(86.8102, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 35.53846153846153\n",
      "loss是： tensor(117.9004, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 30.3125\n",
      "loss是： tensor(112.7486, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 33.65079365079365\n",
      "loss是： tensor(96.5386, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 30.317460317460316\n",
      "loss是： tensor(107.5131, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 1.6901408450704225\n",
      "loss是： tensor(77.8706, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 34.21052631578947\n",
      "loss是： tensor(79.3948, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 13.833333333333332\n",
      "loss是： tensor(86.3197, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 43.70967741935484\n",
      "loss是： tensor(142.7711, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 15.0\n",
      "loss是： tensor(89.9648, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 43.07692307692307\n",
      "loss是： tensor(92.2722, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 18.18181818181818\n",
      "loss是： tensor(94.8803, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 35.13157894736842\n",
      "loss是： tensor(112.0267, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 34.411764705882355\n",
      "loss是： tensor(99.0416, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 42.25352112676056\n",
      "loss是： tensor(117.4095, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 28.80597014925373\n",
      "loss是： tensor(103.8256, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 33.013698630136986\n",
      "loss是： tensor(114.9237, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 6.666666666666666\n",
      "loss是： tensor(27.9189, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 38.25396825396825\n",
      "loss是： tensor(108.4687, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 31.911764705882355\n",
      "loss是： tensor(79.7305, device='cuda:0', grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert计算的loss是： 31.228070175438596\n",
      "loss是： tensor(88.6276, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 32.16666666666667\n",
      "loss是： tensor(98.9815, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 43.07692307692307\n",
      "loss是： tensor(90.6525, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 34.90909090909091\n",
      "loss是： tensor(99.4523, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 39.85507246376812\n",
      "loss是： tensor(122.8882, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 1.3157894736842104\n",
      "loss是： tensor(80.2933, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 31.846153846153843\n",
      "loss是： tensor(109.8000, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 31.904761904761905\n",
      "loss是： tensor(77.5473, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 37.74193548387097\n",
      "loss是： tensor(119.5383, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 22.5\n",
      "loss是： tensor(62.7512, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 45.0\n",
      "loss是： tensor(122.9739, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 27.6271186440678\n",
      "loss是： tensor(100.2459, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 25.32258064516129\n",
      "loss是： tensor(72.7699, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 25.223880597014926\n",
      "loss是： tensor(95.2336, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 36.166666666666664\n",
      "loss是： tensor(114.3174, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 24.82758620689655\n",
      "loss是： tensor(81.8538, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 44.60317460317461\n",
      "loss是： tensor(105.2786, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 41.15942028985507\n",
      "loss是： tensor(107.5968, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 28.529411764705884\n",
      "loss是： tensor(102.1713, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 33.28125\n",
      "loss是： tensor(114.8052, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 38.048780487804876\n",
      "loss是： tensor(76.6133, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 34.166666666666664\n",
      "loss是： tensor(98.9602, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 42.5\n",
      "loss是： tensor(84.3638, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 26.29032258064516\n",
      "loss是： tensor(109.6528, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 26.842105263157894\n",
      "loss是： tensor(93.9490, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 40.87719298245614\n",
      "loss是： tensor(91.4585, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 32.67857142857143\n",
      "loss是： tensor(102.1414, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 38.19672131147541\n",
      "loss是： tensor(116.1467, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 35.714285714285715\n",
      "loss是： tensor(121.9995, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 31.23076923076923\n",
      "loss是： tensor(85.6726, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 32.857142857142854\n",
      "loss是： tensor(102.8146, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 38.793103448275865\n",
      "loss是： tensor(99.0460, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 35.69444444444444\n",
      "loss是： tensor(96.2151, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 32.74193548387097\n",
      "loss是： tensor(90.9743, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 37.01754385964912\n",
      "loss是： tensor(92.7686, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 43.166666666666664\n",
      "loss是： tensor(112.0249, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 31.5625\n",
      "loss是： tensor(95.3135, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 30.76923076923077\n",
      "loss是： tensor(82.0496, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 26.40625\n",
      "loss是： tensor(82.3975, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 31.451612903225804\n",
      "loss是： tensor(83.8016, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 32.25806451612903\n",
      "loss是： tensor(87.0115, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 23.75\n",
      "loss是： tensor(82.6782, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 19.0\n",
      "loss是： tensor(66.3484, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 32.95454545454545\n",
      "loss是： tensor(65.4431, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 35.26315789473684\n",
      "loss是： tensor(99.9986, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 5.735294117647058\n",
      "loss是： tensor(85.7121, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 29.047619047619047\n",
      "loss是： tensor(91.6034, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 42.857142857142854\n",
      "loss是： tensor(115.8861, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 31.5\n",
      "loss是： tensor(119.0707, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 39.25373134328358\n",
      "loss是： tensor(86.7122, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 27.16417910447761\n",
      "loss是： tensor(94.5520, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 23.846153846153847\n",
      "loss是： tensor(69.5781, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 25.0\n",
      "loss是： tensor(74.8893, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 34.44444444444444\n",
      "loss是： tensor(106.2000, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 67.72727272727272\n",
      "loss是： tensor(146.6792, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 15.074626865671641\n",
      "loss是： tensor(73.7452, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 42.20338983050848\n",
      "loss是： tensor(128.7049, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 27.936507936507937\n",
      "loss是： tensor(93.3097, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 25.555555555555554\n",
      "loss是： tensor(91.4683, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 32.59259259259259\n",
      "loss是： tensor(103.8190, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 26.0\n",
      "loss是： tensor(81.4144, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 31.142857142857142\n",
      "loss是： tensor(91.4906, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 41.96428571428571\n",
      "loss是： tensor(95.3783, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 31.408450704225352\n",
      "loss是： tensor(91.5471, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 31.21212121212121\n",
      "loss是： tensor(109.5857, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 12.166666666666666\n",
      "loss是： tensor(57.7107, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 45.9375\n",
      "loss是： tensor(86.7077, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 14.657534246575343\n",
      "loss是： tensor(60.4526, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 10.701754385964913\n",
      "loss是： tensor(69.4380, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 46.39344262295082\n",
      "loss是： tensor(124.0226, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 28.867924528301888\n",
      "loss是： tensor(85.0613, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 39.44444444444444\n",
      "loss是： tensor(106.0195, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 29.545454545454547\n",
      "loss是： tensor(100.5174, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 41.4\n",
      "loss是： tensor(95.6909, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 31.044776119402986\n",
      "loss是： tensor(106.7411, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 33.65079365079365\n",
      "loss是： tensor(114.5864, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 26.393442622950822\n",
      "loss是： tensor(91.2455, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 37.096774193548384\n",
      "loss是： tensor(108.7407, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 34.60317460317461\n",
      "loss是： tensor(116.9058, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 19.558823529411764\n",
      "loss是： tensor(73.9441, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 39.66666666666667\n",
      "loss是： tensor(114.2557, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 31.52542372881356\n",
      "loss是： tensor(87.6148, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 31.639344262295083\n",
      "loss是： tensor(74.2688, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 40.79365079365079\n",
      "loss是： tensor(90.6597, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 38.656716417910445\n",
      "loss是： tensor(101.1487, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 31.343283582089555\n",
      "loss是： tensor(104.9352, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 38.714285714285715\n",
      "loss是： tensor(83.1884, device='cuda:0', grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert计算的loss是： 20.666666666666668\n",
      "loss是： tensor(59.6380, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 30.51724137931035\n",
      "loss是： tensor(109.3043, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 68.80952380952381\n",
      "loss是： tensor(109.4120, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 34.18181818181818\n",
      "loss是： tensor(116.8490, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 34.0983606557377\n",
      "loss是： tensor(81.6723, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 36.166666666666664\n",
      "loss是： tensor(79.1215, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 31.52542372881356\n",
      "loss是： tensor(93.1612, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 36.935483870967744\n",
      "loss是： tensor(118.0879, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 33.38709677419355\n",
      "loss是： tensor(89.1603, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 35.714285714285715\n",
      "loss是： tensor(94.4317, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 37.205882352941174\n",
      "loss是： tensor(106.6077, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 28.214285714285715\n",
      "loss是： tensor(85.2341, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 38.70967741935484\n",
      "loss是： tensor(93.3698, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 36.721311475409834\n",
      "loss是： tensor(85.5887, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 31.83098591549296\n",
      "loss是： tensor(109.0714, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 33.38461538461539\n",
      "loss是： tensor(117.4388, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 4.285714285714286\n",
      "loss是： tensor(69.3044, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 20.684931506849313\n",
      "loss是： tensor(80.8469, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 33.333333333333336\n",
      "loss是： tensor(80.3452, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 22.46153846153846\n",
      "loss是： tensor(76.2080, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 38.98550724637681\n",
      "loss是： tensor(101.2306, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 31.833333333333332\n",
      "loss是： tensor(74.5364, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 38.93939393939394\n",
      "loss是： tensor(96.6904, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 55.08196721311475\n",
      "loss是： tensor(108.9740, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 33.23076923076923\n",
      "loss是： tensor(117.6961, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 44.883720930232556\n",
      "loss是： tensor(98.3917, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 39.49152542372882\n",
      "loss是： tensor(116.2953, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 30.178571428571427\n",
      "loss是： tensor(90.8654, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 25.245901639344265\n",
      "loss是： tensor(75.0722, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 30.8955223880597\n",
      "loss是： tensor(109.6852, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 22.62295081967213\n",
      "loss是： tensor(78.0170, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 33.07692307692307\n",
      "loss是： tensor(121.5580, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 29.836065573770494\n",
      "loss是： tensor(72.8058, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 40.76923076923077\n",
      "loss是： tensor(102.5176, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 35.36231884057971\n",
      "loss是： tensor(98.3241, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 27.77777777777778\n",
      "loss是： tensor(104.5254, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 26.379310344827584\n",
      "loss是： tensor(74.2618, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 13.05084745762712\n",
      "loss是： tensor(132.9275, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 35.60606060606061\n",
      "loss是： tensor(90.7606, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 25.283018867924525\n",
      "loss是： tensor(90.0648, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 38.857142857142854\n",
      "loss是： tensor(128.0684, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 36.440677966101696\n",
      "loss是： tensor(98.3632, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 49.333333333333336\n",
      "loss是： tensor(122.0546, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 28.857142857142858\n",
      "loss是： tensor(114.2453, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 4.5\n",
      "loss是： tensor(66.4453, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 64.42622950819671\n",
      "loss是： tensor(129.7186, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 34.30769230769231\n",
      "loss是： tensor(81.6793, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 28.03030303030303\n",
      "loss是： tensor(107.8731, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 37.94117647058823\n",
      "loss是： tensor(76.0969, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 29.666666666666668\n",
      "loss是： tensor(84.2087, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 29.35483870967742\n",
      "loss是： tensor(91.5109, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 30.862068965517242\n",
      "loss是： tensor(94.7399, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 39.64912280701754\n",
      "loss是： tensor(101.0707, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 24.6875\n",
      "loss是： tensor(105.6824, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 42.12121212121212\n",
      "loss是： tensor(109.3349, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 30.32258064516129\n",
      "loss是： tensor(96.3149, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 33.23529411764706\n",
      "loss是： tensor(101.5825, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 47.301587301587304\n",
      "loss是： tensor(143.2998, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 44.705882352941174\n",
      "loss是： tensor(114.2578, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 41.54929577464789\n",
      "loss是： tensor(117.9997, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 8.253968253968253\n",
      "loss是： tensor(63.9488, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 25.294117647058822\n",
      "loss是： tensor(75.7146, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 29.0625\n",
      "loss是： tensor(95.6617, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 38.275862068965516\n",
      "loss是： tensor(124.4291, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 34.59016393442623\n",
      "loss是： tensor(84.4981, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 36.666666666666664\n",
      "loss是： tensor(87.4788, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 35.07692307692308\n",
      "loss是： tensor(115.8999, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 29.411764705882355\n",
      "loss是： tensor(110.9034, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 31.40625\n",
      "loss是： tensor(85.5810, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 65.0\n",
      "loss是： tensor(141.7072, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 33.442622950819676\n",
      "loss是： tensor(91.0467, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 21.89189189189189\n",
      "loss是： tensor(46.0943, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "280/300\n",
      "bert计算的loss是： 39.50819672131148\n",
      "loss是： tensor(132.7301, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 45.16129032258065\n",
      "loss是： tensor(108.7039, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 31.61764705882353\n",
      "loss是： tensor(92.8335, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 42.76923076923077\n",
      "loss是： tensor(85.7586, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 27.21311475409836\n",
      "loss是： tensor(85.3918, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 21.272727272727273\n",
      "loss是： tensor(62.5624, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 38.03030303030303\n",
      "loss是： tensor(106.6632, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 36.39344262295082\n",
      "loss是： tensor(127.5179, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 36.388888888888886\n",
      "loss是： tensor(127.3229, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 34.6551724137931\n",
      "loss是： tensor(101.8131, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 43.15789473684211\n",
      "loss是： tensor(85.7504, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 30.847457627118647\n",
      "loss是： tensor(81.2289, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 33.27272727272727\n",
      "loss是： tensor(69.7568, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 34.920634920634924\n",
      "loss是： tensor(95.3487, device='cuda:0', grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert计算的loss是： 32.27272727272727\n",
      "loss是： tensor(108.9231, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 26.75324675324675\n",
      "loss是： tensor(96.9188, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 36.12903225806451\n",
      "loss是： tensor(86.0440, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 30.46153846153846\n",
      "loss是： tensor(103.1172, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 32.59259259259259\n",
      "loss是： tensor(82.8325, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 27.611940298507463\n",
      "loss是： tensor(102.0593, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 18.985507246376812\n",
      "loss是： tensor(78.5396, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 37.096774193548384\n",
      "loss是： tensor(100.7866, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 32.0\n",
      "loss是： tensor(79.6254, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 33.96551724137931\n",
      "loss是： tensor(90.4429, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 64.84375\n",
      "loss是： tensor(150.3075, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 12.173913043478262\n",
      "loss是： tensor(93.3550, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 39.230769230769226\n",
      "loss是： tensor(110.5257, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 33.59375\n",
      "loss是： tensor(93.1554, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 42.68656716417911\n",
      "loss是： tensor(130.2887, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 23.934426229508198\n",
      "loss是： tensor(94.6079, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 5.7142857142857135\n",
      "loss是： tensor(59.9409, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 28.96551724137931\n",
      "loss是： tensor(97.5442, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 23.384615384615387\n",
      "loss是： tensor(95.8083, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 35.75757575757576\n",
      "loss是： tensor(75.0474, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 29.166666666666664\n",
      "loss是： tensor(107.7673, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 23.620689655172416\n",
      "loss是： tensor(89.0331, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 29.71014492753623\n",
      "loss是： tensor(95.8755, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 29.285714285714285\n",
      "loss是： tensor(83.2359, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 56.031746031746025\n",
      "loss是： tensor(108.8839, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 34.76190476190476\n",
      "loss是： tensor(106.1994, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 26.769230769230766\n",
      "loss是： tensor(86.8968, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 40.0\n",
      "loss是： tensor(103.8179, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 35.69620253164557\n",
      "loss是： tensor(115.7864, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 33.50877192982456\n",
      "loss是： tensor(106.0561, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 21.846153846153843\n",
      "loss是： tensor(79.8162, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 23.513513513513512\n",
      "loss是： tensor(40.9461, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 20.508474576271183\n",
      "loss是： tensor(86.2556, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 33.78787878787879\n",
      "loss是： tensor(104.4027, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 38.35820895522388\n",
      "loss是： tensor(98.2365, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 36.507936507936506\n",
      "loss是： tensor(114.1870, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 34.40677966101695\n",
      "loss是： tensor(109.9842, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 56.11940298507463\n",
      "loss是： tensor(153.1941, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 33.8235294117647\n",
      "loss是： tensor(108.1009, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 36.18181818181818\n",
      "loss是： tensor(92.4276, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 27.6271186440678\n",
      "loss是： tensor(75.4342, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 34.15384615384615\n",
      "loss是： tensor(98.6626, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 6.7272727272727275\n",
      "loss是： tensor(47.2333, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 28.225806451612904\n",
      "loss是： tensor(82.8165, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 31.864406779661017\n",
      "loss是： tensor(88.0848, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 26.22950819672131\n",
      "loss是： tensor(107.9087, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 34.0983606557377\n",
      "loss是： tensor(104.5150, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 28.42857142857143\n",
      "loss是： tensor(122.9876, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 34.62686567164179\n",
      "loss是： tensor(105.1445, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 25.23076923076923\n",
      "loss是： tensor(92.4474, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 31.52542372881356\n",
      "loss是： tensor(118.5997, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 52.67605633802817\n",
      "loss是： tensor(138.8981, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 31.454545454545453\n",
      "loss是： tensor(73.1176, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 41.904761904761905\n",
      "loss是： tensor(125.1681, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 25.217391304347828\n",
      "loss是： tensor(52.9530, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 23.96825396825397\n",
      "loss是： tensor(76.5986, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 39.66101694915254\n",
      "loss是： tensor(87.7004, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 30.185185185185187\n",
      "loss是： tensor(75.0324, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 3.870967741935484\n",
      "loss是： tensor(76.3481, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 33.38709677419355\n",
      "loss是： tensor(92.5434, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 27.01492537313433\n",
      "loss是： tensor(78.4952, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 44.705882352941174\n",
      "loss是： tensor(115.1936, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 26.71428571428571\n",
      "loss是： tensor(93.0730, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 65.57692307692308\n",
      "loss是： tensor(119.5661, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 28.852459016393443\n",
      "loss是： tensor(92.3901, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 40.983606557377044\n",
      "loss是： tensor(96.7140, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 39.0625\n",
      "loss是： tensor(99.5450, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 33.6\n",
      "loss是： tensor(99.3380, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 27.301587301587304\n",
      "loss是： tensor(78.0591, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 44.333333333333336\n",
      "loss是： tensor(116.4121, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 42.592592592592595\n",
      "loss是： tensor(98.8358, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 26.0\n",
      "loss是： tensor(80.8071, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 29.692307692307693\n",
      "loss是： tensor(112.2309, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 32.567567567567565\n",
      "loss是： tensor(114.1677, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 31.694915254237287\n",
      "loss是： tensor(91.7720, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 35.39682539682539\n",
      "loss是： tensor(90.9173, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 26.774193548387096\n",
      "loss是： tensor(86.4579, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 34.8780487804878\n",
      "loss是： tensor(81.9451, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 29.152542372881353\n",
      "loss是： tensor(77.9637, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 36.25\n",
      "loss是： tensor(85.9266, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 42.714285714285715\n",
      "loss是： tensor(88.1747, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 46.769230769230774\n",
      "loss是： tensor(129.4103, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 27.246376811594203\n",
      "loss是： tensor(94.3923, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 23.333333333333336\n",
      "loss是： tensor(82.6037, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 34.84375\n",
      "loss是： tensor(96.2258, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 42.075471698113205\n",
      "loss是： tensor(90.8418, device='cuda:0', grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert计算的loss是： 32.57575757575758\n",
      "loss是： tensor(103.9164, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 20.434782608695656\n",
      "loss是： tensor(86.1718, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 37.5\n",
      "loss是： tensor(104.6207, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 21.475409836065573\n",
      "loss是： tensor(67.5080, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 58.305084745762706\n",
      "loss是： tensor(117.1223, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 34.516129032258064\n",
      "loss是： tensor(90.5670, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 33.14814814814815\n",
      "loss是： tensor(88.2178, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 21.311475409836067\n",
      "loss是： tensor(65.0454, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 33.442622950819676\n",
      "loss是： tensor(106.5871, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 28.4375\n",
      "loss是： tensor(93.4784, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 44.516129032258064\n",
      "loss是： tensor(100.5706, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 28.688524590163933\n",
      "loss是： tensor(97.0450, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 35.3125\n",
      "loss是： tensor(106.3557, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 27.746478873239436\n",
      "loss是： tensor(84.4317, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 20.0\n",
      "loss是： tensor(45.8810, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 27.966101694915256\n",
      "loss是： tensor(72.6688, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 25.416666666666664\n",
      "loss是： tensor(56.1390, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 37.61194029850746\n",
      "loss是： tensor(96.8927, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 33.333333333333336\n",
      "loss是： tensor(106.1409, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 49.34426229508197\n",
      "loss是： tensor(121.5712, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 33.857142857142854\n",
      "loss是： tensor(103.6640, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 60.16949152542373\n",
      "loss是： tensor(110.0805, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 30.0\n",
      "loss是： tensor(101.5571, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 34.12698412698413\n",
      "loss是： tensor(97.1823, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 8.833333333333332\n",
      "loss是： tensor(68.8442, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 46.0344827586207\n",
      "loss是： tensor(110.2242, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 9.84126984126984\n",
      "loss是： tensor(64.7553, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 28.42857142857143\n",
      "loss是： tensor(84.2910, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 32.98701298701299\n",
      "loss是： tensor(94.5145, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 27.166666666666668\n",
      "loss是： tensor(86.7600, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 43.389830508474574\n",
      "loss是： tensor(130.0283, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 36.39344262295082\n",
      "loss是： tensor(95.4095, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 25.78125\n",
      "loss是： tensor(78.1572, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 34.90909090909091\n",
      "loss是： tensor(97.0956, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 28.591549295774648\n",
      "loss是： tensor(124.5366, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 31.475409836065573\n",
      "loss是： tensor(102.9615, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 33.728813559322035\n",
      "loss是： tensor(81.0049, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 40.638297872340424\n",
      "loss是： tensor(125.6346, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 23.943661971830984\n",
      "loss是： tensor(104.9554, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 41.61290322580645\n",
      "loss是： tensor(90.2642, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 30.158730158730158\n",
      "loss是： tensor(82.1763, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 45.090909090909086\n",
      "loss是： tensor(145.7555, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 37.61194029850746\n",
      "loss是： tensor(98.4001, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 23.650793650793652\n",
      "loss是： tensor(95.9522, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 29.0625\n",
      "loss是： tensor(81.1399, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 36.5625\n",
      "loss是： tensor(120.0187, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 40.64516129032258\n",
      "loss是： tensor(102.2759, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 34.42622950819672\n",
      "loss是： tensor(86.9090, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 26.5625\n",
      "loss是： tensor(89.8768, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 29.655172413793103\n",
      "loss是： tensor(75.8127, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 37.77777777777778\n",
      "loss是： tensor(99.2305, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 70.83333333333333\n",
      "loss是： tensor(161.1676, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 28.955223880597014\n",
      "loss是： tensor(81.0394, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 7.611940298507463\n",
      "loss是： tensor(84.8876, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 35.65217391304348\n",
      "loss是： tensor(100.0094, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 36.37931034482759\n",
      "loss是： tensor(78.4055, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 30.677966101694913\n",
      "loss是： tensor(76.3225, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 30.985915492957744\n",
      "loss是： tensor(99.1390, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 34.0\n",
      "loss是： tensor(89.7325, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 32.64705882352941\n",
      "loss是： tensor(102.4968, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 35.14285714285714\n",
      "loss是： tensor(60.0231, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 30.56338028169014\n",
      "loss是： tensor(101.8449, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 24.516129032258064\n",
      "loss是： tensor(68.1152, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 43.650793650793645\n",
      "loss是： tensor(103.1493, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 34.827586206896555\n",
      "loss是： tensor(87.9786, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 30.153846153846153\n",
      "loss是： tensor(100.4706, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 27.878787878787882\n",
      "loss是： tensor(114.1181, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 29.122807017543863\n",
      "loss是： tensor(123.2054, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 34.82142857142857\n",
      "loss是： tensor(71.5771, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 9.0\n",
      "loss是： tensor(95.5469, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 8.115942028985508\n",
      "loss是： tensor(78.5716, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 26.818181818181817\n",
      "loss是： tensor(84.7318, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 40.32258064516129\n",
      "loss是： tensor(85.0756, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 31.03448275862069\n",
      "loss是： tensor(88.0388, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 63.230769230769226\n",
      "loss是： tensor(130.5205, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 35.370370370370374\n",
      "loss是： tensor(84.8101, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 24.153846153846153\n",
      "loss是： tensor(70.4681, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 36.36363636363636\n",
      "loss是： tensor(110.5439, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 46.46153846153847\n",
      "loss是： tensor(130.9374, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 38.87096774193549\n",
      "loss是： tensor(95.0144, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 28.615384615384617\n",
      "loss是： tensor(79.3479, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 32.794117647058826\n",
      "loss是： tensor(104.9085, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 38.392857142857146\n",
      "loss是： tensor(98.3408, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 35.0\n",
      "loss是： tensor(61.2676, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 33.04347826086956\n",
      "loss是： tensor(77.8159, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 32.5\n",
      "loss是： tensor(104.5516, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 34.84848484848485\n",
      "loss是： tensor(106.1588, device='cuda:0', grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert计算的loss是： 27.89473684210526\n",
      "loss是： tensor(72.7650, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 30.535714285714285\n",
      "loss是： tensor(75.6266, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 59.26470588235294\n",
      "loss是： tensor(153.1609, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 32.38095238095238\n",
      "loss是： tensor(69.1493, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 28.676470588235293\n",
      "loss是： tensor(71.9388, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 29.193548387096776\n",
      "loss是： tensor(75.7451, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 41.25\n",
      "loss是： tensor(97.4629, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 31.21212121212121\n",
      "loss是： tensor(107.8355, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 30.923076923076923\n",
      "loss是： tensor(87.3143, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 32.72727272727273\n",
      "loss是： tensor(90.6492, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 38.208955223880594\n",
      "loss是： tensor(109.1307, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 38.771929824561404\n",
      "loss是： tensor(99.1666, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 26.76470588235294\n",
      "loss是： tensor(107.6922, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 30.545454545454547\n",
      "loss是： tensor(68.2088, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 10.793650793650793\n",
      "loss是： tensor(77.3756, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 29.193548387096776\n",
      "loss是： tensor(82.1095, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 26.769230769230766\n",
      "loss是： tensor(81.4903, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 47.32142857142857\n",
      "loss是： tensor(99.1272, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 34.3859649122807\n",
      "loss是： tensor(102.8718, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 27.073170731707318\n",
      "loss是： tensor(68.5039, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 30.303030303030305\n",
      "loss是： tensor(103.8449, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 34.76190476190476\n",
      "loss是： tensor(88.7763, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 30.441176470588232\n",
      "loss是： tensor(78.1037, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 27.77777777777778\n",
      "loss是： tensor(84.9681, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 33.714285714285715\n",
      "loss是： tensor(80.7158, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 34.74576271186441\n",
      "loss是： tensor(82.8255, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 26.557377049180324\n",
      "loss是： tensor(67.2178, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 70.17241379310346\n",
      "loss是： tensor(139.4871, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 28.923076923076923\n",
      "loss是： tensor(93.5861, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 32.5\n",
      "loss是： tensor(86.8622, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 41.47540983606557\n",
      "loss是： tensor(149.2924, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 28.194444444444446\n",
      "loss是： tensor(85.6189, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 24.107142857142854\n",
      "loss是： tensor(76.8704, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 32.142857142857146\n",
      "loss是： tensor(91.1399, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 29.857142857142858\n",
      "loss是： tensor(89.0292, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 32.64705882352941\n",
      "loss是： tensor(96.2890, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 32.698412698412696\n",
      "loss是： tensor(101.8944, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 27.28813559322034\n",
      "loss是： tensor(71.0869, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 32.58620689655173\n",
      "loss是： tensor(110.8370, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 47.91044776119403\n",
      "loss是： tensor(138.0538, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 30.338983050847457\n",
      "loss是： tensor(102.2365, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 35.14285714285714\n",
      "loss是： tensor(114.7151, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 7.560975609756097\n",
      "loss是： tensor(46.2856, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "290/300\n",
      "bert计算的loss是： 42.06896551724138\n",
      "loss是： tensor(93.5679, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 31.566265060240966\n",
      "loss是： tensor(116.7229, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 55.625\n",
      "loss是： tensor(125.6730, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 28.24561403508772\n",
      "loss是： tensor(100.6723, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 22.90909090909091\n",
      "loss是： tensor(80.2888, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 45.2112676056338\n",
      "loss是： tensor(116.5962, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 40.847457627118644\n",
      "loss是： tensor(98.5560, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 37.096774193548384\n",
      "loss是： tensor(95.4952, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 34.52830188679245\n",
      "loss是： tensor(78.8988, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 28.125\n",
      "loss是： tensor(95.7332, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 23.442622950819676\n",
      "loss是： tensor(77.7409, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 27.692307692307693\n",
      "loss是： tensor(83.8720, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 34.21875\n",
      "loss是： tensor(89.5582, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 10.588235294117647\n",
      "loss是： tensor(93.5517, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 36.29032258064516\n",
      "loss是： tensor(106.3263, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 35.833333333333336\n",
      "loss是： tensor(95.5430, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 40.0\n",
      "loss是： tensor(98.5978, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 34.642857142857146\n",
      "loss是： tensor(92.6040, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 30.666666666666668\n",
      "loss是： tensor(91.3762, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 36.27450980392157\n",
      "loss是： tensor(80.4947, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 33.59375\n",
      "loss是： tensor(76.2969, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 30.441176470588232\n",
      "loss是： tensor(75.6011, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 34.25\n",
      "loss是： tensor(81.4278, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 32.8125\n",
      "loss是： tensor(84.8893, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 26.129032258064516\n",
      "loss是： tensor(57.5568, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 31.896551724137932\n",
      "loss是： tensor(93.8388, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 25.357142857142854\n",
      "loss是： tensor(60.7855, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 38.42857142857143\n",
      "loss是： tensor(128.7968, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 16.056338028169016\n",
      "loss是： tensor(90.2678, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 44.05797101449275\n",
      "loss是： tensor(105.3825, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 32.87878787878788\n",
      "loss是： tensor(100.6122, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 34.21052631578947\n",
      "loss是： tensor(96.1314, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 31.724137931034484\n",
      "loss是： tensor(72.9891, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 31.29032258064516\n",
      "loss是： tensor(89.6773, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 36.95652173913044\n",
      "loss是： tensor(106.8776, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 40.0\n",
      "loss是： tensor(86.3539, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 32.06349206349206\n",
      "loss是： tensor(100.8272, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 30.64516129032258\n",
      "loss是： tensor(88.9839, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 34.90566037735849\n",
      "loss是： tensor(83.4645, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 33.833333333333336\n",
      "loss是： tensor(93.1307, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 30.491803278688522\n",
      "loss是： tensor(75.4321, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 42.75862068965517\n",
      "loss是： tensor(92.2273, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 40.70422535211268\n",
      "loss是： tensor(96.5334, device='cuda:0', grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert计算的loss是： 30.806451612903224\n",
      "loss是： tensor(82.2319, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 25.0\n",
      "loss是： tensor(73.5348, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 26.73913043478261\n",
      "loss是： tensor(75.0752, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 36.1764705882353\n",
      "loss是： tensor(105.3326, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 31.285714285714285\n",
      "loss是： tensor(117.2055, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 23.484848484848484\n",
      "loss是： tensor(67.6938, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 34.92957746478873\n",
      "loss是： tensor(118.2343, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 16.785714285714285\n",
      "loss是： tensor(88.6550, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 27.666666666666664\n",
      "loss是： tensor(76.6938, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 30.0\n",
      "loss是： tensor(99.2759, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 31.71875\n",
      "loss是： tensor(92.4071, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 37.6271186440678\n",
      "loss是： tensor(109.9147, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 31.492537313432834\n",
      "loss是： tensor(94.7630, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 41.206896551724135\n",
      "loss是： tensor(105.1635, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 15.0\n",
      "loss是： tensor(88.7132, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 10.350877192982457\n",
      "loss是： tensor(70.0480, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 32.096774193548384\n",
      "loss是： tensor(91.2331, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 26.22950819672131\n",
      "loss是： tensor(100.9696, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 46.46153846153847\n",
      "loss是： tensor(100.4835, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 34.35483870967742\n",
      "loss是： tensor(83.0396, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 24.375\n",
      "loss是： tensor(98.4407, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 48.41269841269841\n",
      "loss是： tensor(130.4038, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 34.516129032258064\n",
      "loss是： tensor(105.6470, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 31.940298507462686\n",
      "loss是： tensor(101.9315, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 57.368421052631575\n",
      "loss是： tensor(118.9059, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 36.74418604651163\n",
      "loss是： tensor(74.6876, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 26.896551724137932\n",
      "loss是： tensor(83.1695, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 34.56140350877193\n",
      "loss是： tensor(99.0500, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 32.295081967213115\n",
      "loss是： tensor(79.2363, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 13.076923076923077\n",
      "loss是： tensor(73.4433, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 11.75438596491228\n",
      "loss是： tensor(56.2227, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 31.666666666666664\n",
      "loss是： tensor(85.3833, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 39.696969696969695\n",
      "loss是： tensor(113.7952, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 33.57142857142857\n",
      "loss是： tensor(116.0195, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 28.243243243243242\n",
      "loss是： tensor(98.9517, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 31.636363636363637\n",
      "loss是： tensor(84.0402, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 50.0\n",
      "loss是： tensor(123.5773, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 26.0\n",
      "loss是： tensor(78.2202, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 70.0\n",
      "loss是： tensor(157.4266, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 37.01492537313433\n",
      "loss是： tensor(108.4578, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 24.0\n",
      "loss是： tensor(84.3704, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 29.166666666666664\n",
      "loss是： tensor(85.1193, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 37.5\n",
      "loss是： tensor(94.0856, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 33.0\n",
      "loss是： tensor(91.9571, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 35.15151515151515\n",
      "loss是： tensor(100.4932, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 25.714285714285715\n",
      "loss是： tensor(93.4648, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 27.666666666666664\n",
      "loss是： tensor(102.6292, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 42.42424242424242\n",
      "loss是： tensor(125.3706, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 58.484848484848484\n",
      "loss是： tensor(93.1971, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 57.67857142857143\n",
      "loss是： tensor(115.3227, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 26.333333333333332\n",
      "loss是： tensor(85.0366, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 34.57142857142857\n",
      "loss是： tensor(85.2838, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 30.16949152542373\n",
      "loss是： tensor(86.7138, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 34.827586206896555\n",
      "loss是： tensor(99.9198, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 29.846153846153847\n",
      "loss是： tensor(88.8968, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 37.12121212121212\n",
      "loss是： tensor(109.9456, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 40.79365079365079\n",
      "loss是： tensor(100.8885, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 30.41095890410959\n",
      "loss是： tensor(93.4019, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 36.206896551724135\n",
      "loss是： tensor(95.4905, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 1.267605633802817\n",
      "loss是： tensor(77.3840, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 38.59375\n",
      "loss是： tensor(106.1776, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 27.761194029850746\n",
      "loss是： tensor(62.5896, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 33.6764705882353\n",
      "loss是： tensor(88.0020, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 38.68852459016394\n",
      "loss是： tensor(91.8233, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 40.81967213114754\n",
      "loss是： tensor(102.4507, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 34.82142857142857\n",
      "loss是： tensor(98.8798, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 32.06349206349206\n",
      "loss是： tensor(77.9711, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 31.052631578947366\n",
      "loss是： tensor(87.5233, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 35.39682539682539\n",
      "loss是： tensor(107.9960, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 33.970588235294116\n",
      "loss是： tensor(98.6174, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 28.545454545454547\n",
      "loss是： tensor(95.6030, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 33.48837209302326\n",
      "loss是： tensor(53.3166, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 58.484848484848484\n",
      "loss是： tensor(127.4576, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 29.0\n",
      "loss是： tensor(71.2159, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 24.705882352941178\n",
      "loss是： tensor(72.4833, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 30.454545454545453\n",
      "loss是： tensor(74.3907, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 42.90322580645161\n",
      "loss是： tensor(99.3524, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 28.983050847457626\n",
      "loss是： tensor(76.3772, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 32.0\n",
      "loss是： tensor(74.4060, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 32.87878787878788\n",
      "loss是： tensor(81.3734, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 35.714285714285715\n",
      "loss是： tensor(91.8297, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 34.44444444444444\n",
      "loss是： tensor(90.2497, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 21.69230769230769\n",
      "loss是： tensor(58.6111, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 33.60655737704918\n",
      "loss是： tensor(98.2229, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 39.31506849315069\n",
      "loss是： tensor(132.4948, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 31.428571428571427\n",
      "loss是： tensor(75.2971, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 47.01754385964912\n",
      "loss是： tensor(130.7353, device='cuda:0', grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert计算的loss是： 22.80701754385965\n",
      "loss是： tensor(66.0438, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 34.30769230769231\n",
      "loss是： tensor(90.6456, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 34.42622950819672\n",
      "loss是： tensor(111.4881, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 38.80597014925373\n",
      "loss是： tensor(89.9778, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 29.18918918918919\n",
      "loss是： tensor(103.9123, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 33.93939393939394\n",
      "loss是： tensor(92.3221, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 39.25373134328358\n",
      "loss是： tensor(84.6911, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 24.75\n",
      "loss是： tensor(44.6801, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 36.98412698412699\n",
      "loss是： tensor(87.5853, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 43.89830508474576\n",
      "loss是： tensor(104.0552, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 33.492063492063494\n",
      "loss是： tensor(99.8716, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 28.059701492537314\n",
      "loss是： tensor(94.5489, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 8.813559322033898\n",
      "loss是： tensor(74.7239, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 34.5\n",
      "loss是： tensor(77.8556, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 36.865671641791046\n",
      "loss是： tensor(115.0721, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 30.333333333333332\n",
      "loss是： tensor(72.8607, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 28.88888888888889\n",
      "loss是： tensor(113.1609, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 32.27272727272727\n",
      "loss是： tensor(84.3564, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 40.0\n",
      "loss是： tensor(98.8591, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 31.515151515151516\n",
      "loss是： tensor(80.7982, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 24.626865671641788\n",
      "loss是： tensor(81.0127, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 36.98412698412699\n",
      "loss是： tensor(130.2875, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 20.952380952380953\n",
      "loss是： tensor(80.5275, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 37.37704918032787\n",
      "loss是： tensor(89.2975, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 35.08196721311475\n",
      "loss是： tensor(85.0670, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 40.0\n",
      "loss是： tensor(88.8055, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 29.516129032258064\n",
      "loss是： tensor(112.8126, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 59.726027397260275\n",
      "loss是： tensor(122.4941, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 25.901639344262296\n",
      "loss是： tensor(97.1647, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 30.19607843137255\n",
      "loss是： tensor(94.6852, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 25.263157894736842\n",
      "loss是： tensor(56.8325, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 25.423728813559322\n",
      "loss是： tensor(86.7908, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 29.054054054054053\n",
      "loss是： tensor(84.6293, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 54.7887323943662\n",
      "loss是： tensor(104.8759, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 30.327868852459016\n",
      "loss是： tensor(80.6323, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 35.294117647058826\n",
      "loss是： tensor(107.1582, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 35.53846153846153\n",
      "loss是： tensor(104.0457, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 32.41379310344827\n",
      "loss是： tensor(84.5676, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 31.186440677966104\n",
      "loss是： tensor(93.8097, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 40.151515151515156\n",
      "loss是： tensor(86.4862, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 31.23076923076923\n",
      "loss是： tensor(80.2850, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 31.875\n",
      "loss是： tensor(70.8058, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 30.0\n",
      "loss是： tensor(83.7801, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 28.47457627118644\n",
      "loss是： tensor(80.9064, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 33.84615384615385\n",
      "loss是： tensor(89.8391, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 28.793103448275865\n",
      "loss是： tensor(68.5691, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 27.16417910447761\n",
      "loss是： tensor(77.0114, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 28.727272727272727\n",
      "loss是： tensor(87.0700, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 31.333333333333332\n",
      "loss是： tensor(78.2911, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 29.215686274509803\n",
      "loss是： tensor(69.5990, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 2.631578947368421\n",
      "loss是： tensor(64.6729, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 32.280701754385966\n",
      "loss是： tensor(83.8374, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 44.50704225352113\n",
      "loss是： tensor(132.4369, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 38.57142857142857\n",
      "loss是： tensor(77.0021, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 36.0\n",
      "loss是： tensor(111.7974, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 27.454545454545457\n",
      "loss是： tensor(79.2269, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 44.909090909090914\n",
      "loss是： tensor(121.0859, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 49.61538461538461\n",
      "loss是： tensor(101.6221, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 13.934426229508198\n",
      "loss是： tensor(71.2571, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 26.0\n",
      "loss是： tensor(71.1820, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 23.38709677419355\n",
      "loss是： tensor(66.4960, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 28.852459016393443\n",
      "loss是： tensor(59.1880, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 28.928571428571427\n",
      "loss是： tensor(94.8775, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 26.103896103896105\n",
      "loss是： tensor(107.9189, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 26.29032258064516\n",
      "loss是： tensor(116.0500, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 19.322033898305083\n",
      "loss是： tensor(65.7043, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 14.328358208955223\n",
      "loss是： tensor(63.5750, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 25.967741935483872\n",
      "loss是： tensor(109.4664, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 29.137931034482758\n",
      "loss是： tensor(66.4719, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 45.689655172413794\n",
      "loss是： tensor(114.6460, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 48.75\n",
      "loss是： tensor(121.3370, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 38.275862068965516\n",
      "loss是： tensor(108.1173, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 33.10344827586207\n",
      "loss是： tensor(86.7712, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 30.333333333333332\n",
      "loss是： tensor(84.3341, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 54.17910447761194\n",
      "loss是： tensor(100.9259, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 40.1639344262295\n",
      "loss是： tensor(87.7260, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bert计算的loss是： 41.11111111111111\n",
      "loss是： tensor(76.7693, device='cuda:0', grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--model_dir', type=str, default=\"model/\")\n",
    "    parser.add_argument('--batch_size', type=int, default=128)\n",
    "    parser.add_argument('--epochs', type=int, default=300)\n",
    "    parser.add_argument('--runs', type=int, default=5)\n",
    "    parser.add_argument('--domain', type=str, default=\"laptop\")\n",
    "    parser.add_argument('--data_dir', type=str, default=\"data/prep_data/\")\n",
    "    parser.add_argument('--valid', type=int, default=150) #number of validation data.\n",
    "    parser.add_argument('--lr', type=float, default=0.0001)\n",
    "    parser.add_argument('--dropout', type=float, default=0.55)\n",
    "    parser.add_argument('--crf', type=bool, default=True)\n",
    "    parser.add_argument('--generate_data', type=bool, default=False)\n",
    "    parser.add_argument('--add_num_loss', type=bool, default=True)\n",
    "    args = parser.parse_known_args()[0]\n",
    "\n",
    "    run(args.domain, args.data_dir, args.model_dir, args.valid, args.runs, args.epochs, args.lr, args.dropout, args.batch_size, args.crf, args.generate_data, args.add_num_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_0.4",
   "language": "python",
   "name": "pytorch_0.4"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "274.667px",
    "left": "454.333px",
    "right": "20px",
    "top": "136px",
    "width": "550px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
