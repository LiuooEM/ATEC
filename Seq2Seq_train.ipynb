{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-26T09:14:38.362920Z",
     "start_time": "2021-05-26T09:14:37.901072Z"
    }
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "import torch\n",
    "import time\n",
    "import json\n",
    "import numpy as np\n",
    "import math\n",
    "import random\n",
    "import codecs\n",
    "import os\n",
    "from config import flags\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-26T09:14:38.366650Z",
     "start_time": "2021-05-26T09:14:38.363933Z"
    }
   },
   "outputs": [],
   "source": [
    "seed = 34\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "random_state = seed\n",
    "flags.seed = seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-26T09:14:38.373945Z",
     "start_time": "2021-05-26T09:14:38.367945Z"
    }
   },
   "outputs": [],
   "source": [
    "def batch_generator(X, y, batch_size=128, return_idx=False, crf=False):\n",
    "    for offset in range(0, X.shape[0], batch_size):\n",
    "        batch_X_len=np.sum(X[offset:offset+batch_size]!=0, axis=1)\n",
    "        batch_idx=batch_X_len.argsort()[::-1]\n",
    "        batch_X_len=batch_X_len[batch_idx]\n",
    "        batch_X_mask=(X[offset:offset+batch_size]!=0)[batch_idx].astype(np.uint8)\n",
    "        batch_X=X[offset:offset+batch_size][batch_idx] \n",
    "        batch_y=y[offset:offset+batch_size][batch_idx]\n",
    "        batch_X = torch.from_numpy(batch_X).long().cuda()\n",
    "        batch_X_mask=torch.from_numpy(batch_X_mask).long().cuda()\n",
    "        batch_y = torch.from_numpy(batch_y).long().cuda()\n",
    "        if len(batch_y.size())==2 and not crf:\n",
    "            batch_y=torch.nn.utils.rnn.pack_padded_sequence(batch_y, batch_X_len, batch_first=True)\n",
    "        if return_idx:\n",
    "            yield (batch_X, batch_y, batch_X_len, batch_X_mask, batch_idx)\n",
    "        else:\n",
    "            yield (batch_X, batch_y, batch_X_len, batch_X_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-26T09:14:38.378683Z",
     "start_time": "2021-05-26T09:14:38.375339Z"
    }
   },
   "outputs": [],
   "source": [
    "def valid_loss(model, valid_X, valid_y, batch_size=16, crf=False):\n",
    "    model.eval()\n",
    "    losses=[]\n",
    "    for batch in batch_generator(valid_X, valid_y, batch_size=batch_size, crf=crf):\n",
    "        batch_valid_X, batch_valid_y, batch_valid_X_len, batch_X_mask = batch\n",
    "        loss=model(batch_valid_X, batch_valid_X_len, batch_valid_y)\n",
    "        losses.append(loss.item())\n",
    "    model.train()\n",
    "    return sum(losses)/len(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-26T09:14:38.381703Z",
     "start_time": "2021-05-26T09:14:38.379463Z"
    }
   },
   "outputs": [],
   "source": [
    "from model import Encoder, Decoder, Seq2Seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-26T09:14:38.386696Z",
     "start_time": "2021-05-26T09:14:38.382429Z"
    }
   },
   "outputs": [],
   "source": [
    "def train(train_X, train_y, valid_X, valid_y, model, model_fn, optimizer, parameters, epochs, batch_size, run_epoch, crf):\n",
    "    best_loss=float(\"inf\")\n",
    "    valid_history=[]\n",
    "    train_history=[]\n",
    "    for epoch in range(epochs):\n",
    "        pred_y=np.zeros((train_X.shape[0], train_X.shape[1]), np.int16)\n",
    "        offset = range(0, train_X.shape[0], batch_size)\n",
    "        i_th = 0\n",
    "        results = []\n",
    "        for batch in batch_generator(train_X, train_y, batch_size, crf=crf):\n",
    "            batch_train_X, batch_train_y, batch_train_X_len, batch_train_X_mask=batch\n",
    "            loss = model(batch_train_X, batch_train_X_len, batch_train_y)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm(parameters, 1.)\n",
    "            optimizer.step()\n",
    "        loss=valid_loss(model, train_X, train_y, batch_size, crf=crf)\n",
    "        train_history.append(loss)\n",
    "        loss=valid_loss(model, valid_X, valid_y, batch_size, crf=crf)\n",
    "        valid_history.append(loss)\n",
    "        if loss<best_loss:\n",
    "            best_loss=loss\n",
    "            torch.save(model, model_fn)\n",
    "        shuffle_idx=np.random.permutation(len(train_X))\n",
    "        train_X=train_X[shuffle_idx]\n",
    "        train_y=train_y[shuffle_idx]\n",
    "        print(str(epoch) + '/' + str(epochs))\n",
    "        \n",
    "    model=torch.load(model_fn)\n",
    "    return train_history, valid_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-26T09:14:38.392804Z",
     "start_time": "2021-05-26T09:14:38.387451Z"
    }
   },
   "outputs": [],
   "source": [
    "def run(domain, data_dir, model_dir, valid_split, epochs, lr, dropout, batch_size, runs, crf):\n",
    "    gen_emb=np.load(data_dir+\"gen.vec.npy\")\n",
    "    ae_data=np.load(data_dir+domain+\".npz\")\n",
    "    train_data = ae_data['train_X']\n",
    "    train_label = ae_data['train_y']\n",
    "    \"\"\"\n",
    "    train_X, valid_X, train_y, valid_y = train_test_split(train_data,\n",
    "                                                          train_label,\n",
    "                                                          test_size = valid_split,\n",
    "                                                          random_state = random_state)\n",
    "    \"\"\"\n",
    "    idx = np.arange(ae_data['train_X'].shape[0])\n",
    "    np.random.shuffle(idx)\n",
    "    valid_X=train_data[-valid_split:]\n",
    "    valid_y=train_label[-valid_split:]\n",
    "    train_X=train_data[:-valid_split]\n",
    "    train_y=train_label[:-valid_split]\n",
    "    \n",
    "    print(\"数据集总大小：\", len(ae_data['train_X']))\n",
    "    print(\"训练集大小：\", len(train_X))\n",
    "    print(\"验证集大小：\", len(valid_X))\n",
    "\n",
    "    for r in range(runs):\n",
    "        print('正在训练第 ' + str(r + 1) + '轮')\n",
    "        flags.model_dir = model_dir\n",
    "        flags.batch_size = batch_size\n",
    "        flags.epochs = epochs\n",
    "        flags.data_dir = data_dir\n",
    "        flags.lr = lr\n",
    "        flags.dropout = dropout\n",
    "        encoder = Encoder(gen_emb, flags)\n",
    "        decoder = Decoder(flags)\n",
    "        model   = Seq2Seq(encoder, decoder, flags)\n",
    "        model.cuda()\n",
    "        print(model)\n",
    "    \n",
    "        parameters = [p for p in model.parameters() if p.requires_grad]\n",
    "        optimizer=torch.optim.Adam(parameters, lr=lr)\n",
    "        train_history, valid_history = train(train_X, train_y, valid_X, valid_y, model, model_dir+domain+str(r), \n",
    "                                           optimizer, parameters, epochs, batch_size, r, crf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-26T09:20:18.205774Z",
     "start_time": "2021-05-26T09:14:38.394457Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "数据集总大小： 3045\n",
      "训练集大小： 2895\n",
      "验证集大小： 150\n",
      "正在训练第 1轮\n",
      "Seq2Seq(\n",
      "  (encoder): Encoder(\n",
      "    (gen_embedding): Embedding(8518, 300)\n",
      "    (dropout): Dropout(p=0.5, inplace=False)\n",
      "    (rnn): GRU(300, 300, num_layers=2, bidirectional=True)\n",
      "  )\n",
      "  (decoder): Decoder(\n",
      "    (label_embedding): Embedding(3, 50)\n",
      "    (dropout): Dropout(p=0.5, inplace=False)\n",
      "    (attention): Attention(\n",
      "      (attn): Linear(in_features=900, out_features=300, bias=True)\n",
      "    )\n",
      "    (rnn): GRU(650, 300, num_layers=2)\n",
      "    (hidden2label): Linear(in_features=300, out_features=3, bias=True)\n",
      "    (transformer): Linear(in_features=600, out_features=300, bias=True)\n",
      "    (transformer1): Linear(in_features=300, out_features=300, bias=True)\n",
      "    (gate): Linear(in_features=300, out_features=300, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xinke901/lcx/5.25/model.py:173: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  score  = torch.nn.functional.nll_loss(torch.nn.functional.log_softmax(packed_y.data), target_.data)\n",
      "/home/xinke901/anaconda3/envs/de-cnn/lib/python3.7/site-packages/ipykernel_launcher.py:15: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/5\n",
      "1/5\n",
      "2/5\n",
      "3/5\n",
      "4/5\n",
      "正在训练第 2轮\n",
      "Seq2Seq(\n",
      "  (encoder): Encoder(\n",
      "    (gen_embedding): Embedding(8518, 300)\n",
      "    (dropout): Dropout(p=0.5, inplace=False)\n",
      "    (rnn): GRU(300, 300, num_layers=2, bidirectional=True)\n",
      "  )\n",
      "  (decoder): Decoder(\n",
      "    (label_embedding): Embedding(3, 50)\n",
      "    (dropout): Dropout(p=0.5, inplace=False)\n",
      "    (attention): Attention(\n",
      "      (attn): Linear(in_features=900, out_features=300, bias=True)\n",
      "    )\n",
      "    (rnn): GRU(650, 300, num_layers=2)\n",
      "    (hidden2label): Linear(in_features=300, out_features=3, bias=True)\n",
      "    (transformer): Linear(in_features=600, out_features=300, bias=True)\n",
      "    (transformer1): Linear(in_features=300, out_features=300, bias=True)\n",
      "    (gate): Linear(in_features=300, out_features=300, bias=True)\n",
      "  )\n",
      ")\n",
      "0/5\n",
      "1/5\n",
      "2/5\n",
      "3/5\n",
      "4/5\n",
      "正在训练第 3轮\n",
      "Seq2Seq(\n",
      "  (encoder): Encoder(\n",
      "    (gen_embedding): Embedding(8518, 300)\n",
      "    (dropout): Dropout(p=0.5, inplace=False)\n",
      "    (rnn): GRU(300, 300, num_layers=2, bidirectional=True)\n",
      "  )\n",
      "  (decoder): Decoder(\n",
      "    (label_embedding): Embedding(3, 50)\n",
      "    (dropout): Dropout(p=0.5, inplace=False)\n",
      "    (attention): Attention(\n",
      "      (attn): Linear(in_features=900, out_features=300, bias=True)\n",
      "    )\n",
      "    (rnn): GRU(650, 300, num_layers=2)\n",
      "    (hidden2label): Linear(in_features=300, out_features=3, bias=True)\n",
      "    (transformer): Linear(in_features=600, out_features=300, bias=True)\n",
      "    (transformer1): Linear(in_features=300, out_features=300, bias=True)\n",
      "    (gate): Linear(in_features=300, out_features=300, bias=True)\n",
      "  )\n",
      ")\n",
      "0/5\n",
      "1/5\n",
      "2/5\n",
      "3/5\n",
      "4/5\n",
      "正在训练第 4轮\n",
      "Seq2Seq(\n",
      "  (encoder): Encoder(\n",
      "    (gen_embedding): Embedding(8518, 300)\n",
      "    (dropout): Dropout(p=0.5, inplace=False)\n",
      "    (rnn): GRU(300, 300, num_layers=2, bidirectional=True)\n",
      "  )\n",
      "  (decoder): Decoder(\n",
      "    (label_embedding): Embedding(3, 50)\n",
      "    (dropout): Dropout(p=0.5, inplace=False)\n",
      "    (attention): Attention(\n",
      "      (attn): Linear(in_features=900, out_features=300, bias=True)\n",
      "    )\n",
      "    (rnn): GRU(650, 300, num_layers=2)\n",
      "    (hidden2label): Linear(in_features=300, out_features=3, bias=True)\n",
      "    (transformer): Linear(in_features=600, out_features=300, bias=True)\n",
      "    (transformer1): Linear(in_features=300, out_features=300, bias=True)\n",
      "    (gate): Linear(in_features=300, out_features=300, bias=True)\n",
      "  )\n",
      ")\n",
      "0/5\n",
      "1/5\n",
      "2/5\n",
      "3/5\n",
      "4/5\n",
      "正在训练第 5轮\n",
      "Seq2Seq(\n",
      "  (encoder): Encoder(\n",
      "    (gen_embedding): Embedding(8518, 300)\n",
      "    (dropout): Dropout(p=0.5, inplace=False)\n",
      "    (rnn): GRU(300, 300, num_layers=2, bidirectional=True)\n",
      "  )\n",
      "  (decoder): Decoder(\n",
      "    (label_embedding): Embedding(3, 50)\n",
      "    (dropout): Dropout(p=0.5, inplace=False)\n",
      "    (attention): Attention(\n",
      "      (attn): Linear(in_features=900, out_features=300, bias=True)\n",
      "    )\n",
      "    (rnn): GRU(650, 300, num_layers=2)\n",
      "    (hidden2label): Linear(in_features=300, out_features=3, bias=True)\n",
      "    (transformer): Linear(in_features=600, out_features=300, bias=True)\n",
      "    (transformer1): Linear(in_features=300, out_features=300, bias=True)\n",
      "    (gate): Linear(in_features=300, out_features=300, bias=True)\n",
      "  )\n",
      ")\n",
      "0/5\n",
      "1/5\n",
      "2/5\n",
      "3/5\n",
      "4/5\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--model_dir', type=str, default=\"model/Seq2Seq/\")\n",
    "    parser.add_argument('--batch_size', type=int, default=128)\n",
    "    parser.add_argument('--epochs', type=int, default=5)\n",
    "    parser.add_argument('--runs', type=int, default=5)\n",
    "    parser.add_argument('--domain', type=str, default=\"laptop\")\n",
    "    parser.add_argument('--data_dir', type=str, default=\"data/prep_data/\")\n",
    "    parser.add_argument('--valid', type=int, default=150)\n",
    "    parser.add_argument('--lr', type=float, default=0.001)\n",
    "    parser.add_argument('--dropout', type=float, default=0.5)\n",
    "    parser.add_argument('--crf', type=bool, default=False)\n",
    "    args = parser.parse_known_args()[0]\n",
    "\n",
    "    run(args.domain, args.data_dir, args.model_dir, args.valid, args.epochs, args.lr, args.dropout, args.batch_size, args.runs, args.crf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "de-cnn",
   "language": "python",
   "name": "de-cnn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "274.667px",
    "left": "454.333px",
    "right": "20px",
    "top": "136px",
    "width": "550px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
