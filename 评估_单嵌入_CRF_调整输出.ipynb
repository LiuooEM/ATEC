{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-04T10:27:53.945023Z",
     "start_time": "2021-01-04T10:27:51.718665Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xinke901/anaconda3/envs/de-cnn/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/xinke901/anaconda3/envs/de-cnn/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/xinke901/anaconda3/envs/de-cnn/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/xinke901/anaconda3/envs/de-cnn/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/xinke901/anaconda3/envs/de-cnn/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/xinke901/anaconda3/envs/de-cnn/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/xinke901/anaconda3/envs/de-cnn/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/xinke901/anaconda3/envs/de-cnn/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/xinke901/anaconda3/envs/de-cnn/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/xinke901/anaconda3/envs/de-cnn/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/xinke901/anaconda3/envs/de-cnn/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/xinke901/anaconda3/envs/de-cnn/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import torch\n",
    "import time\n",
    "import json\n",
    "import numpy as np\n",
    "import math\n",
    "import random\n",
    "import xml.etree.ElementTree as ET\n",
    "from subprocess import check_output\n",
    "from utils_test import test_as, generate_repositioning_test_data, load_data_test, squad_test_data, save_test, predict_boundary_test_result\n",
    "from generate_test_pred import generate_test_pred\n",
    "from generate_num_pred_label import generate_num_test_data, json_to_csv, predict_test_label, return_predicted_test_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-04T10:27:53.948718Z",
     "start_time": "2021-01-04T10:27:53.946052Z"
    }
   },
   "outputs": [],
   "source": [
    "np.random.seed(1337)\n",
    "random.seed(1337)\n",
    "torch.manual_seed(1337)\n",
    "torch.cuda.manual_seed(1337)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-04T10:27:53.956865Z",
     "start_time": "2021-01-04T10:27:53.949986Z"
    }
   },
   "outputs": [],
   "source": [
    "class Model(torch.nn.Module):\n",
    "    #定义神经网络层\n",
    "    def __init__(self, gen_emb, domain_emb, num_classes=3, dropout=0.5, crf=True):\n",
    "        super(Model, self).__init__()\n",
    "        #构建通用领域嵌入矩阵，先生成相应大小的空间，再将参数传进去\n",
    "        self.gen_embedding = torch.nn.Embedding(gen_emb.shape[0], gen_emb.shape[1])\n",
    "        self.gen_embedding.weight=torch.nn.Parameter(torch.from_numpy(gen_emb), requires_grad=False)\n",
    "        #构建特定领域嵌入矩阵，先生成相应大小的空间，再将参数传进去\n",
    "        \n",
    "        #设定模型的卷积层参数\n",
    "        #第一层第一种卷积核,输入维度是400，输出维度是128，跨步是5，填充是2\n",
    "        self.conv1=torch.nn.Conv1d(gen_emb.shape[1], 128, 5, padding=2)\n",
    "        self.conv2=torch.nn.Conv1d(gen_emb.shape[1], 128, 3, padding=1)\n",
    "        self.dropout=torch.nn.Dropout(dropout)\n",
    "        \n",
    "        #剩下三层的参数相同，输入维度是256，输出维度是128，跨步是5，填充是2\n",
    "        self.conv3=torch.nn.Conv1d(256, 256, 5, padding=2)\n",
    "        self.conv4=torch.nn.Conv1d(256, 256, 5, padding=2)\n",
    "        self.conv5=torch.nn.Conv1d(256, 256, 5, padding=2)\n",
    "        #设置解码层1：就是一个全连接层进行分类，分为3类\n",
    "        self.linear_ae=torch.nn.Linear(256, num_classes)\n",
    "        self.crf_flag=crf\n",
    "        #设置解码层2：CRF\n",
    "        if self.crf_flag:\n",
    "            from allennlp.modules import ConditionalRandomField\n",
    "            self.crf=ConditionalRandomField(num_classes)\n",
    "          \n",
    "    #定义操作\n",
    "    def forward(self, x, x_len, x_mask, x_tag=None, testing=True):\n",
    "        #cat是拼接的意思，0是竖着拼，1是横着拼，2是（3维的情况）\n",
    "        #这段代码好像是传入batchsize的单词的编号，然后去除这些单词嵌入\n",
    "        x_emb=self.gen_embedding(x)\n",
    "        x_emb=self.dropout(x_emb).transpose(1, 2)\n",
    "        x_conv=torch.nn.functional.relu(torch.cat((self.conv1(x_emb), \n",
    "                                                   self.conv2(x_emb)), dim=1))\n",
    "        \n",
    "        x_conv=self.dropout(x_conv)\n",
    "        x_conv=torch.nn.functional.relu(self.conv3(x_conv))\n",
    "        x_conv=self.dropout(x_conv)\n",
    "        x_conv=torch.nn.functional.relu(self.conv4(x_conv))\n",
    "        x_conv=self.dropout(x_conv)\n",
    "        x_conv=torch.nn.functional.relu(self.conv5(x_conv))\n",
    "        x_conv=x_conv.transpose(1, 2)\n",
    "        x_logit=self.linear_ae(x_conv)\n",
    "        #如果选择testing，就是输出预测值\n",
    "        if testing:\n",
    "            if self.crf_flag:\n",
    "                score=self.crf.viterbi_tags(x_logit, x_mask)\n",
    "            else:\n",
    "                x_logit=x_logit.transpose(2, 0)\n",
    "                score=torch.nn.functional.log_softmax(x_logit).transpose(2, 0)\n",
    "        else:\n",
    "            if self.crf_flag:\n",
    "                score=-self.crf(x_logit, x_tag, x_mask)\n",
    "            else:\n",
    "                x_logit=torch.nn.utils.rnn.pack_padded_sequence(x_logit, x_len, batch_first=True)\n",
    "                score=torch.nn.functional.nll_loss(torch.nn.functional.log_softmax(x_logit.data), x_tag.data)\n",
    "        return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-04T10:27:53.964615Z",
     "start_time": "2021-01-04T10:27:53.957744Z"
    }
   },
   "outputs": [],
   "source": [
    "def label_rest_xml(fn, output_fn, corpus, label):\n",
    "    dom=ET.parse(fn)\n",
    "    root=dom.getroot()\n",
    "    pred_y=[]\n",
    "    for zx, sent in enumerate(root.iter(\"sentence\") ) :\n",
    "        tokens=corpus[zx]\n",
    "        lb=label[zx]\n",
    "        opins=ET.Element(\"Opinions\")\n",
    "        token_idx, pt, tag_on=0, 0, False\n",
    "        start, end=-1, -1\n",
    "        for ix, c in enumerate(sent.find('text').text):\n",
    "            if token_idx<len(tokens) and pt>=len(tokens[token_idx] ):\n",
    "                pt=0\n",
    "                token_idx+=1\n",
    "\n",
    "            if token_idx<len(tokens) and lb[token_idx]==1 and pt==0 and c!=' ':\n",
    "                if tag_on:\n",
    "                    end=ix\n",
    "                    tag_on=False\n",
    "                    opin=ET.Element(\"Opinion\")\n",
    "                    opin.attrib['target']=sent.find('text').text[start:end]\n",
    "                    opin.attrib['from']=str(start)\n",
    "                    opin.attrib['to']=str(end)\n",
    "                    opins.append(opin)\n",
    "                start=ix\n",
    "                tag_on=True\n",
    "            elif token_idx<len(tokens) and lb[token_idx]==2 and pt==0 and c!=' ' and not tag_on:\n",
    "                start=ix\n",
    "                tag_on=True\n",
    "            elif token_idx<len(tokens) and (lb[token_idx]==0 or lb[token_idx]==1) and tag_on and pt==0:\n",
    "                end=ix\n",
    "                tag_on=False \n",
    "                opin=ET.Element(\"Opinion\")\n",
    "                opin.attrib['target']=sent.find('text').text[start:end]\n",
    "                opin.attrib['from']=str(start)\n",
    "                opin.attrib['to']=str(end)\n",
    "                opins.append(opin)\n",
    "            elif token_idx>=len(tokens) and tag_on:\n",
    "                end=ix\n",
    "                tag_on=False \n",
    "                opin=ET.Element(\"Opinion\")\n",
    "                opin.attrib['target']=sent.find('text').text[start:end]\n",
    "                opin.attrib['from']=str(start)\n",
    "                opin.attrib['to']=str(end)\n",
    "                opins.append(opin)\n",
    "            if c==' ':\n",
    "                pass\n",
    "            elif tokens[token_idx][pt:pt+2]=='``' or tokens[token_idx][pt:pt+2]==\"''\":\n",
    "                pt+=2\n",
    "            else:\n",
    "                pt+=1\n",
    "        if tag_on:\n",
    "            tag_on=False\n",
    "            end=len(sent.find('text').text)\n",
    "            opin=ET.Element(\"Opinion\")\n",
    "            opin.attrib['target']=sent.find('text').text[start:end]\n",
    "            opin.attrib['from']=str(start)\n",
    "            opin.attrib['to']=str(end)\n",
    "            opins.append(opin)\n",
    "        sent.append(opins )\n",
    "    dom.write(output_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-04T10:27:53.972423Z",
     "start_time": "2021-01-04T10:27:53.965387Z"
    }
   },
   "outputs": [],
   "source": [
    "def label_laptop_xml(fn, output_fn, corpus, label):\n",
    "    dom=ET.parse(fn)\n",
    "    root=dom.getroot()\n",
    "    pred_y=[]\n",
    "    for zx, sent in enumerate(root.iter(\"sentence\") ) :\n",
    "        tokens=corpus[zx]\n",
    "        lb=label[zx]\n",
    "        opins=ET.Element(\"aspectTerms\")\n",
    "        token_idx, pt, tag_on=0, 0, False\n",
    "        start, end=-1, -1\n",
    "        for ix, c in enumerate(sent.find('text').text):\n",
    "            if token_idx<len(tokens) and pt>=len(tokens[token_idx] ):\n",
    "                pt=0\n",
    "                token_idx+=1\n",
    "\n",
    "            if token_idx<len(tokens) and lb[token_idx]==1 and pt==0 and c!=' ':\n",
    "                if tag_on:\n",
    "                    end=ix\n",
    "                    tag_on=False\n",
    "                    opin=ET.Element(\"aspectTerm\")\n",
    "                    opin.attrib['term']=sent.find('text').text[start:end]\n",
    "                    opin.attrib['from']=str(start)\n",
    "                    opin.attrib['to']=str(end)\n",
    "                    opins.append(opin)\n",
    "                start=ix\n",
    "                tag_on=True\n",
    "            elif token_idx<len(tokens) and lb[token_idx]==2 and pt==0 and c!=' ' and not tag_on:\n",
    "                start=ix\n",
    "                tag_on=True\n",
    "            elif token_idx<len(tokens) and (lb[token_idx]==0 or lb[token_idx]==1) and tag_on and pt==0:\n",
    "                end=ix\n",
    "                tag_on=False \n",
    "                opin=ET.Element(\"aspectTerm\")\n",
    "                opin.attrib['term']=sent.find('text').text[start:end]\n",
    "                opin.attrib['from']=str(start)\n",
    "                opin.attrib['to']=str(end)\n",
    "                opins.append(opin)\n",
    "            elif token_idx>=len(tokens) and tag_on:\n",
    "                end=ix\n",
    "                tag_on=False \n",
    "                opin=ET.Element(\"aspectTerm\")\n",
    "                opin.attrib['term']=sent.find('text').text[start:end]\n",
    "                opin.attrib['from']=str(start)\n",
    "                opin.attrib['to']=str(end)\n",
    "                opins.append(opin)\n",
    "            if c==' ' or ord(c)==160:\n",
    "                pass\n",
    "            elif tokens[token_idx][pt:pt+2]=='``' or tokens[token_idx][pt:pt+2]==\"''\":\n",
    "                pt+=2\n",
    "            else:\n",
    "                pt+=1\n",
    "        if tag_on:\n",
    "            tag_on=False\n",
    "            end=len(sent.find('text').text)\n",
    "            opin=ET.Element(\"aspectTerm\")\n",
    "            opin.attrib['term']=sent.find('text').text[start:end]\n",
    "            opin.attrib['from']=str(start)\n",
    "            opin.attrib['to']=str(end)\n",
    "            opins.append(opin)\n",
    "        sent.append(opins )\n",
    "    dom.write(output_fn)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-04T10:27:53.979501Z",
     "start_time": "2021-01-04T10:27:53.973172Z"
    }
   },
   "outputs": [],
   "source": [
    "ae_data=np.load(\"data/prep_data/laptop.npz\")\n",
    "    \n",
    "#分割训练集，此处是固定分割\n",
    "test_x = ae_data['test_X']\n",
    "#print(\"test_x\", test_x)\n",
    "batch_test_X_len=np.sum(test_x[0:128]!=0, axis=1)\n",
    "#print(\"batch_test_X_len\", batch_test_X_len)\n",
    "batch_idx=batch_test_X_len.argsort()[::-1]\n",
    "#print(\"batch_idx\", batch_idx)\n",
    "r_idx=batch_idx.argsort()\n",
    "#print(\"r_idx\", r_idx)\n",
    "batch_test_X_len=batch_test_X_len[batch_idx]\n",
    "#print(\"batch_test_X_len\", batch_test_X_len)\n",
    "batch_test_X_mask=(test_x[0:128]!=0)[batch_idx].astype(np.uint8)\n",
    "#print(\"batch_test_X_mask\", batch_test_X_mask)\n",
    "batch_test_X=test_x[0:128][batch_idx]\n",
    "#print(\"batch_test_X\", batch_test_X)\n",
    "\n",
    "word_idx_fn = 'data/prep_data/word_idx.json'\n",
    "with open(word_idx_fn) as f:\n",
    "    word_idx=json.load(f)\n",
    "idx_word={}\n",
    "for key,val in word_idx.items():\n",
    "    idx_word[val]=key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-04T10:27:53.987980Z",
     "start_time": "2021-01-04T10:27:53.980309Z"
    }
   },
   "outputs": [],
   "source": [
    "def test(model, test_X, test_y, raw_X, domain, command, template, run_epoch, boundary_process, crf, generate_data, batch_size, num_process):\n",
    "    #pred_y就是我要的，预测出的标注\n",
    "    pred_y=np.zeros((test_X.shape[0], test_X.shape[1]), np.int16)\n",
    "    model.eval()\n",
    "    \"\"\"\n",
    "    作用：\n",
    "    1.把输入的每batch句子按照句子中单词多少重新排列\n",
    "    2.获得转换的索引batch_idx\n",
    "    3.获得反转换的索引r_idx\n",
    "    \"\"\"\n",
    "    for offset in range(0, test_X.shape[0], batch_size):\n",
    "        #统计一个batchsize内每行句子中的单词个数\n",
    "        batch_test_X_len=np.sum(test_X[offset:offset+batch_size]!=0, axis=1)\n",
    "        #按照每行句子单词数量从多到少，将每行句子原本的索引存到batch_idx中\n",
    "        batch_idx=batch_test_X_len.argsort()[::-1]\n",
    "        #现在batch_test_X_len就变成了按照每行句子单词数量从多到少的排序\n",
    "        batch_test_X_len=batch_test_X_len[batch_idx]\n",
    "        #按照每行单词数量从多到少排列，如果该列有单词，设值为1，否则为0\n",
    "        batch_test_X_mask=(test_X[offset:offset+batch_size]!=0)[batch_idx].astype(np.uint8)\n",
    "        #把输入数据集变成按照每行句子单词从大到小的排列\n",
    "        batch_test_X=test_X[offset:offset+batch_size][batch_idx]\n",
    "        #将numpy转成torch tensor\n",
    "        batch_test_X_mask=torch.autograd.Variable(torch.from_numpy(batch_test_X_mask).long().cuda())\n",
    "        batch_test_X=torch.autograd.Variable(torch.from_numpy(batch_test_X).long().cuda())\n",
    "        #获得预测的标签\n",
    "        batch_pred_y=model(batch_test_X, batch_test_X_len, batch_test_X_mask, testing=True)\n",
    "        #将输入句子还原的索引\n",
    "        r_idx=batch_idx.argsort()\n",
    "        if crf:\n",
    "            batch_pred_y=[batch_pred_y[idx] for idx in r_idx]\n",
    "            for ix in range(len(batch_pred_y)):\n",
    "                for jx in range(len(batch_pred_y[ix][0])):\n",
    "                    pred_y[offset+ix,jx]=batch_pred_y[ix][0][jx]\n",
    "        else:\n",
    "            batch_pred_y=batch_pred_y.data.cpu().numpy().argmax(axis=2)[r_idx]\n",
    "            pred_y[offset:offset+batch_size,:batch_pred_y.shape[1]]=batch_pred_y\n",
    "    assert len(pred_y)==len(test_X)\n",
    "    \n",
    "    if generate_data:\n",
    "        \"\"\"\n",
    "        输出test数据集的预测结果\n",
    "        \"\"\"\n",
    "        results = []\n",
    "        for j_th in range(len(test_X)):\n",
    "            result = []\n",
    "            words_num = test_X[j_th]\n",
    "            words_str = []\n",
    "            test_y_part = test_y[j_th]\n",
    "            #把数字转换成单词\n",
    "            for w in words_num:\n",
    "                if(w != 0):\n",
    "                    words_str.append(idx_word[w])\n",
    "            pred = pred_y[j_th]\n",
    "            for words_str, test_y_part, pred in zip(words_str, test_y_part, pred):\n",
    "                result.append(\" \".join([words_str, str(test_y_part), str(pred)]))\n",
    "            results.append(result)\n",
    "        test_as(results, run_epoch)\n",
    "        generate_repositioning_test_data(run_epoch)\n",
    "        #生成预测结果\n",
    "        predict_boundary_test_result(run_epoch)\n",
    "        \"\"\"\n",
    "        结束\n",
    "        \"\"\"\n",
    "    if boundary_process:\n",
    "        pred_y = generate_test_pred(run_epoch)\n",
    "    \n",
    "    #方面个数预测，应该在边界预测之后\n",
    "    if num_process:\n",
    "        if boundary_process:\n",
    "            #根据pred_label重构原始文件，再处理\n",
    "            results = []\n",
    "            for j_th in range(len(test_X)):\n",
    "                result = []\n",
    "                words_num = test_X[j_th]\n",
    "                words_str = []\n",
    "                test_y_part = test_y[j_th]\n",
    "                #把数字转换成单词\n",
    "                for w in words_num:\n",
    "                    if(w != 0):\n",
    "                        words_str.append(idx_word[w])\n",
    "                pred = pred_y[j_th]\n",
    "                for words_str, test_y_part, pred in zip(words_str, test_y_part, pred):\n",
    "                    result.append(\" \".join([words_str, str(test_y_part), str(pred)]))\n",
    "                results.append(result)\n",
    "            test_as(results, run_epoch)\n",
    "            generate_num_test_data(run_epoch)\n",
    "            json_to_csv(run_epoch)\n",
    "            predict_test_label(run_epoch)\n",
    "            pred_y = return_predicted_test_label(run_epoch, pred_y)\n",
    "        else:\n",
    "            generate_num_test_data(run_epoch)\n",
    "            json_to_csv(run_epoch)\n",
    "            predict_test_label(run_epoch)\n",
    "            pred_y = return_predicted_test_label(run_epoch, pred_y)\n",
    "\n",
    "    command=command.split()\n",
    "    if domain=='restaurant':\n",
    "        label_rest_xml(template, command[6], raw_X, pred_y)\n",
    "        acc=check_output(command ).split()\n",
    "        print(acc)\n",
    "        return float(acc[9][10:])\n",
    "    elif domain=='laptop':\n",
    "        label_laptop_xml(template, command[4], raw_X, pred_y)\n",
    "        acc=check_output(command ).split()\n",
    "        print(acc)\n",
    "        return float(acc[15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-04T10:27:53.992924Z",
     "start_time": "2021-01-04T10:27:53.989263Z"
    }
   },
   "outputs": [],
   "source": [
    "def evaluate(runs, data_dir, model_dir, domain, boundary_process, crf, generate_data, batch_size, add_num_loss, num_process, command, template):\n",
    "    #读所有数据，里面有训练数据也有测试数据（但是数据全是数字）\n",
    "    ae_data=np.load(data_dir+domain+\".npz\")\n",
    "    #读测试数据，数据是字符\n",
    "    with open(data_dir+domain+\"_raw_test.json\") as f:\n",
    "        raw_X=json.load(f)\n",
    "    results=[]\n",
    "    if add_num_loss:\n",
    "        model_dir = 'model_num/'\n",
    "    for r in range(runs):\n",
    "        #载入训练好的模型\n",
    "        model=torch.load(model_dir+domain+str(r))\n",
    "        result=test(model, ae_data['test_X'], ae_data['test_y'], raw_X, domain, command, template, r, boundary_process, crf, generate_data, batch_size, num_process)\n",
    "        results.append(result)\n",
    "    if boundary_process:\n",
    "        with open('evaluate_log/log_boundary_process.txt', 'a') as log:\n",
    "            log.write(str(time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime())) + '\\n')\n",
    "            log.write(str(sum(results)/len(results)) + '\\n' + '\\n')\n",
    "    else:\n",
    "        with open('evaluate_log/log.txt', 'a') as log:\n",
    "            log.write(str(time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime())) + '\\n')\n",
    "            log.write(str(sum(results)/len(results)) + '\\n' + '\\n')\n",
    "    print(sum(results)/len(results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-04T10:28:19.786074Z",
     "start_time": "2021-01-04T10:27:53.993832Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[b'Aspects', b'--------------------------------------', b'#System', b'Aspect', b'Terms=579', b'#Gold', b'Aspect', b'Terms=654', b'Pre:', b'0.8531952', b'(494/579)', b'Rec:', b'0.75535166', b'(494/654)', b'F:', b'0.80129766', b'Categories', b'--------------------------------------', b'#System', b'Aspect', b'Categories=0', b'#Gold', b'Aspect', b'Categories=0', b'Pre:', b'NaN', b'(0/0)', b'Rec:', b'NaN', b'(0/0)', b'F:', b'NaN']\n",
      "[b'Aspects', b'--------------------------------------', b'#System', b'Aspect', b'Terms=772', b'#Gold', b'Aspect', b'Terms=654', b'Pre:', b'0.6735751', b'(520/772)', b'Rec:', b'0.795107', b'(520/654)', b'F:', b'0.7293128', b'Categories', b'--------------------------------------', b'#System', b'Aspect', b'Categories=0', b'#Gold', b'Aspect', b'Categories=0', b'Pre:', b'NaN', b'(0/0)', b'Rec:', b'NaN', b'(0/0)', b'F:', b'NaN']\n",
      "[b'Aspects', b'--------------------------------------', b'#System', b'Aspect', b'Terms=730', b'#Gold', b'Aspect', b'Terms=654', b'Pre:', b'0.67671233', b'(494/730)', b'Rec:', b'0.75535166', b'(494/654)', b'F:', b'0.7138728', b'Categories', b'--------------------------------------', b'#System', b'Aspect', b'Categories=0', b'#Gold', b'Aspect', b'Categories=0', b'Pre:', b'NaN', b'(0/0)', b'Rec:', b'NaN', b'(0/0)', b'F:', b'NaN']\n",
      "[b'Aspects', b'--------------------------------------', b'#System', b'Aspect', b'Terms=597', b'#Gold', b'Aspect', b'Terms=654', b'Pre:', b'0.8492462', b'(507/597)', b'Rec:', b'0.77522933', b'(507/654)', b'F:', b'0.8105515', b'Categories', b'--------------------------------------', b'#System', b'Aspect', b'Categories=0', b'#Gold', b'Aspect', b'Categories=0', b'Pre:', b'NaN', b'(0/0)', b'Rec:', b'NaN', b'(0/0)', b'F:', b'NaN']\n",
      "[b'Aspects', b'--------------------------------------', b'#System', b'Aspect', b'Terms=625', b'#Gold', b'Aspect', b'Terms=654', b'Pre:', b'0.8304', b'(519/625)', b'Rec:', b'0.79357797', b'(519/654)', b'F:', b'0.8115716', b'Categories', b'--------------------------------------', b'#System', b'Aspect', b'Categories=0', b'#Gold', b'Aspect', b'Categories=0', b'Pre:', b'NaN', b'(0/0)', b'Rec:', b'NaN', b'(0/0)', b'F:', b'NaN']\n",
      "0.7733212719999999\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--runs', type=int, default=5)\n",
    "    parser.add_argument('--data_dir', type=str, default=\"data/prep_data/\")\n",
    "    parser.add_argument('--model_dir', type=str, default=\"model/\")\n",
    "    parser.add_argument('--domain', type=str, default=\"laptop\")\n",
    "    parser.add_argument('--crf', type=bool, default=True)\n",
    "    parser.add_argument('--generate_data', type=bool, default=False)\n",
    "    parser.add_argument('--batch_size', type=int, default=256)\n",
    "    parser.add_argument('--add_num_loss', type=bool, default=False)\n",
    "    parser.add_argument('--num_process', type=bool, default=True)\n",
    "    parser.add_argument('--boundary_process', type=bool, default=True)\n",
    "    args = parser.parse_known_args()[0]\n",
    "\n",
    "    if args.domain=='restaurant':\n",
    "        command=\"java -cp script/A.jar absa16.Do Eval -prd data/official_data/pred.xml -gld data/official_data/EN_REST_SB1_TEST.xml.gold -evs 2 -phs A -sbt SB1\"\n",
    "        template=\"data/official_data/EN_REST_SB1_TEST.xml.A\"\n",
    "    elif args.domain=='laptop':\n",
    "        command=\"java -cp script/eval.jar Main.Aspects data/official_data/pred.xml data/official_data/Laptops_Test_Gold.xml\"\n",
    "        template=\"data/official_data/Laptops_Test_Data_PhaseA.xml\"\n",
    "    \n",
    "    evaluate(args.runs, args.data_dir, args.model_dir, args.domain, args.boundary_process, args.crf, args.generate_data, args.batch_size, args.add_num_loss, args.num_process, command, template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-04T10:28:19.818064Z",
     "start_time": "2021-01-04T10:28:19.791337Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.793139786"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#原始GE-CNN\n",
    "[b'Aspects',b'#System', b'Aspect', b'Terms=615', b'#Gold', b'Aspect', b'Terms=654', b'Pre:', b'0.8113821', b'(499/615)', b'Rec:', b'0.762997', b'(499/654)', b'F:', b'0.78644603',]\n",
    "[b'Aspects',b'#System', b'Aspect', b'Terms=670', b'#Gold', b'Aspect', b'Terms=654', b'Pre:', b'0.78507465', b'(526/670)', b'Rec:', b'0.80428135', b'(526/654)', b'F:', b'0.794562',]\n",
    "[b'Aspects',b'#System', b'Aspect', b'Terms=607', b'#Gold', b'Aspect', b'Terms=654', b'Pre:', b'0.82372326', b'(500/607)', b'Rec:', b'0.764526', b'(500/654)', b'F:', b'0.7930215',]\n",
    "[b'Aspects',b'#System', b'Aspect', b'Terms=630', b'#Gold', b'Aspect', b'Terms=654', b'Pre:', b'0.8095238', b'(510/630)', b'Rec:', b'0.7798165', b'(510/654)', b'F:', b'0.7943926',]\n",
    "[b'Aspects',b'#System', b'Aspect', b'Terms=668', b'#Gold', b'Aspect', b'Terms=654', b'Pre:', b'0.78892213', b'(527/668)', b'Rec:', b'0.8058104', b'(527/654)', b'F:', b'0.7972768',]\n",
    "0.793139786"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-04T10:28:19.825761Z",
     "start_time": "2021-01-04T10:28:19.820807Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.823576714"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#使用boundary_process\n",
    "[b'Aspects',b'#System', b'Aspect', b'Terms=603', b'#Gold', b'Aspect', b'Terms=654', b'Pre:', b'0.85240465', b'(514/603)', b'Rec:', b'0.7859327', b'(514/654)', b'F:', b'0.81782025']\n",
    "[b'Aspects',b'#System', b'Aspect', b'Terms=645', b'#Gold', b'Aspect', b'Terms=654', b'Pre:', b'0.8310078', b'(536/645)', b'Rec:', b'0.81957185', b'(536/654)', b'F:', b'0.82525015']\n",
    "[b'Aspects',b'#System', b'Aspect', b'Terms=590', b'#Gold', b'Aspect', b'Terms=654', b'Pre:', b'0.8661017', b'(511/590)', b'Rec:', b'0.78134555', b'(511/654)', b'F:', b'0.8215434']\n",
    "[b'Aspects',b'#System', b'Aspect', b'Terms=620', b'#Gold', b'Aspect', b'Terms=654', b'Pre:', b'0.85', b'(527/620)', b'Rec:', b'0.8058104', b'(527/654)', b'F:', b'0.82731557']\n",
    "[b'Aspects',b'#System', b'Aspect', b'Terms=656', b'#Gold', b'Aspect', b'Terms=654', b'Pre:', b'0.8246951', b'(541/656)', b'Rec:', b'0.8272171', b'(541/654)', b'F:', b'0.8259542']\n",
    "0.823576714"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-04T10:28:19.830972Z",
     "start_time": "2021-01-04T10:28:19.826846Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.81419356"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#使用num_process\n",
    "[b'Aspects',b'#System', b'Aspect', b'Terms=563', b'#Gold', b'Aspect', b'Terms=654', b'Pre:', b'0.86856127', b'(489/563)', b'Rec:', b'0.7477064', b'(489/654)', b'F:', b'0.80361545']\n",
    "[b'Aspects',b'#System', b'Aspect', b'Terms=596', b'#Gold', b'Aspect', b'Terms=654', b'Pre:', b'0.8624161', b'(514/596)', b'Rec:', b'0.7859327', b'(514/654)', b'F:', b'0.8224']\n",
    "[b'Aspects',b'#System', b'Aspect', b'Terms=557', b'#Gold', b'Aspect', b'Terms=654', b'Pre:', b'0.87971276', b'(490/557)', b'Rec:', b'0.74923545', b'(490/654)', b'F:', b'0.8092485']\n",
    "[b'Aspects',b'#System', b'Aspect', b'Terms=574', b'#Gold', b'Aspect', b'Terms=654', b'Pre:', b'0.87108016', b'(500/574)', b'Rec:', b'0.764526', b'(500/654)', b'F:', b'0.81433225']\n",
    "[b'Aspects',b'#System', b'Aspect', b'Terms=600', b'#Gold', b'Aspect', b'Terms=654', b'Pre:', b'0.85833335', b'(515/600)', b'Rec:', b'0.78746176', b'(515/654)', b'F:', b'0.8213716']\n",
    "0.81419356"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#同时使用\n",
    "#待优化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-04T10:28:23.998680Z",
     "start_time": "2021-01-04T10:28:19.831975Z"
    }
   },
   "outputs": [],
   "source": [
    "predict_test_label(0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_0.4",
   "language": "python",
   "name": "pytorch_0.4"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
